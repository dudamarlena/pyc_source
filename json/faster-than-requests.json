{"info": {"author": "Juan Carlos", "author_email": "juancarlospaco@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Environment :: Console", "Environment :: Other Environment", "Intended Audience :: Developers", "Intended Audience :: Other Audience", "Natural Language :: English", "Operating System :: OS Independent", "Operating System :: POSIX :: Linux", "Programming Language :: Python", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.8", "Programming Language :: Python :: 3 :: Only", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Software Development"], "description": "<meta name='keywords' content='python, requests, faster, speed, benchmark, pycurl, wget, urllib, rapido, velocidad, optimizacion, cython, pypy, urllib3, urllib2, urllib4, urllib5, urllib6, urllib7, urllib8, urllib9, pywget, cpython, http, httpclient, curl, libcurl, ssl, docker, json, ndjson, https, rapido, veloz, performance, critical, compiled, module, modulo, loc, minimalismo, minimalism, simple, small, tiny, argentina, spanish, compare, mejora, scraper, scrapy, data science, open data, open api'>\n\n\n# Faster-than-Requests\n\n[![screenshot](https://source.unsplash.com/eH_ftJYhaTY/800x402 \"Please Star this repo on GitHub!\")](https://youtu.be/QiKwnlyhKrk?t=5)\n\n![screenshot](temp.png \"Please Star this repo on GitHub!\")\n\n![](https://img.shields.io/github/languages/top/juancarlospaco/faster-than-requests?style=for-the-badge)\n![](https://img.shields.io/github/stars/juancarlospaco/faster-than-requests?style=for-the-badge \"Star faster-than-requests on GitHub!\")\n![](https://img.shields.io/maintenance/yes/2020?style=for-the-badge \"2020\")\n![](https://img.shields.io/github/languages/code-size/juancarlospaco/faster-than-requests?style=for-the-badge)\n![](https://img.shields.io/github/issues-raw/juancarlospaco/faster-than-requests?style=for-the-badge \"Bugs\")\n![](https://img.shields.io/github/issues-pr-raw/juancarlospaco/faster-than-requests?style=for-the-badge \"PRs\")\n![](https://img.shields.io/github/last-commit/juancarlospaco/faster-than-requests?style=for-the-badge \"Commits\")\n\n<img src=\"http://feeds.feedburner.com/RecentCommitsToFaster-than-requestsmaster.1.gif\" title=\"Recent Commits to Faster Than Requests\" width=\"99%\" height=\"75px\">\n\n| Library                       | Speed    | Files | LOC  | Dependencies          | Developers | Multi-Threaded Web Scraper Built-in |\n|-------------------------------|----------|-------|------|-----------------------|------------|-------------------------------------|\n| PyWGET                        | `152.39` | 1     | 338  | Wget                  | >17        | :negative_squared_cross_mark:       |\n| Requests                      | `15.58`  | >20   | 2558 | >=7                   | >527       | :negative_squared_cross_mark:       |\n| Requests (cached object)      |  `5.50`  | >20   | 2558 | >=7                   | >527       | :negative_squared_cross_mark:       |\n| Urllib                        |  `4.00`  | ???   | 1200 | 0 (std lib)           | ???        | :negative_squared_cross_mark:       |\n| Urllib3                       |  `3.55`  | >40   | 5242 | 0 (No SSL), >=5 (SSL) | >188       | :negative_squared_cross_mark:       |\n| PyCurl                        |  `0.75`  | >15   | 5932 | Curl, LibCurl         | >50        | :negative_squared_cross_mark:       |\n| PyCurl (no SSL)               |  `0.68`  | >15   | 5932 | Curl, LibCurl         | >50        | :negative_squared_cross_mark:       |\n| Faster_than_requests          |  `0.45`  | 1     | 99   | 0                     | 1          | :heavy_check_mark: 6, [One-Liner](https://github.com/juancarlospaco/faster-than-requests/blob/master/examples/multithread_web_scraper.py#L2) |\n\n<details>\n\n- Lines Of Code counted using [CLOC](https://github.com/AlDanial/cloc).\n- Direct dependencies of the package when ready to run.\n- Benchmarks run on Docker from Dockerfile on this repo.\n- Developers counted from the Contributors list of Git.\n- Speed is IRL time to complete 10000 HTTP local requests.\n- Stats as of year 2019.\n- x86_64 64Bit AMD, SSD, Arch Linux.\n\n</details>\n\n\n# Use\n\n```python\nimport faster_than_requests as requests\n\nrequests.get(\"http://httpbin.org/get\")                                      # GET\nrequests.post(\"http://httpbin.org/post\", \"Some Data Here\")                  # POST\nrequests.download(\"http://example.com/foo.jpg\", \"out.jpg\")                  # Download a file\nrequests.scraper([\"http://foo.io\", \"http://bar.io\"], threads=True)          # Multi-Threaded Web Scraper\nrequests.scraper5([\"http://foo.io\"], sqlite_file_path=\"database.db\")        # URL-to-SQLite Web Scraper\nrequests.scraper6([\"http://python.org\"], [\"(www|http:|https:)+[^\\s]+[\\w]\"]) # Regex-powered Web Scraper\n```\n\n# Table Of Contents\n\n|                         |                             |                               |                           |\n|:-----------------------:|:---------------------------:|:-----------------------------:|:-------------------------:|\n| [get()](#get)           | [post()](#post)             | [put()](#put)                 | [head()](#head)           |\n| [patch()](#patch)       | [delete()](#delete)         | [download()](#download)       | [download2()](#download2) |\n| [scraper()](#scraper)   | [scraper2()](#scraper2)     | [scraper3()](#scraper3)       | [scraper4()](#scraper4)   |\n| [scraper5()](#scraper5) | [scraper6()](#scraper6)     | [debugs()](#debugs)           | [get2str()](#get2str)     |\n| [get2str2()](#get2str2) | [get2ndjson()](#get2ndjson) | [get2dict()](#get2dict)       | [get2json()](#get2json)   |\n| [post2str()](#post2str) | [post2dict()](#post2dict)   | [post2json()](#post2json)     | [post2list()](#post2list) |\n| [requests()](#requests) | [requests2()](#requests2)   | [tuples2json()](#tuples2json) | [set_headers()](#set_headers) |\n| [download3()](#download3) |                           |                               |                               |\n| [How to set Timeout](#How-to-set-Timeout) | [How to set Max Redirects](#How-to-set-Max-Redirects) | [How to set User Agent](#How-to-set-User-Agent) | [How to set Proxy](#How-to-set-Proxy) |\n| [How to Install](#install) | [How to Windows](#windows) | [FAQ](#faq) | [Get Help](https://github.com/juancarlospaco/faster-than-requests/issues/new/choose) |\n| [PyPI](https://pypi.org/project/faster-than-requests) | [GitHub Actions / CI](https://github.com/juancarlospaco/faster-than-requests/actions?query=workflow%3APYTHON) | [Examples](https://github.com/juancarlospaco/faster-than-requests/tree/master/examples) | [Sponsors](#sponsors) |\n\n\n# get()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP GET and returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://dev.to`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.get(\"http://example.com\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n**See Also:**\n[get2str()](https://github.com/juancarlospaco/faster-than-requests#get2str) and [get2str2()](https://github.com/juancarlospaco/faster-than-requests#get2str2)\n\n</details>\n\n\n\n# post()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP POST and returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://dev.to`.\n- `body` the Body data, string type, required, can be empty string. To Post Files use this too.\n- `multipart_data` MultiPart data, optional, list of tupes type, must not be empty list, example `[(\"key\", \"value\")]`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.post(\"http://httpbin.org/post\", \"Some Data Here\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n# put()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP PUT and returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://nim-lang.org`.\n- `body` the Body data, string type, required, can be empty string.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.put(\"http://httpbin.org/post\", \"Some Data Here\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n# delete()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP DELETE and returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://nim-lang.org`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.delete(\"http://example.com/api/something\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n# patch()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP PATCH and returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://archlinux.org`.\n- `body` the Body data, string type, required, can be empty string.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.patch(\"http://example.com\", \"My Body Data Here\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n# head()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP HEAD and returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://nim-lang.org`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.head(\"http://example.com/api/something\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n# Extras: Go beyond requests\n\n## scraper()\n<details>\n\n**Description:**\nMulti-Threaded Ready-Made URL-Deduplicating Web Scraper from a list of URLs.\n\n![](multithread-scraper.png)\n\nAll arguments are optional, it only needs the URL to get to work.\nScraper is designed to be like a 2-Step Web Scraper, that makes a first pass collecting all URL Links and then a second pass actually fetching those URLs.\nRequests are processed asynchronously. This means that it doesn\u2019t need to wait for a request to be finished to be processed.\n\n**Arguments:**\n- `list_of_urls` List of URLs, URL must be string type, required, must not be empty list, example `[\"http://example.io\"]`.\n- `html_tag` HTML Tag to parse, string type, optional, defaults to `\"a\"` being Links, example `\"h1\"`.\n- `case_insensitive` Case Insensitive, `True` for Case Insensitive, boolean type, optional, defaults to `True`, example `True`.\n- `deduplicate_urls` Deduplicate `list_of_urls` removing repeated URLs, boolean type, optional, defaults to `False`, example `False`.\n- `threads` Passing `threads = True` uses Multi-Threading, `threads = False` will Not use Multi-Threading, boolean type, optional, omitting it will Not use Multi-Threading.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.scraper([\"https://nim-lang.org\", \"http://example.com\"], threads=True)\n```\n\n**Returns:** Scraped Webs.\n\n</details>\n\n\n\n## scraper2()\n<details>\n\n**Description:**\nMulti-Tag Ready-Made URL-Deduplicating Web Scraper from a list of URLs.\nAll arguments are optional, it only needs the URL to get to work.\nScraper is designed to be like a 2-Step Web Scraper, that makes a first pass collecting all URL Links and then a second pass actually fetching those URLs.\nRequests are processed asynchronously. This means that it doesn\u2019t need to wait for a request to be finished to be processed.\nYou can think of this scraper as a parallel evolution of the original scraper.\n\n**Arguments:**\n- `list_of_urls` List of URLs, URL must be string type, required, must not be empty list, example `[\"http://example.io\"]`.\n- `list_of_tags` List of HTML Tags to parse, List type, optional, defaults to `[\"a\"]` being Links, example `[\"h1\", \"h2\"]`.\n- `case_insensitive` Case Insensitive, `True` for Case Insensitive, boolean type, optional, defaults to `True`, example `True`.\n- `deduplicate_urls` Deduplicate `list_of_urls` removing repeated URLs, boolean type, optional, defaults to `False`, example `False`.\n- `verbose` Verbose, print to terminal console the progress, bool type, optional, defaults to `True`, example `False`.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value, example `42`.\n- `threads` Passing `threads = True` uses Multi-Threading, `threads = False` will Not use Multi-Threading, boolean type, optional, omitting it will Not use Multi-Threading.\n- `agent` User Agent, string type, optional, must not be empty string.\n- `redirects` Maximum Redirects, integer type, optional, defaults to `5`, must be positive integer.\n- `timeout` Timeout, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `-1`, must be a positive integer value, example `42`.\n- `header` HTTP Header, any HTTP Headers can be put here, list type, optional, example `[(\"key\", \"value\")]`.\n- `proxy_url` HTTPS Proxy Full URL, string type, optional, must not be empty string.\n- `proxy_auth` HTTPS Proxy Authentication, string type, optional, defaults to `\"\"`, empty string is ignored.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.scraper2([\"https://nim-lang.org\", \"http://example.com\"], list_of_tags=[\"h1\", \"h2\"], case_insensitive=False)\n```\n\n**Returns:** Scraped Webs.\n\n</details>\n\n\n## scraper3()\n<details>\n\n**Description:**\nMulti-Tag Ready-Made URL-Deduplicating Web Scraper from a list of URLs.\n\n![](multitag-scraper.png)\n\nThis Scraper is designed with lots of extra options on the arguments.\nAll arguments are optional, it only needs the URL to get to work.\nScraper is designed to be like a 2-Step Web Scraper, that makes a first pass collecting all URL Links and then a second pass actually fetching those URLs.\nYou can think of this scraper as a parallel evolution of the original scraper.\n\n**Arguments:**\n- `list_of_urls` List of URLs, URL must be string type, required, must not be empty list, example `[\"http://example.io\"]`.\n- `list_of_tags` List of HTML Tags to parse, List type, optional, defaults to `[\"a\"]` being Links, example `[\"h1\", \"h2\"]`.\n- `case_insensitive` Case Insensitive, `True` for Case Insensitive, boolean type, optional, defaults to `True`, example `True`.\n- `deduplicate_urls` Deduplicate `list_of_urls` removing repeated URLs, boolean type, optional, defaults to `False`, example `False`.\n- `start_with` Match at the start of the line, similar to `str().startswith()`, string type, optional, example `\"<cite \"`.\n- `ends_with` Match at the end of the line, similar to `str().endswith()`,  string type, optional, example `\"</cite>\"`.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value, example `42`.\n- `line_start` Slice the line at the start by this index, integer type, optional, defaults to `0` meaning no slicing since string start at index 0, example `3` cuts off 3 letters of the line at the start.\n- `line_end` Slice the line at the end by this *reverse* index, integer type, optional, defaults to `1` meaning no slicing since string ends at reverse index 1, example `9` cuts off 9 letters of the line at the end.\n- `pre_replacements` List of tuples of strings to replace *before* parsing, replacements are in parallel, List type, optional, example `[(\"old\", \"new\"), (\"red\", \"blue\")]` will replace `\"old\"` with `\"new\"` and will replace `\"red\"` with `\"blue\"`.\n- `post_replacements` List of tuples of strings to replace *after* parsing, replacements are in parallel, List type, optional, example `[(\"old\", \"new\"), (\"red\", \"blue\")]` will replace `\"old\"` with `\"new\"` and will replace `\"red\"` with `\"blue\"`.\n- `agent` User Agent, string type, optional, must not be empty string.\n- `redirects` Maximum Redirects, integer type, optional, defaults to `5`, must be positive integer.\n- `timeout` Timeout, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `-1`, must be a positive integer value, example `42`.\n- `header` HTTP Header, any HTTP Headers can be put here, list type, optional, example `[(\"key\", \"value\")]`.\n- `proxy_url` HTTPS Proxy Full URL, string type, optional, must not be empty string.\n- `proxy_auth` HTTPS Proxy Authentication, string type, optional, defaults to `\"\"`, empty string is ignored.\n- `verbose` Verbose, print to terminal console the progress, bool type, optional, defaults to `True`, example `False`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.scraper3([\"https://nim-lang.org\", \"http://example.com\"], list_of_tags=[\"h1\", \"h2\"], case_insensitive=False)\n```\n\n**Returns:** Scraped Webs.\n\n</details>\n\n\n## scraper4()\n<details>\n\n**Description:**\nImages and Photos Ready-Made Web Scraper from a list of URLs.\n\n![](photo-scraper.png)\n\nThe Images and Photos scraped from the first URL will be put into a new sub-folder named `0`,\nImages and Photos scraped from the second URL will be put into a new sub-folder named `1`, and so on.\nAll arguments are optional, it only needs the URL to get to work.\nYou can think of this scraper as a parallel evolution of the original scraper.\n\n**Arguments:**\n- `list_of_urls` List of URLs, URL must be string type, required, must not be empty list, example `[\"https://unsplash.com/s/photos/cat\", \"https://unsplash.com/s/photos/dog\"]`.\n- `case_insensitive` Case Insensitive, `True` for Case Insensitive, boolean type, optional, defaults to `True`, example `True`.\n- `deduplicate_urls` Deduplicate `list_of_urls` removing repeated URLs, boolean type, optional, defaults to `False`, example `False`.\n- `visited_urls` Do not visit same URL twice, even if redirected into, keeps track of visited URLs, bool type, optional, defaults to `True`.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value, example `42`.\n- `folder` Directory to download Images and Photos, string type, optional, defaults to current folder, must not be empty string, example `/tmp`.\n- `force_extension` Force file extension to be this file extension, string type, optional, defaults to `\".jpg\"`, must not be empty string, example `\".png\"`.\n- `https_only` Force to download images on Secure HTTPS only ignoring plain HTTP, sometimes HTTPS may redirect to HTTP, bool type, optional, defaults to `False`, example `True`.\n- `html_output` Collect all scraped Images and Photos into 1 HTML file with all elements scraped, bool type, optional, defaults to `True`, example `False`.\n- `csv_output` Collect all scraped URLs into 1 CSV file with all links scraped, bool type, optional, defaults to `True`, example `False`.\n- `verbose` Verbose, print to terminal console the progress, bool type, optional, defaults to `True`, example `False`.\n- `print_alt` print to terminal console the `alt` attribute of the Images and Photos, bool type, optional, defaults to `False`, example `True`.\n- `picture` Scrap images from the new HTML5 `<picture>` tags instead of `<img>` tags, `<picture>` are Responsive images for several resolutions but also you get duplicated images, bool type, optional, defaults to `False`, example `True`.\n- `agent` User Agent, string type, optional, must not be empty string.\n- `redirects` Maximum Redirects, integer type, optional, defaults to `5`, must be positive integer.\n- `timeout` Timeout, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `-1`, must be a positive integer value, example `42`.\n- `header` HTTP Header, any HTTP Headers can be put here, list type, optional, example `[(\"key\", \"value\")]`.\n- `proxy_url` HTTPS Proxy Full URL, string type, optional, must not be empty string.\n- `proxy_auth` HTTPS Proxy Authentication, string type, optional, defaults to `\"\"`, empty string is ignored.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.scraper4([\"https://unsplash.com/s/photos/cat\", \"https://unsplash.com/s/photos/dog\"])\n```\n\n**Returns:** None.\n\n</details>\n\n\n## scraper5()\n<details>\n\n**Description:**\nRecursive Web Scraper to SQLite Database, you give it an URL, it gives back an SQLite.\n\n![](sqlite-scraper.png)\n\nSQLite database can be visualized with any SQLite WYSIWYG, like https://sqlitebrowser.org\nIf the script gets interrupted like with CTRL+C it will try its best to keep data consistent.\nAdditionally it will create a CSV file with all the scraped URLs.\nHTTP Headers are stored as Pretty-Printed JSON.\nDate and Time are stored as Unix Timestamps.\nAll arguments are optional, it only needs the URL and SQLite file path to get to work.\nYou can think of this scraper as a parallel evolution of the original scraper.\n\n**Arguments:**\n- `list_of_urls` List of URLs, URL must be string type, required, must not be empty list, example `[\"https://unsplash.com/s/photos/cat\", \"https://unsplash.com/s/photos/dog\"]`.\n- `sqlite_file_path` Full file path to a new SQLite Database, must be `.db` file extension, string type, required, must not be empty string, example `\"scraped_data.db\"`.\n- `skip_ends_with` Skip the URL if ends with this pattern, list type, optional, must not be empty list, example `[\".jpg\", \".pdf\"]`.\n- `case_insensitive` Case Insensitive, `True` for Case Insensitive, boolean type, optional, defaults to `True`, example `True`.\n- `deduplicate_urls` Deduplicate `list_of_urls` removing repeated URLs, boolean type, optional, defaults to `False`, example `False`.\n- `visited_urls` Do not visit same URL twice, even if redirected into, keeps track of visited URLs, bool type, optional, defaults to `True`.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value, example `42`.\n- `https_only` Force to download images on Secure HTTPS only ignoring plain HTTP, sometimes HTTPS may redirect to HTTP, bool type, optional, defaults to `False`, example `True`.\n- `only200` Only commit to Database the successful scraping pages, ignore all errors, bool type, optional, example `True`.\n- `agent` User Agent, string type, optional, must not be empty string.\n- `redirects` Maximum Redirects, integer type, optional, defaults to `5`, must be positive integer.\n- `timeout` Timeout, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `-1`, must be a positive integer value, example `42`.\n- `max_loops` Maximum total Loops to do while scraping, like a global guard for infinite redirections, integer type, optional, example `999`.\n- `max_deep` Maximum total scraping Recursive Deep, like a global guard for infinite deep recursivity, integer type, optional, example `999`.\n- `header` HTTP Header, any HTTP Headers can be put here, list type, optional, example `[(\"key\", \"value\")]`.\n- `proxy_url` HTTPS Proxy Full URL, string type, optional, must not be empty string.\n- `proxy_auth` HTTPS Proxy Authentication, string type, optional, defaults to `\"\"`, empty string is ignored.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.scraper5([\"https://example.com\"], \"scraped_data.db\")\n```\n\n**Returns:** None.\n\n</details>\n\n\n## scraper6()\n<details>\n\n**Description:**\nRegex powered Web Scraper from a list of URLs.\nScrap web content using a list of Perl Compatible Regular Expressions (PCRE standard).\nYou can configure the Regular Expressions to be case insensitive or multiline or extended.\n\nThis Scraper is designed for developers that know Regular Expressions.\n[Learn Regular Expressions.](https://github.com/ziishaned/learn-regex#translations)\n\nAll arguments are optional, it only needs the URL and the Regex to get to work.\nYou can think of this scraper as a parallel evolution of the original scraper.\n\n**Regex Arguments:**\n(Arguments focused on Regular Expression parsing and matching)\n\n- `list_of_regex` List of Perl Compatible Regular Expressions (PCRE standard) to match the URL against, List type, required, example `[\"(www|http:|https:)+[^\\s]+[\\w]\"]`.\n- `case_insensitive` Case Insensitive Regular Expressions, do caseless matching, `True` for Case Insensitive, boolean type, optional, defaults to `False`, example `True`.\n- `multiline` Multi-Line Regular Expressions, `^` and `$` match newlines within data, boolean type, optional, defaults to `False`, example `True`.\n- `extended` Extended Regular Expressions, ignore all whitespaces and `#` comments, boolean type, optional, defaults to `False`, example `True`.\n- `dot` Dot `.` matches anything, including new lines, boolean type, optional, defaults to `False`, example `True`.\n- `start_with` Perl Compatible Regular Expression to match at the start of the line, similar to `str().startswith()` but with Regular Expressions, string type, optional.\n- `ends_with`  Perl Compatible Regular Expression to match at the end of the line,  similar to `str().endswith()` but with Regular Expressions, string type, optional.\n- `post_replacement_regex` Perl Compatible Regular Expressions (PCRE standard) to replace *after* parsing, string type, optional, this option works with `post_replacement_by`, this is like a Regex post-processing, this option is for experts on Regular Expressions.\n- `post_replacement_by` string **to replace by** *after* parsing, string type, optional, this option works with `post_replacement_regex`, this is like a Regex post-processing, this option is for experts on Regular Expressions.\n- `re_start` Perl Compatible Regular Expression matchs start at this index, positive integer type, optional, defaults to `0`, this option is for experts on Regular Expressions.\n\n**Arguments:**\n- `list_of_urls` List of URLs, URL must be string type, required, must not be empty list, example `[\"http://example.io\"]`.\n- `deduplicate_urls` Deduplicate `list_of_urls` removing repeated URLs, boolean type, optional, defaults to `False`, example `False`.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value, example `42`.\n- `agent` User Agent, string type, optional, must not be empty string.\n- `redirects` Maximum Redirects, integer type, optional, defaults to `5`, must be positive integer.\n- `timeout` Timeout, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `-1`, must be a positive integer value, example `42`.\n- `header` HTTP Header, any HTTP Headers can be put here, list type, optional, example `[(\"key\", \"value\")]`.\n- `proxy_url` HTTPS Proxy Full URL, string type, optional, must not be empty string.\n- `proxy_auth` HTTPS Proxy Authentication, string type, optional, defaults to `\"\"`, empty string is ignored.\n- `verbose` Verbose, print to terminal console the progress, bool type, optional, defaults to `True`, example `False`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.scraper6([\"http://nim-lang.org\", \"http://python.org\"], [\"(www|http:|https:)+[^\\s]+[\\w]\"])\n```\n\n**Returns:** Scraped Webs.\n\n</details>\n\n\n## get2str()\n<details>\n\n**Description:**\nTakes an URL string, makes an HTTP GET and returns a string with the response Body.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://archlinux.org`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.get2str(\"http://example.com\")\n```\n\n**Returns:** Response body, `string` type, can be empty string.\n\n</details>\n\n\n\n## get2str2()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP GET for each URL, and returns a list of strings with the response Body.\n\n**Arguments:**\n- `list_of_urls` A list of the remote URLs, list type, required. Objects inside the list must be string type.\n- `threads` Passing `threads = True` uses Multi-Threading, `threads = False` will Not use Multi-Threading, omitting it will Not use Multi-Threading.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.get2str2([\"http://example.com/foo\", \"http://example.com/bar\"], threads = True)\n```\n\n**Returns:**\nList of response bodies, `list` type, values of the list are string type,\nvalues of the list can be empty string, can be empty list.\n\n</details>\n\n\n\n## get2ndjson()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP GET for each URL, returns a list of strings with the response, and writes the responses to a NDJSON file, it can accumulate several JSON responses into a single file.\n\n**Arguments:**\n- `list_of_urls` A list of the remote URLs, list type, required. Objects inside the list must be string type.\n- `ndjson_file_path` Full path to a local writable NDJSON file, string type, required, file can be non-existent and it will be created, if it exists it will the overwritten.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.get2ndjson([\"http://example.com/foo\", \"http://example.com/bar\"], \"/some/folder/some/file.ndjson\")\n```\n\n**Returns:** None.\n\n</details>\n\n\n\n## get2dict()\n<details>\n\n**Description:**\nTakes an URL, makes an HTTP GET, returns a dict with the response Body.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://alpinelinux.org`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.get2dict(\"http://example.com\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n## get2json()\n<details>\n\n**Description:**\nTakes an URL, makes an HTTP GET, returns a Minified Computer-friendly single-line JSON with the response Body.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string, example `https://alpinelinux.org`.\n- `pretty_print` Pretty Printed JSON, optional, defaults to `False`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.get2json(\"http://example.com\", pretty_print=True)\n```\n\n**Returns:** Response Body, Minified or Pretty-Printed JSON.\n\n</details>\n\n\n\n## post2str()\n<details>\n\n**Description:**\nTakes an URL, makes an HTTP POST, returns the response Body as string type.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string.\n- `body` the Body data, string type, required, can be empty string.\n- `multipart_data` MultiPart data, optional, list of tupes type, must not be empty list, example `[(\"key\", \"value\")]`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.post2str(\"http://example.com/api/foo\", \"My Body Data Here\")\n```\n\n**Returns:** Response body, `string` type, can be empty string.\n\n</details>\n\n\n\n## post2dict()\n<details>\n\n**Description:**\nTakes an URL, makes a HTTP POST on that URL, returns a dict with the response.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string.\n- `body` the Body data, string type, required, can be empty string.\n- `multipart_data` MultiPart data, optional, list of tupes type, must not be empty list, example `[(\"key\", \"value\")]`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.post2dict(\"http://example.com/api/foo\", \"My Body Data Here\")\n```\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n## post2json()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP GET for each URL, returns a list of responses.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string.\n- `body` the Body data, string type, required, can be empty string.\n- `multipart_data` MultiPart data, optional, list of tupes type, must not be empty list, example `[(\"key\", \"value\")]`.\n- `pretty_print` Pretty Printed JSON, optional, defaults to `False`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.post2json(\"http://example.com/api/foo\", \"My Body Data Here\")\n```\n\n**Returns:** Response, string type.\n\n</details>\n\n\n\n## post2list()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP POST for each URL, returns a list of responses.\n\n**Arguments:**\n- `list_of_urls` the remote URLS, list type, required, the objects inside the list must be string type.\n- `body` the Body data, string type, required, can be empty string.\n- `multipart_data` MultiPart data, optional, list of tupes type, must not be empty list, example `[(\"key\", \"value\")]`.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.post2list(\"http://example.com/api/foo\", \"My Body Data Here\")\n```\n\n**Returns:**\nList of response bodies, `list` type, values of the list are string type,\nvalues of the list can be empty string, can be empty list.\n\n</details>\n\n\n\n## download()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP GET for each URL, returns a list of responses.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string.\n- `filename` the local filename, string type, required, must not be empty string, full path recommended, can be relative path, includes file extension.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.download(\"http://example.com/api/foo\", \"my_file.ext\")\n```\n\n**Returns:** None.\n\n</details>\n\n\n\n## download2()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP GET Download for each URL of the list.\n\n**Arguments:**\n- `list_of_files` list of tuples, tuples must be 2 items long, first item is URL and second item is filename.\nThe remote URL, string type, required, must not be empty string, is the first item on the tuple.\nThe local filename, string type, required, must not be empty string, can be full path, can be relative path, must include file extension.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value.\n- `threads` Passing `threads = True` uses Multi-Threading, `threads = False` will Not use Multi-Threading, omitting it will Not use Multi-Threading.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.download2([(\"http://example.com/cat.jpg\", \"kitten.jpg\"), (\"http://example.com/dog.jpg\", \"doge.jpg\")])\n```\n\n**Returns:** None.\n\n</details>\n\n\n## download3()\n<details>\n\n**Description:**\nTakes a list of URLs, makes 1 HTTP GET Download for each URL of the list.\nIt will Retry again and again in loop until the file is downloaded or `tries` is `0`, whatever happens first.\nIf all retries have failed and `tries` is `0` it will error out.\n\n**Arguments:**\n- `list_of_files` list of tuples, tuples must be 2 items long, first item is URL and second item is filename.\nThe remote URL, string type, required, must not be empty string, is the first item on the tuple.\nThe local filename, string type, required, must not be empty string, can be full path, can be relative path, must include file extension.\n- `delay` Delay between a download and the next one, MicroSeconds precision (1000 = 1 Second), integer type, optional, defaults to `0`, must be a positive integer value.\n- `tries` how many Retries to try, positive integer type, optional, defaults to `9`, must be a positive integer value.\n- `backoff` Back-Off between retries, positive integer type, optional, defaults to `2`, must be a positive integer value.\n- `jitter` Jitter applied to the Back-Off between retries (Modulo math operation), positive integer type, optional, defaults to `2`, must be a positive integer value.\n- `verbose` be Verbose, bool type, optional, defaults to `True`.\n\n**Returns:** None.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.download3(\n  [(\"http://INVALID/cat.jpg\", \"kitten.jpg\"), (\"http://INVALID/dog.jpg\", \"doge.jpg\")],\n  delay = 1, tries = 9, backoff = 2, jitter = 2, verbose = True,\n)\n```\n\nExamples of Failed download output (intended):\n\n```console\n$ python3 example_fail_all_retry.py\n\nRetry: 3 of 3\n(url: \"http://NONEXISTENT\", filename: \"a.json\")\nNo such file or directory\nAdditional info: \"Name or service not known\"\nRetrying in 64 microseconds...\nRetry: 2 of 3\n(url: \"http://NONEXISTENT\", filename: \"a.json\")\nNo such file or directory\nAdditional info: \"Name or service not known\"\nRetrying in 128 microseconds (Warning: This is the last Retry!).\nRetry: 1 of 3\n(url: \"http://NONEXISTENT\", filename: \"a.json\")\nNo such file or directory\nAdditional info: \"Name or service not known\"\nRetrying in 256 microseconds (Warning: This is the last Retry!).\nTraceback (most recent call last):\n  File \"example_fail_all_retry.py\", line 3, in <module>\n    downloader.download3()\n  ...\n\n$\n```\n\n</details>\n\n\n## set_headers()\n<details>\n\n**Description:**\nSet the HTTP Headers from the arguments.\n\n**Arguments:**\n- `headers` HTTP Headers, list type, required,\na list of tuples, tuples must be 2 items long,\nmust not be empty list, must not be empty tuple,\nthe first item of the tuple is the key and second item of the tuple is value,\nkeys must not be empty string, values can be empty string, both must the stripped.\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.set_headers(headers = [(\"key\", \"value\")])\n```\n\n```python\nimport faster_than_requests as requests\nrequests.set_headers([(\"key0\", \"value0\"), (\"key1\", \"value1\")])\n```\n\n```python\nimport faster_than_requests as requests\nrequests.set_headers([(\"content-type\", \"text/plain\"), (\"dnt\", \"1\")])\n```\n\n**Returns:** None.\n\n</details>\n\n\n\n## requests()\n<details>\n\n**Description:**\nLow level API of Requests with everything available as argument to build a detailed custom HTTP request.\n\n**Arguments:**\n- `url` the remote URL, string type, required, must not be empty string.\n- `body` the Body data, string type, required, can be empty string.\n- `http_method` HTTP method, string type, required, must not be empty string, values can be `\"GET\"`, `\"POST\"`, etc.\n- `debugs` Debug mode, bool type, required, default is `False`.\n- `http_headers` HTTP Headers, list type, required,\na list of tuples, tuples must be 2 items long,\nmust not be empty list, must not be empty tuple,\nthe first item of the tuple is the key and second item of the tuple is value,\nkeys must not be empty string, values can be empty string, both must the stripped.\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n## requests2()\n<details>\n\n**Description:**\nLow level API of Requests with everything available as argument with extra options to build a detailed custom HTTP request.\nIts like `request()` function but re-imagined with different features.\nMay be slower than `request()`.\n\n**Arguments:**\n- `url` Remote URL, string type, required, must not be empty string.\n- `body` the Body data, string type, required, can be empty string.\n- `proxyUrl` Full URL of your Network Proxy, string type, required, can be empty string.\n- `proxyAuth` Auth of your Network Proxy if any, string type, required, can be empty string.\n- `userAgent` User Agent to use for the requests, string type, required, can be empty string.\n- `timeout` Timeout, integer type, Milliseconds precision, must be non-zero positive integer, can be `-1`.\n- `maxRedirects` Maximum Redirects, integer type, must be non-zero positive integer, can be `1`.\n- `http_method` HTTP method, string type, required, must not be empty string, values can be `\"GET\"`, `\"POST\"`, etc.\n- `http_headers` HTTP Headers, list type, required,\na list of tuples, tuples must be 2 items long,\nmust not be empty list, must not be empty tuple,\nthe first item of the tuple is the key and second item of the tuple is value,\nkeys must not be empty string, values can be empty string, both must the stripped.\n\n**Returns:**\nResponse, `dict` type, values of the dict are string type,\nvalues of the dict can be empty string, but keys are always consistent.\n\n</details>\n\n\n\n## debugs()\n<details>\n\n**Description:**\nDebug the internal Configuration of the library, takes no arguments, returns nothing,\nprints the pretty-printed human-friendly multi-line JSON Configuration to standard output terminal.\n\n\nExamples:\n\n```python\nimport faster_than_requests as requests\nrequests.debugs()\n```\n\n**Arguments:** None.\n\n**Returns:** None.\n\n</details>\n\n\n\n## tuples2json()\n<details>\n\n**Description:**\nConvert Tuples to JSON, this is a UX improvement just for convenience, this is 100% optional,\nreturns the Tuples converted to Minified computer-friendly single-line JSON.\n\n**Arguments:**\n- `tuples` A list containing Tuples, list type, required,\na list of tuples, tuples must be 2 items long,\nmust not be empty list, must not be empty tuple,\nthe first item of the tuple is the key and second item of the tuple is value,\nkeys must not be empty string, values can be empty string, both must the stripped.\n\n**Returns:** JSON, string type.\n\n</details>\n\n\n# How to set Timeout\n\nSet Timeout by changing the environment variable `REQUESTS_TIMEOUT`, `int` type, must be a non-zero positive value, milliseconds precision, `1000` is `1` Second, can be `-1`.\nThis is 100% Optional, this is provided as Extra feature.\n\nExamples:\n\n```bash\n$ export REQUESTS_TIMEOUT = 42000\n$ # This is the Bash command line terminal!.\n```\n\n\n# How to set Max Redirects\n\nSet Max Redirects by changing the environment variable `REQUESTS_MAXREDIRECTS`, `int` type, must be a zero or positive value, can be `1`, can be `0`.\nThis is 100% Optional, this is provided as Extra feature.\n\nExamples:\n\n```bash\n$ export REQUESTS_MAXREDIRECTS = 3\n$ # This is the Bash command line terminal!.\n```\n\n\n# How to set User Agent\n\nSet User Agent by changing the environment variable `REQUESTS_USERAGENT`, `str` type, can be empty string, can be `\"\"`.\nThis is 100% Optional, this is provided as Extra feature.\n\nExamples:\n\n```bash\n$ export REQUESTS_USERAGENT = \"\"\n$ # This is the Bash command line terminal!.\n```\n\n\n# How to set Proxy\n\nSet Proxy by changing the environment variable `HTTPS_PROXY` or ,`HTTP_PROXY`, `str` type, empty string is No proxy.\n\nSet Proxy Authentication by changing the environment variable `HTTPS_PROXY_AUTH` or ,`HTTP_PROXY_AUTH`, `str` type, empty string is No Auth.\nThis is 100% Optional, this is provided as Extra feature.\n\nExamples:\n\n```bash\n$ export HTTPS_PROXY = \"http://yourProxyUrl:8080\"\n$ # This is the Bash command line terminal!.\n```\n\n\n# How to set Debug Mode\n\nSet Debug Mode by changing the environment variable `REQUESTS_DEBUG`, `bool` type, can be empty string,\nDebug Mode prints progress in real time each second on the terminal as JSON string, Debug Mode is slow.\nThis is 100% Optional, this is provided as Extra feature.\n\nExamples:\n\n```bash\n$ export REQUESTS_DEBUG = \"true\"\n$ # This is the Bash command line terminal!.\n```\n\n\n[**For more Examples check the Examples and Tests.**](https://github.com/juancarlospaco/faster-than-requests/blob/master/examples/example.py)\n\nInstead of having a pair of functions with a lot of arguments that you should provide to make it work,\nwe have tiny functions with very few arguments that do one thing and do it as fast as possible.\n\nA lot of functions are oriented to Data Science, Big Data, Open Data, Web Scrapping, working with HTTP REST JSON APIs.\n\n\n# Install\n\n- `pip install faster_than_requests`\n\n\n# Docker\n\n- Make a quick test drive on Docker!.\n\n```bash\n$ ./build-docker.sh\n$ ./run-docker.sh\n$ ./server4benchmarks &  # Inside Docker.\n$ python3 benchmark.py   # Inside Docker.\n```\n\n\n# Dependencies\n\n- **None**\n\n\n# Platforms\n\n- \u2705 Linux\n- \u2705 Windows\n- \u2705 Mac\n- \u2705 Android\n- \u2705 Raspberry Pi\n- \u2705 BSD\n\n\n# Extras\n\nMore Faster Libraries...\n\n- https://github.com/juancarlospaco/faster-than-csv#faster-than-csv\n- https://github.com/juancarlospaco/faster-than-walk#faster-than-walk\n- We want to make Open Source faster, better, stronger.\n\n\n# Requisites\n\n- Python 3.\n- GCC.\n- 64 Bit.\n\n\n# Windows\n\n- Documentation assumes experience with Git, GitHub, cmd, Compiled software, PC with Administrator.\n- If installation fails on Windows, just use the Source Code:\n\n![win-compile](https://user-images.githubusercontent.com/1189414/63147831-b8bf6100-bfd5-11e9-9e6e-91d61040f139.png \"Git Clone and Compile on Windows 10 with only Git and Nim installed, just 2 commands!\")\n\n**The only software needed is [Git for Windows](https://github.com/git-for-windows/git/releases/latest) and [Nim](https://github.com/dom96/choosenim#windows).**\n\nReboot after install. Administrator required for install. Everything must be 64Bit.\n\nIf that fails too, dont waste time and go directly for [Docker for Windows.](https://docs.docker.com/docker-for-windows).\n\nFor info about how to install [Git for Windows](https://github.com/git-for-windows/git/releases/latest), read [Git for Windows](https://github.com/git-for-windows/git/releases/latest) Documentation.\n\n[For info about how to install Nim, read Nim Documentation.](https://nim-lang.org/install.html)\n\nFor info about how to install [Docker for Windows.](https://docs.docker.com/docker-for-windows), read [Docker for Windows.](https://docs.docker.com/docker-for-windows) Documentation.\n\n[GitHub Actions Build everything from zero on each push, use it as guidance too.](https://github.com/juancarlospaco/faster-than-requests/actions?query=workflow%3APYTHON)\n\n- Git Clone and Compile on Windows 10 on just 2 commands!.\n- [Alternatively you can try Docker for Windows.](https://docs.docker.com/docker-for-windows)\n- [Alternatively you can try WSL for Windows.](https://docs.microsoft.com/en-us/windows/wsl/about)\n- **The file extension must be `.pyd`, NOT `.dll`. Compile with `-d:ssl` to use HTTPS.**\n\n```\nnimble install nimpy\nnim c -d:ssl -d:danger --app:lib --out:faster_than_requests.pyd faster_than_requests.nim\n```\n\n\n# Stars\n\n![Star faster-than-requests on GitHub](https://starchart.cc/juancarlospaco/faster-than-requests.svg \"Star faster-than-requests on GitHub!\")\n\n\n# Sponsors\n\n- **None. Become a Sponsor and help improve this library with the features you want!.**\n- If you are a company or commercial user we need Sponsors!.\n\n\n# FAQ\n\n- Whats the idea, inspiration, reason, etc ?.\n\n[Feel free to Fork, Clone, Download, Improve, Reimplement, Play with this Open Source. Make it 10 times faster, 10 times smaller.](http://tonsky.me/blog/disenchantment)\n\n- This works with SSL ?.\n\nYes.\n\n- This works without SSL ?.\n\nYes.\n\n- This requires Cython ?.\n\nNo.\n\n- This runs on PyPy ?.\n\nNo.\n\n- This runs on Python2 ?.\n\nI dunno. (Not supported)\n\n- This runs on 32Bit ?.\n\nNo.\n\n- This runs with Clang ?.\n\nNo.\n\n- Where to get help ?.\n\nhttps://github.com/juancarlospaco/faster-than-requests/issues\n\n- How to set the URL ?.\n\n`url=\"http://example.com\"` (1st argument always).\n\n- How to set the HTTP Body ?.\n\n`body=\"my body\"`\n\n- How to set an HTTP Header key=value ?.\n\n[setHeaders()](https://github.com/juancarlospaco/faster-than-requests#setheaders)\n\n- How to set HTTP Proxy ?.\n\n[requests2()](https://github.com/juancarlospaco/faster-than-requests#requests2)\n\nOR\n\n`export https_proxy = \"http://yourProxyUrl:8080\"`\n\n`export http_proxy =  \"http://yourProxyUrl:8080\"`\n\nStandard Linux Bash environment variables for proxy.\n\nIt will be automatically read from the environment variables.\n\n- Whats NDJSON ?.\n\nhttps://github.com/ndjson/ndjson-spec\n\n- How can be faster than PyCurl ?.\n\nI dunno.\n\n- Why use Tuple instead of Dict for HTTP Headers ?.\n\nFor speed performance reasons, `dict` is slower, bigger, heavier and mutable compared to `tuple`.\n\n- Why needs 64Bit ?.\n\nMaybe it works on 32Bit, but is not supported, integer sizes are too small, and performance can be worse.\n\n- Why needs Python 3 ?.\n\nMaybe it works on Python 2, but is not supported, and performance can be worse, we suggest to migrate to Python3.\n\n- Can I wrap the functions on a `try: except:` block ?.\n\nFunctions do not have internal `try: except:` blocks,\nso you can wrap them inside `try: except:` blocks if you need very resilient code.\n\n- PIP fails to install or fails build the wheel ?.\n\nAdd at the end of the PIP install command:\n\n` --isolated --disable-pip-version-check --no-cache-dir --no-binary :all: `\n\nNot my Bug.\n\n- How to Build the project ?.\n\n`build.sh` or `build.nims`\n\n- How to Package the project ?.\n\n`package.sh` or `package.nims`\n\n- This requires Nimble ?.\n\nNo.\n\n- Whats the unit of measurement for speed ?.\n\nUnmmodified raw output of Python `timeit` module.\n\nPlease send Pull Request to Python to improve the output of `timeit`.\n\n- The LoC is a lie, not counting the lines of code of the Compiler ?.\n\nProjects that use Cython wont count the whole Cython on the LoC, so we wont neither.\n\n\n[  \u2b06\ufe0f  \u2b06\ufe0f  \u2b06\ufe0f  \u2b06\ufe0f  ](#faster-than-requests \"Go to top\")", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/juancarlospaco/faster-than-requests/releases", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/juancarlospaco/faster-than-requests#faster-than-requests", "keywords": "requests,pycurl,python3,cpython,speed,cython,performance,scrapy,curl,pycurl,urllib3,requests3,c", "license": "MIT", "maintainer": "Juan Carlos", "maintainer_email": "juancarlospaco@gmail.com", "name": "faster-than-requests", "package_url": "https://pypi.org/project/faster-than-requests/", "platform": "Linux", "project_url": "https://pypi.org/project/faster-than-requests/", "project_urls": {"Bugs": "https://github.com/juancarlospaco/faster-than-requests/issues", "CoC": "https://github.com/juancarlospaco/faster-than-requests/blob/master/CODE_OF_CONDUCT.md", "Docs": "https://github.com/juancarlospaco/faster-than-requests#faster-than-requests", "Download": "https://github.com/juancarlospaco/faster-than-requests/releases", "Homepage": "https://github.com/juancarlospaco/faster-than-requests#faster-than-requests"}, "release_url": "https://pypi.org/project/faster-than-requests/0.9.8/", "requires_dist": null, "requires_python": ">3.7", "summary": "Faster & simpler requests replacement for Python.", "version": "0.9.8"}, "last_serial": 6850080, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "c32bd0844c9d5ea80314050865680c9c", "sha256": "5d4636f9ffc26c811f5b45d4a7b696520d6135be70aeb3010b6a33f442afb3fe"}, "downloads": -1, "filename": "faster_than_requests-0.1.zip", "has_sig": false, "md5_digest": "c32bd0844c9d5ea80314050865680c9c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 249875, "upload_time": "2019-01-04T03:07:03", "upload_time_iso_8601": "2019-01-04T03:07:03.146299Z", "url": "https://files.pythonhosted.org/packages/71/dd/b2160ca61114a35238bf9d5f5ccb88ca759b6d2f9246e6e13f4d263c270d/faster_than_requests-0.1.zip"}], "0.5": [{"comment_text": "", "digests": {"md5": "605e43d822844f17b97f06cbb119b4e8", "sha256": "63c65af9002bb9bc5988e7389260b5aae4a907d1e04be8b0ebbb0bf6db8c61fa"}, "downloads": -1, "filename": "faster_than_requests-0.5.zip", "has_sig": false, "md5_digest": "605e43d822844f17b97f06cbb119b4e8", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6", "size": 271212, "upload_time": "2019-05-18T22:13:27", "upload_time_iso_8601": "2019-05-18T22:13:27.989389Z", "url": "https://files.pythonhosted.org/packages/57/86/c1645359414b586757cdaee3c8baa684c0ab7fc309ed640998d2f94a3aae/faster_than_requests-0.5.zip"}], "0.9.2": [{"comment_text": "", "digests": {"md5": "7ac0a8ae3d34e571396667a9af49c9e1", "sha256": "32709ed55c201aa4712f30225f5226ca8fcc9c33db8f7628179fb237c7952e7b"}, "downloads": -1, "filename": "faster_than_requests-0.9.2.zip", "has_sig": false, "md5_digest": "7ac0a8ae3d34e571396667a9af49c9e1", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6", "size": 373991, "upload_time": "2019-12-01T22:51:03", "upload_time_iso_8601": "2019-12-01T22:51:03.238316Z", "url": "https://files.pythonhosted.org/packages/90/15/72ffe29bf34b40b4b9881a89e0fbbf9d22ea0b493681965fd08ce1fafa7b/faster_than_requests-0.9.2.zip"}], "0.9.6": [{"comment_text": "", "digests": {"md5": "7c37f14b265ee70bc65dd2df0c4f9702", "sha256": "9a4eb658a74278f7ee9bb9dee074020730a5c888d42697f80aaa644b79cbe053"}, "downloads": -1, "filename": "faster_than_requests-0.9.6.zip", "has_sig": false, "md5_digest": "7c37f14b265ee70bc65dd2df0c4f9702", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.6", "size": 410985, "upload_time": "2019-12-24T02:03:11", "upload_time_iso_8601": "2019-12-24T02:03:11.319498Z", "url": "https://files.pythonhosted.org/packages/8a/27/94d6335b0dd7861a7f5383046f550e23d89b6797d919b6899b2396807313/faster_than_requests-0.9.6.zip"}], "0.9.8": [{"comment_text": "", "digests": {"md5": "1a7bf58e46084cd26112acf0176b8f0f", "sha256": "5d0c2c49f7258d8ac944272270d9cf1cb172577f65f894735bfe0fd15c1913ac"}, "downloads": -1, "filename": "faster_than_requests-0.9.8.zip", "has_sig": false, "md5_digest": "1a7bf58e46084cd26112acf0176b8f0f", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.7", "size": 435641, "upload_time": "2020-03-20T14:28:13", "upload_time_iso_8601": "2020-03-20T14:28:13.859284Z", "url": "https://files.pythonhosted.org/packages/c3/a8/3b98e72cdeabe085635ccebe10f25dbb8a938f95bc549afb69ac04fb90a7/faster_than_requests-0.9.8.zip"}]}, "urls": [{"comment_text": "", "digests": {"md5": "1a7bf58e46084cd26112acf0176b8f0f", "sha256": "5d0c2c49f7258d8ac944272270d9cf1cb172577f65f894735bfe0fd15c1913ac"}, "downloads": -1, "filename": "faster_than_requests-0.9.8.zip", "has_sig": false, "md5_digest": "1a7bf58e46084cd26112acf0176b8f0f", "packagetype": "sdist", "python_version": "source", "requires_python": ">3.7", "size": 435641, "upload_time": "2020-03-20T14:28:13", "upload_time_iso_8601": "2020-03-20T14:28:13.859284Z", "url": "https://files.pythonhosted.org/packages/c3/a8/3b98e72cdeabe085635ccebe10f25dbb8a938f95bc549afb69ac04fb90a7/faster_than_requests-0.9.8.zip"}]}