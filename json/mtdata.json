{"info": {"author": "Thamme Gowda", "author_email": "tgowdan@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3 :: Only", "Topic :: Text Processing", "Topic :: Text Processing :: Filters", "Topic :: Text Processing :: General", "Topic :: Text Processing :: Linguistic", "Topic :: Utilities"], "description": "# MTData\nMTData tool is written to reduce the burden of preparing the datasets for machine translation.\nIt provides commandline and python APIs that can be either used as a standalone tool, \nor call it from shell scripts or embed it in python application for preparing MT experiments.\n\nWith this you DON'T have to :\n- Know where the URLs are for data sets: WMT tests and devs for \\[2014, 2015, ... 2020], Paracrawl, \n  Europarl, News Commentary, WikiTitles ...\n- Know how to extract files : .tar, .tar.gz, .tgz, .zip, .gz, ...\n- Know how to parse .tmx, .sgm, .tsv\n- Know if parallel data is in one .tsv file or two sgm files \n- (And more over the time. Create an issue discuss more of such \"you dont have to\" topics)\n\nbecause, [MTData](https://github.com/thammegowda/mtdata) does all the above under the hood.\n\n## Installation\n```bash\n# coming soon to pypi\n# pip install mtdata \n\ngit clone https://github.com/thammegowda/mtdata \ncd mtdata\npip install .  # add \"--editable\" flag for development mode \n\n\n```\n\n## CLI Usage\n- After pip installation, the CLI can be called using `mtdata` command  or `python -m mtdata`\n- There are two sub commands: `list` for listing the datasets, and `get` for getting them   \n### `mtdata list`\n```bash\nmtdata list -h\nusage: mtdata list [-h] [-l LANGS] [-n [NAMES [NAMES ...]]]\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -l LANGS, --langs LANGS\n                        Language pairs; e.g.: de-en\n  -n [NAMES [NAMES ...]], --names [NAMES [NAMES ...]]\n                        Name of dataset set; eg europarl_v9.\n``` \n\n```bash\n# List everything\nmtdata list\n\n# List a lang pair \nmtdata list -l de-en\n\n# List a dataset by name(s)\nmtdata list -n europarl_v9\nmtdata list -n europarl_v9 news_commentary_v14\n\n# list by both language pair and dataset name\nmtdata list -l de-en -n europarl_v9 news_commentary_v14 newstest201{4,5,6,7,8,9}_deen\n```\n\n## `mtdata get`\n```bash\nmtdata get -h\nusage: mtdata get [-h] -l LANGS [-n [NAMES [NAMES ...]]] -o OUT\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -l LANGS, --langs LANGS\n                        Language pairs; e.g.: de-en\n  -n [NAMES [NAMES ...]], --names [NAMES [NAMES ...]]\n                        Name of dataset set; eg europarl_v9.\n  -o OUT, --out OUT     Output directory name\n```\nHere is an example showing collection and preparation of DE-EN datasets. \n```bash\nmtdata get  -l de-en -n europarl_v9 news_commentary_v14 newstest201{4,5,6,7,8,9}_deen -o de-en\n```\n\n## How to extend:\nPlease help grow the datasets by adding missing+new datasets to `index.py` module.\nHere is an example listing europarl-v9 corpus.\n```python\nfrom mtdata.index import entries, Entry\nEUROPARL_v9 = 'http://www.statmt.org/europarl/v9/training/europarl-v9.%s-%s.tsv.gz'\nfor pair in ['de en', 'cs en', 'cs pl', 'es pt', 'fi en', 'lt en']:\n    l1, l2 = pair.split()\n    entries.append(Entry(langs=(l1, l2), name='europarl_v9', url=EUROPARL_v9 % (l1, l2)))\n```\nIf a datset is inside an archive such as `zip` or `tar`\n```python\nfrom mtdata.index import entries, Entry\nwmt_sets = {\n    'newstest2014': [('de', 'en'), ('cs', 'en'), ('fr', 'en'), ('ru', 'en'), ('hi', 'en')],\n    'newsdev2015': [('fi', 'en'), ('en', 'fi')]\n}\nfor set_name, pairs in wmt_sets.items():\n    for l1, l2 in pairs:\n        src = f'dev/{set_name}-{l1}{l2}-src.{l1}.sgm'\n        ref = f'dev/{set_name}-{l1}{l2}-ref.{l2}.sgm'\n        name = f'{set_name}_{l1}{l2}'\n        entries.append(Entry((l1, l2), name=name, filename='wmt20dev.tgz', in_paths=[src, ref],\n                             url='http://data.statmt.org/wmt20/translation-task/dev.tgz'))\n# filename='wmt20dev.tgz' -- is manually set, because url has dev.gz that can be confusing\n# in_paths=[src, ref]  -- listing two sgm files inside the tarball\n```\n\n## Developers:\n- [Thamme Gowda](https://twitter.com/thammegowda) \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/thammegowda/mtdata", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/thammegowda/mtdata", "keywords": "machine translation,datasets,NLP,natural language processing,computational linguistics", "license": "University of Southern California (USC) Restricted License", "maintainer": "", "maintainer_email": "", "name": "mtdata", "package_url": "https://pypi.org/project/mtdata/", "platform": "any", "project_url": "https://pypi.org/project/mtdata/", "project_urls": {"Download": "https://github.com/thammegowda/mtdata", "Homepage": "https://github.com/thammegowda/mtdata"}, "release_url": "https://pypi.org/project/mtdata/0.1/", "requires_dist": ["wget"], "requires_python": ">=3.7", "summary": "mtdata is a tool to download datasets for machine translation", "version": "0.1"}, "last_serial": 6959901, "releases": {"0.1": [{"comment_text": "", "digests": {"md5": "87fba2db26994a93890bdc6e62e9e115", "sha256": "0e0eef2f2627eedd29e7ca38010670f0a51f9355de46720fab0931498957ce09"}, "downloads": -1, "filename": "mtdata-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "87fba2db26994a93890bdc6e62e9e115", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 17858, "upload_time": "2020-04-06T08:01:37", "upload_time_iso_8601": "2020-04-06T08:01:37.068632Z", "url": "https://files.pythonhosted.org/packages/69/b3/fa0338849183beb610e0b9fda98f7c0974a4bb01bd9684b2fd858322d3c3/mtdata-0.1-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9f93911d75927b507507a38c68e1d9fd", "sha256": "4530d954305ffe24e8ede4409421da4dfa1f006fe1b2980777d0826a456e2ee2"}, "downloads": -1, "filename": "mtdata-0.1.tar.gz", "has_sig": false, "md5_digest": "9f93911d75927b507507a38c68e1d9fd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 11768, "upload_time": "2020-04-06T08:01:39", "upload_time_iso_8601": "2020-04-06T08:01:39.517153Z", "url": "https://files.pythonhosted.org/packages/8d/5b/6f3bed524faaa9992fd7820eabfcfb0a38df83e4db527308d706d9218786/mtdata-0.1.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "87fba2db26994a93890bdc6e62e9e115", "sha256": "0e0eef2f2627eedd29e7ca38010670f0a51f9355de46720fab0931498957ce09"}, "downloads": -1, "filename": "mtdata-0.1-py3-none-any.whl", "has_sig": false, "md5_digest": "87fba2db26994a93890bdc6e62e9e115", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 17858, "upload_time": "2020-04-06T08:01:37", "upload_time_iso_8601": "2020-04-06T08:01:37.068632Z", "url": "https://files.pythonhosted.org/packages/69/b3/fa0338849183beb610e0b9fda98f7c0974a4bb01bd9684b2fd858322d3c3/mtdata-0.1-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9f93911d75927b507507a38c68e1d9fd", "sha256": "4530d954305ffe24e8ede4409421da4dfa1f006fe1b2980777d0826a456e2ee2"}, "downloads": -1, "filename": "mtdata-0.1.tar.gz", "has_sig": false, "md5_digest": "9f93911d75927b507507a38c68e1d9fd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 11768, "upload_time": "2020-04-06T08:01:39", "upload_time_iso_8601": "2020-04-06T08:01:39.517153Z", "url": "https://files.pythonhosted.org/packages/8d/5b/6f3bed524faaa9992fd7820eabfcfb0a38df83e4db527308d706d9218786/mtdata-0.1.tar.gz"}]}