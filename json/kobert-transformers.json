{"info": {"author": "Jangwon Park", "author_email": "adieujw@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.6", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# DistilKoBERT\n\nDistillation of KoBERT (`SKTBrain KoBERT` \uacbd\ub7c9\ud654)\n\n**January 27th, 2020 - Update**: 10GB\uc758 Corpus\ub97c \uac00\uc9c0\uace0 \uc0c8\ub85c \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4. Subtask\uc5d0\uc11c \uc131\ub2a5\uc774 \uc18c\ud3ed \uc0c1\uc2b9\ud588\uc2b5\ub2c8\ub2e4.\n\n## Pretraining DistilKoBERT\n\n- \uae30\uc874\uc758 12 layer\ub97c **3 layer**\ub85c \uc904\uc600\uc73c\uba70, \uae30\ud0c0 configuration\uc740 kobert\ub97c \uadf8\ub300\ub85c \ub530\ub790\uc2b5\ub2c8\ub2e4.\n  - [\uc6d0 \ub17c\ubb38](https://arxiv.org/abs/1910.01108)\uc740 6 layer\ub97c \ucc44\ud0dd\ud558\uc600\uc2b5\ub2c8\ub2e4.\n- Layer \ucd08\uae30\ud654\uc758 \uacbd\uc6b0 \uae30\uc874 KoBERT\uc758 1, 5, 9\ubc88\uc9f8 layer \uac12\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n- Pretraining Corpus\ub294 \ud55c\uad6d\uc5b4 \uc704\ud0a4, \ub098\ubb34\uc704\ud0a4, \ub274\uc2a4 \ub4f1 \uc57d 10GB\uc758 \ub370\uc774\ud130\ub97c \uc0ac\uc6a9\ud588\uc73c\uba70, 3 epoch \ud559\uc2b5\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\n## KoBERT / DistilKoBERT for transformers library\n\n- \uae30\uc874\uc758 KoBERT\ub97c transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uace7\ubc14\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ub9de\ucdc4\uc2b5\ub2c8\ub2e4.\n  - transformers v2.2.2\ubd80\ud130 \uac1c\uc778\uc774 \ub9cc\ub4e0 \ubaa8\ub378\uc744 transformers\ub97c \ud1b5\ud574 \uc9c1\uc811 \uc5c5\ub85c\ub4dc/\ub2e4\uc6b4\ub85c\ub4dc\ud558\uc5ec \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4\n  - DistilKoBERT \uc5ed\uc2dc transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc5d0\uc11c \uace7\ubc14\ub85c \ub2e4\uc6b4 \ubc1b\uc544\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n### Dependencies\n\n- torch>=1.1.0\n- transformers>=2.2.2\n- sentencepiece>=0.1.82\n\n### How to Use\n\n```python\n>>> from transformers import BertModel, DistilBertModel\n>>> bert_model = BertModel.from_pretrained('monologg/kobert')\n>>> distilbert_model = DistilBertModel.from_pretrained('monologg/distilkobert')\n```\n\n- Tokenizer\ub97c \uc0ac\uc6a9\ud558\ub824\uba74, \ub8e8\ud2b8 \ub514\ub809\ud1a0\ub9ac\uc758 `tokenization_kobert.py` \ud30c\uc77c\uc744 \ubcf5\uc0ac\ud55c \ud6c4, `KoBertTokenizer`\ub97c \uc784\ud3ec\ud2b8\ud558\uba74 \ub429\ub2c8\ub2e4.\n  - KoBERT\uc640 DistilKoBERT \ubaa8\ub450 \ub3d9\uc77c\ud55c \ud1a0\ud06c\ub098\uc774\uc800\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n  - **\uae30\uc874 KoBERT\uc758 \uacbd\uc6b0 Special Token\uc774 \uc81c\ub300\ub85c \ubd84\ub9ac\ub418\uc9c0 \uc54a\ub294 \uc774\uc288\uac00 \uc788\uc5b4\uc11c \ud574\ub2f9 \ubd80\ubd84\uc744 \uc218\uc815\ud558\uc5ec \ubc18\uc601\ud558\uc600\uc2b5\ub2c8\ub2e4.** ([Issue link](https://github.com/SKTBrain/KoBERT/issues/11))\n\n```python\n>>> from tokenization_kobert import KoBertTokenizer\n>>> tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert\ub3c4 \ub3d9\uc77c\n>>> tokenizer.tokenize(\"[CLS] \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. [SEP]\")\n['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]']\n>>> tokenizer.convert_tokens_to_ids(['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]'])\n[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n```\n\n## What is different between BERT and DistilBERT\n\n- DistilBert\ub294 \uae30\uc874\uc758 Bert\uc640 \ub2ec\ub9ac token-type embedding\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n  - Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 DistilBertModel\uc744 \uc0ac\uc6a9\ud560 \ub54c \uae30\uc874 BertModel \uacfc \ub2ec\ub9ac `token_type_ids`\ub97c \ub123\uc744 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n- \ub610\ud55c DistilBert\ub294 pooler\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\n\n  - \uace0\ub85c \uae30\uc874 BertModel\uc758 \uacbd\uc6b0 forward\uc758 return \uac12\uc73c\ub85c `sequence_output, pooled_output, (hidden_states), (attentions)`\uc744 \ubf51\uc544\ub0b4\uc9c0\ub9cc, DistilBertModel\uc758 \uacbd\uc6b0 `sequence_output, (hidden_states), (attentions)`\ub97c \ubf51\uc544\ub0c5\ub2c8\ub2e4.\n  - DistilBert\uc5d0\uc11c `[CLS]` \ud1a0\ud070\uc744 \ubf51\uc544\ub0b4\ub824\uba74 `sequence_output[0][:, 0]`\ub97c \uc801\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4.\n\n## Kobert-Transformers python library\n\n[![Release](https://img.shields.io/badge/release-v0.2.1-green)](https://pypi.org/project/kobert-transformers/)\n[![Downloads](https://pepy.tech/badge/kobert-transformers)](https://pepy.tech/project/kobert-transformers)\n[![license](https://img.shields.io/badge/license-Apache%202.0-red)](https://github.com/monologg/DistilKoBERT/blob/master/LICENSE)\n\n- tokenization_kobert.py\ub97c \ub7a9\ud551\ud55c \ud30c\uc774\uc36c \ub77c\uc774\ube0c\ub7ec\ub9ac\n- KoBERT, DistilKoBERT\ub97c Huggingface Transformers \ub77c\uc774\ube0c\ub7ec\ub9ac \ud615\ud0dc\ub85c \uc784\ud3ec\ud2b8\n\n### Install Kobert-Transformers\n\n```bash\n$ pip3 install kobert-transformers\n```\n\n### How to Use\n\n```python\n>>> import torch\n>>> from kobert_transformers import get_distilkobert_model, get_kobert_model\n\n>>> model = get_distilkobert_model()\n>>> input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n>>> attention_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n>>> last_layer_hidden_state, _ = model(input_ids, attention_mask)\n>>> last_layer_hidden_state\ntensor([[[-0.4294,  0.1849,  0.2622,  ..., -0.8856, -0.0617, -0.0664],\n         [ 0.0580,  0.2065,  0.1131,  ..., -0.9954, -1.2588, -0.1635],\n         [-0.3945,  0.0641, -0.2223,  ..., -0.9819, -0.9723,  0.0929]],\n\n        [[ 0.1698, -0.2389, -0.0153,  ..., -0.0329, -0.0892, -0.0428],\n         [ 0.1348, -0.5269, -0.2861,  ..., -0.6471, -0.6776, -0.2948],\n         [ 0.0655, -0.4104, -0.0467,  ..., -0.5906, -0.6362, -0.0361]]],\n       grad_fn=<AddcmulBackward>)\n```\n\n```python\n>>> from kobert_transformers import get_tokenizer\n>>> tokenizer = get_tokenizer()\n>>> tokenizer.tokenize(\"[CLS] \ud55c\uad6d\uc5b4 \ubaa8\ub378\uc744 \uacf5\uc720\ud569\ub2c8\ub2e4. [SEP]\")\n['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]']\n>>> tokenizer.convert_tokens_to_ids(['[CLS]', '\u2581\ud55c\uad6d', '\uc5b4', '\u2581\ubaa8\ub378', '\uc744', '\u2581\uacf5\uc720', '\ud569\ub2c8\ub2e4', '.', '[SEP]'])\n[2, 4958, 6855, 2046, 7088, 1050, 7843, 54, 3]\n```\n\n## Result on Sub-task\n\n|                     | KoBERT | DistilKoBERT | Bert-multilingual |\n| ------------------- | ------ | ------------ | ----------------- |\n| Model Size (MB)     | 351    | 108          | 681               |\n| **NSMC** (acc)      | 89.63  | 88.41        | 87.07             |\n| **Naver NER** (F1)  | 84.23  | 82.14        | 81.78             |\n| **KorQuAD** (EM/F1) | TBD    | TBD          | 77.04/87.85       |\n\n- NSMC (Naver Sentiment Movie Corpus) ([Implementation of KoBERT-nsmc](https://github.com/monologg/KoBERT-nsmc))\n- Naver NER (NER task on Naver NLP Challenge 2018) ([Implementation of KoBERT-NER](https://github.com/monologg/KoBERT-NER))\n\n## Reference\n\n- [KoBERT](https://github.com/SKTBrain/KoBERT)\n- [Huggingface Transformers](https://github.com/huggingface/transformers)\n- [DistilBERT Github](https://github.com/huggingface/transformers/blob/master/examples/distillation/README.md)\n- [DistilBERT Paper](https://arxiv.org/abs/1910.01108)\n- [\ub525\ub7ec\ub2dd\uc73c\ub85c \ub3d9\ub124\uc0dd\ud65c \uac8c\uc2dc\uae00 \ud544\ud130\ub9c1\ud558\uae30](https://medium.com/daangn/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9C%BC%EB%A1%9C-%EB%8F%99%EB%84%A4%EC%83%9D%ED%99%9C-%EA%B2%8C%EC%8B%9C%EA%B8%80-%ED%95%84%ED%84%B0%EB%A7%81%ED%95%98%EA%B8%B0-263cfe4bc58d)\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/monologg/DistilKoBERT", "keywords": "distilkobert kobert bert pytorch transformers lightweight", "license": "Apache-2.0", "maintainer": "", "maintainer_email": "", "name": "kobert-transformers", "package_url": "https://pypi.org/project/kobert-transformers/", "platform": "", "project_url": "https://pypi.org/project/kobert-transformers/", "project_urls": {"Homepage": "https://github.com/monologg/DistilKoBERT"}, "release_url": "https://pypi.org/project/kobert-transformers/0.3.0/", "requires_dist": ["torch (>=1.1.0)", "transformers (>=2.2.2)"], "requires_python": ">=3", "summary": "Transformers library for KoBERT, DistilKoBERT", "version": "0.3.0"}, "last_serial": 7008764, "releases": {"0.3.0": [{"comment_text": "", "digests": {"md5": "23ba06e19f4ef512ece6de6f78797331", "sha256": "ff462640acb7b4a3637d2f2afb8bfd842c59bbc3053a4ca040bc3ef12f697d33"}, "downloads": -1, "filename": "kobert_transformers-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "23ba06e19f4ef512ece6de6f78797331", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8480, "upload_time": "2020-02-12T17:24:48", "upload_time_iso_8601": "2020-02-12T17:24:48.157686Z", "url": "https://files.pythonhosted.org/packages/d1/e4/2090062beab003460f7109af611aa330359e60ad676ae350486e55c13806/kobert_transformers-0.3.0-py3-none-any.whl"}]}, "urls": [{"comment_text": "", "digests": {"md5": "23ba06e19f4ef512ece6de6f78797331", "sha256": "ff462640acb7b4a3637d2f2afb8bfd842c59bbc3053a4ca040bc3ef12f697d33"}, "downloads": -1, "filename": "kobert_transformers-0.3.0-py3-none-any.whl", "has_sig": false, "md5_digest": "23ba06e19f4ef512ece6de6f78797331", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 8480, "upload_time": "2020-02-12T17:24:48", "upload_time_iso_8601": "2020-02-12T17:24:48.157686Z", "url": "https://files.pythonhosted.org/packages/d1/e4/2090062beab003460f7109af611aa330359e60ad676ae350486e55c13806/kobert_transformers-0.3.0-py3-none-any.whl"}]}