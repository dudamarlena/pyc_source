{"info": {"author": "Christopher Flynn", "author_email": "crf204@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Natural Language :: English", "Programming Language :: Python :: 2", "Programming Language :: Python :: 2.7", "Programming Language :: Python :: 3", "Programming Language :: Python :: 3.5", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Programming Language :: Python :: Implementation :: CPython", "Topic :: Database", "Topic :: Database :: Front-Ends"], "description": "databricks-dbapi\n================\n\n|pypi| |pyversions|\n\n.. |pypi| image:: https://img.shields.io/pypi/v/databricks-dbapi.svg\n    :target: https://pypi.python.org/pypi/databricks-dbapi\n\n.. |pyversions| image:: https://img.shields.io/pypi/pyversions/databricks-dbapi.svg\n    :target: https://pypi.python.org/pypi/databricks-dbapi\n\nA thin wrapper around `pyhive <https://github.com/dropbox/PyHive>`_ for creating a `DBAPI <https://www.python.org/dev/peps/pep-0249/>`_ connection to an interactive Databricks cluster.\n\nAlso provides a SQLAlchemy Dialect for Databricks interactive clusters.\n\nInstallation\n------------\n\nInstall using pip:\n\n.. code-block:: bash\n\n    pip install databricks-dbapi\n\n\nFor SQLAlchemy support install with:\n\n.. code-block:: bash\n\n    pip install databricks-dbapi[sqlalchemy]\n\nUsage\n-----\n\nThe ``connect()`` function returns a ``pyhive`` Hive connection object, which internally wraps a ``thrift`` connection.\n\nUsing a Databricks API token (recommended):\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n\n\n    token = os.environ[\"DATABRICKS_TOKEN\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    cluster = os.environ[\"DATABRICKS_CLUSTER\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        cluster=cluster,\n        token=token,\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\")\n\n    print(cursor.fetchone())\n    print(cursor.fetchall())\n\n\nUsing your username and password (not recommended):\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n\n\n    user = os.environ[\"DATABRICKS_USER\"]\n    password = os.environ[\"DATABRICKS_PASSWORD\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    cluster = os.environ[\"DATABRICKS_CLUSTER\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        cluster=cluster,\n        user=user,\n        password=password\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\")\n\n    print(cursor.fetchone())\n    print(cursor.fetchall())\n\n\nConnecting on Azure platform, or with ``http_path``:\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n\n\n    token = os.environ[\"DATABRICKS_TOKEN\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    http_path = os.environ[\"DATABRICKS_HTTP_PATH\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        http_path=http_path,\n        token=token,\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\")\n\n    print(cursor.fetchone())\n    print(cursor.fetchall())\n\n\nThe ``pyhive`` connection also provides async functionality:\n\n.. code-block:: python\n\n    import os\n\n    from databricks_dbapi import databricks\n    from TCLIService.ttypes import TOperationState\n\n\n    token = os.environ[\"DATABRICKS_TOKEN\"]\n    host = os.environ[\"DATABRICKS_HOST\"]\n    cluster = os.environ[\"DATABRICKS_CLUSTER\"]\n\n\n    connection = databricks.connect(\n        host=host,\n        cluster=cluster,\n        token=token,\n    )\n    cursor = connection.cursor()\n\n    cursor.execute(\"SELECT * FROM some_table LIMIT 100\", async_=True)\n\n    status = cursor.poll().operationState\n    while status in (TOperationState.INITIALIZED_STATE, TOperationState.RUNNING_STATE):\n        logs = cursor.fetch_logs()\n        for message in logs:\n            print(message)\n\n        # If needed, an asynchronous query can be cancelled at any time with:\n        # cursor.cancel()\n\n        status = cursor.poll().operationState\n\n    print(cursor.fetchall())\n\n\n\nSQLAlchemy\n----------\n\nOnce the ``databricks-dbapi`` package is installed, the ``databricks+pyhive`` dialect/driver will be registered to SQLAlchemy. Fill in the required information when passing the engine URL.\n\n.. code-block:: python\n\n    from sqlalchemy import *\n    from sqlalchemy.engine import create_engine\n    from sqlalchemy.schema import *\n\n\n    # Standard Databricks with user + password\n    # provide user, password, company name for url, database name, cluster name\n    engine = create_engine(\n        \"databricks+pyhive://<user>:<password>@<companyname>.cloud.databricks.com:443/<database>\",\n        connect_args={\"cluster\": \"<cluster>\"}\n    )\n\n    # Standard Databricks with token\n    # provide token, company name for url, database name, cluster name\n    engine = create_engine(\n        \"databricks+pyhive://token:<databricks_token>@<companyname>.cloud.databricks.com:443/<database>\",\n        connect_args={\"cluster\": \"<cluster>\"}\n    )\n\n    # Azure Databricks with user + password\n    # provide user, password, region for url, database name, http_path (with cluster name)\n    engine = create_engine(\n        \"databricks+pyhive://<user>:<password>@<region>.azuredatabricks.net:443/<database>\",\n        connect_args={\"http_path\": \"<azure_databricks_http_path>\"}\n    )\n\n    # Azure Databricks with token\n    # provide token, region for url, database name, http_path (with cluster name)\n    engine = create_engine(\n        \"databricks+pyhive://token:<databrickstoken>@<region>.azuredatabricks.net:443/<database>\",\n        connect_args={\"http_path\": \"<azure_databricks_http_path>\"}\n    )\n\n\n    logs = Table(\"my_table\", MetaData(bind=engine), autoload=True)\n    print(select([func.count(\"*\")], from_obj=logs).scalar())\n\n\nRefer to the following documentation for more details on hostname, cluster name, and http path:\n\n* `Databricks <https://docs.databricks.com/user-guide/bi/jdbc-odbc-bi.html>`_\n* `Azure Databricks <https://docs.azuredatabricks.net/user-guide/bi/jdbc-odbc-bi.html>`_\n\n\nRelated\n-------\n\n* `pyhive <https://github.com/dropbox/PyHive>`_\n* `thrift <https://github.com/apache/thrift/tree/master/lib/py>`_\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/crflynn/databricks-dbapi", "keywords": "databricks,hive,dbapi,sqlalchemy,dialect", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "databricks-dbapi", "package_url": "https://pypi.org/project/databricks-dbapi/", "platform": "", "project_url": "https://pypi.org/project/databricks-dbapi/", "project_urls": {"Documentation": "https://github.com/crflynn/databricks-dbapi", "Homepage": "https://github.com/crflynn/databricks-dbapi", "Repository": "https://github.com/crflynn/databricks-dbapi"}, "release_url": "https://pypi.org/project/databricks-dbapi/0.3.0/", "requires_dist": ["pyhive[hive] (>=0.6.1,<0.7.0)", "sqlalchemy (>=1.3,<2.0); extra == \"sqlalchemy\""], "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "summary": "A DBAPI 2.0 interface and SQLAlchemy dialect for Databricks interactive clusters.", "version": "0.3.0"}, "last_serial": 5674807, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "70a8b72f48e4de74359e3408784fb1d2", "sha256": "0539bd1613b4e988d80a1107fe41f2ef82092ea9588cf7b0337e7a9c293eaea9"}, "downloads": -1, "filename": "databricks_dbapi-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "70a8b72f48e4de74359e3408784fb1d2", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 3260, "upload_time": "2018-09-11T01:45:44", "upload_time_iso_8601": "2018-09-11T01:45:44.393108Z", "url": "https://files.pythonhosted.org/packages/59/5b/9184078aa1cc3eeb59178667493399acb0b5cefcbae256ccbd85b81dd6bb/databricks_dbapi-0.1.0-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "17c4d0bfafbfd23eba6862f778b7428f", "sha256": "fe0be07c78b23793cecac9cfc321686bbb78c27433479d7b0b37ab8510b4c109"}, "downloads": -1, "filename": "databricks_dbapi-0.1.0.tar.gz", "has_sig": false, "md5_digest": "17c4d0bfafbfd23eba6862f778b7428f", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3682, "upload_time": "2018-09-11T01:45:45", "upload_time_iso_8601": "2018-09-11T01:45:45.985536Z", "url": "https://files.pythonhosted.org/packages/26/14/a2bd4e1a6d4a78d89c8bc893967f5495265244b24c228bf1c4960f8352c3/databricks_dbapi-0.1.0.tar.gz"}], "0.1.1": [{"comment_text": "", "digests": {"md5": "8c1d8d08bf865998da4c0dbc9a118559", "sha256": "e8cd6d4f265dc1934bf19caec35c8f8361512de5edb2abe625603f8b58790166"}, "downloads": -1, "filename": "databricks_dbapi-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "8c1d8d08bf865998da4c0dbc9a118559", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 3291, "upload_time": "2018-10-11T01:29:35", "upload_time_iso_8601": "2018-10-11T01:29:35.203235Z", "url": "https://files.pythonhosted.org/packages/4c/32/2a7e255371509a4aea6ababb3f95b725b47716fea2df1e2963dba0b72845/databricks_dbapi-0.1.1-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "0f9387b97115998e003c50829ba3273e", "sha256": "20fbe2ed1eb95179de8543273dba596c58cd5505c2494993b0df6b5ad6b83688"}, "downloads": -1, "filename": "databricks_dbapi-0.1.1.tar.gz", "has_sig": false, "md5_digest": "0f9387b97115998e003c50829ba3273e", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 3710, "upload_time": "2018-10-11T01:29:36", "upload_time_iso_8601": "2018-10-11T01:29:36.665582Z", "url": "https://files.pythonhosted.org/packages/74/56/e345f69133c40235f081c7d89f54bece617e740d5ea7ca5bfc1ce366aea0/databricks_dbapi-0.1.1.tar.gz"}], "0.2.0": [{"comment_text": "", "digests": {"md5": "c618a052cdbb074204256f5fc2a8cc21", "sha256": "59ab8930f308cf13e7f5645b7deeb432f8ed2cd374127a86497defc2c49714f5"}, "downloads": -1, "filename": "databricks_dbapi-0.2.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "c618a052cdbb074204256f5fc2a8cc21", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 3767, "upload_time": "2018-12-13T23:16:15", "upload_time_iso_8601": "2018-12-13T23:16:15.419079Z", "url": "https://files.pythonhosted.org/packages/75/66/38cccdc14866d22ad4d25207e85d99feb17e20cbe500ba1245d5f1c0ba6d/databricks_dbapi-0.2.0-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "bba23dc0007c1d6d65902196e16dce3c", "sha256": "f2ac2e9c8765a6d13200264016c5511fb542308bc15bf2cc9d6d8f58cb7b7f1f"}, "downloads": -1, "filename": "databricks_dbapi-0.2.0.tar.gz", "has_sig": false, "md5_digest": "bba23dc0007c1d6d65902196e16dce3c", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 4208, "upload_time": "2018-12-13T23:16:17", "upload_time_iso_8601": "2018-12-13T23:16:17.108981Z", "url": "https://files.pythonhosted.org/packages/07/7f/4aadd858938962ff02b2109d3a52f631afca5beb011333a9a8c5ee6c2400/databricks_dbapi-0.2.0.tar.gz"}], "0.3.0": [{"comment_text": "", "digests": {"md5": "5091afe383a7e7428b1650413b4af115", "sha256": "1a6567fe5235a233ec05024fc537a557ea294c4ceee8d058497a06a4e2a07f4d"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5091afe383a7e7428b1650413b4af115", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6915, "upload_time": "2019-08-14T05:05:17", "upload_time_iso_8601": "2019-08-14T05:05:17.432248Z", "url": "https://files.pythonhosted.org/packages/32/38/4d4271b3bd8cf704468dd728fdbae0e4114494d41a9a60ec9a6bfbe38efb/databricks_dbapi-0.3.0-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "bc131363ac50d83b62c880a4478c0a8f", "sha256": "3c7e529cdeca4004962342d339119e631df1a934f4e7d7da168370f13b1a48c4"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0.tar.gz", "has_sig": false, "md5_digest": "bc131363ac50d83b62c880a4478c0a8f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6157, "upload_time": "2019-08-14T05:05:19", "upload_time_iso_8601": "2019-08-14T05:05:19.344253Z", "url": "https://files.pythonhosted.org/packages/6f/b1/b96f723083f02d201ee680d92fb38877ffa928b7733b5906bedf19e44bd4/databricks_dbapi-0.3.0.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "5091afe383a7e7428b1650413b4af115", "sha256": "1a6567fe5235a233ec05024fc537a557ea294c4ceee8d058497a06a4e2a07f4d"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "5091afe383a7e7428b1650413b4af115", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6915, "upload_time": "2019-08-14T05:05:17", "upload_time_iso_8601": "2019-08-14T05:05:17.432248Z", "url": "https://files.pythonhosted.org/packages/32/38/4d4271b3bd8cf704468dd728fdbae0e4114494d41a9a60ec9a6bfbe38efb/databricks_dbapi-0.3.0-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "bc131363ac50d83b62c880a4478c0a8f", "sha256": "3c7e529cdeca4004962342d339119e631df1a934f4e7d7da168370f13b1a48c4"}, "downloads": -1, "filename": "databricks_dbapi-0.3.0.tar.gz", "has_sig": false, "md5_digest": "bc131363ac50d83b62c880a4478c0a8f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, !=3.4.*", "size": 6157, "upload_time": "2019-08-14T05:05:19", "upload_time_iso_8601": "2019-08-14T05:05:19.344253Z", "url": "https://files.pythonhosted.org/packages/6f/b1/b96f723083f02d201ee680d92fb38877ffa928b7733b5906bedf19e44bd4/databricks_dbapi-0.3.0.tar.gz"}]}