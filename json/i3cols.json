{"info": {"author": "Justin L. Lanfranchi", "author_email": "jll1062@phys.psu.edu", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License"], "description": "# i3cols\n\nConvert IceCube i3 files to columnar Numpy arrays &amp; operate on them.\n\n## Motivation\n\nIceCube .i3 files are formulated for arbitrary event-by-event processing of\n\"events.\"\nThe information that comprises an \"event\" can span multiple frames in the file,\nand the file must be read and written sequentially like linear tape storage\n(i.e., processing requires a finite state machine).\nThis is well-suited to \"online\" and \"real-time\" processing of events.\n\nAdditionally, the IceTray, which is meant to read, process, and produce .i3\nfiles, can process multiple files but is unwaware of file boundaries, despite\nthe fundamental role that \"number of files\" plays in normalizing IceCube Monte\nCarlo simulation data.\n\nBeyond collecting data, performing event splitting, and real-time algorithms,\nanalysis often is most efficient and straightforward to work with specific\nfeatures of events atomically: i.e., in a columnar fashion.\n\nTwo existing options are writing data to ROOT files and HDF5 files. The former\nrequires a learning curve of its own outside of Python/Numpy, though possibly\nthrough projects like [Uproot](https://github.com/scikit-hep/uproot), such files\ncould be more practical to use for Python-based analysis. HDF5 files suffer from\nthe inability to be manipulated in parallel from Python and present an\nunessential layer of complexity around what we actually want and use: Numpy\narrays of data.\n\n**i3cols** allows working with IceCube data in columnar fashion without added\ncomplexity of libraries beyond Numpy, and both the columnar nature and the\nsimplicity of the data representation allows more straightforward as well as\nnew and novel ways of interacting with data which should be more natural and\nefficient for many common use-cases in Python/Numpy:\n\n\n### Basics\n\n1. Apply numpy operations directly to data arrays\n2. Extract data columns to pass to machine learning tools directly or at most\n   with simple indexing; e.g., to extract the azimuthal direction of all\n   particles: `particles[\"dir\"][\"azimuth\"]`.\n3. Memory mapping allows chunked operations and arbitrary reading and/or\n   writing to the same arrays from multiple processes (just make sure the\n   processes don't try to write to the same elements).\n4. New levels of processing entail adding new columns, without the need to\n   duplicate all data that came before. This has the disadvantage that cuts\n   that remove the vast majority of events result in columns that have the same\n   number of rows as the pre-cut. However, the advantage to working this way is\n   that it is trivial to go back to the lowest-level of processing and also to\n   inspect how the cuts affect all variables contained at any level of\n   processing. (A future goal is to also accommodate efficiently storing a\n   subset of rows by creating a \"subscalar\" column that only contains rows\n   where its or another column's \"valid\" array is True.)\n5. Numpy arrays with structured dtypes can be passed directly to Numba for\n   efficient looping with similar performance to compiled C++ code, as Numba is\n   just-in-time (JIT)-compiled to machine code.\n6. There is no dependency upon IceCube software once data has been extracted.\n   This can be seen as a positive (portability) and a negative (IceCube software\n   has been developed, in part, to perform analysis tasks).\n7. If you think of a new item you want to extract from the source i3 files\n   after already performing an extraction, it is _much_ faster and the process\n   only yields the new column (rather than an entirely new file, as is the case\n   with HDF5 extraction).\n8. Basic operations like transferring specific frame items can be achieved with\n   simple UNIX utilities (`cp`, `rsync`, etc.)\n9. A future goal is to implement versioning with each column that is either\n   explicitly accessible (if desired) or transparent (if desired) to users,\n   such that different processing versions can live side-by-side without\n   affecting one another.\n\n\n### Flattening hierarchies for fast analysis without losing hierarchies\n\n1. Source i3 files are invisible to analysis, if you want (the fact that data\n    came from hundreds of thousands of small i3 files does not slow down\n    analysis or grind cluster storage to a halt). If the concatenated arrays\n    are too large to fit into memory, you can operate on arrays in arbitrary\n    chunks of events and/or work directly on the data on disk via Numpy's\n    built-in memory mapping (which is transparent to Numpy operations).\n2. Source i3 files can be explicitly known to analysis if you want (via the\n    Numpy arrays called in i3cols \"category indexes\").\n3. Flattened datastructures allow efficient operations on them. E.g., looking\n    at all pulses is trivial without needing to traverse a hierarchy. But\n    information about the hierarchy is preserved, so operating in that manner\n    is still possible (and still very fast with Numpy and/or Numba).\n\n### Data storage\n\nThere are two kinds of data currently supported:\n\n1. **Scalar data**: One item per event. This includes arbitrarily complex dtypes, including a vector of N of a thing. The only requirement is that every event must also have exactly N of those things. This requires a _data_ array which has one entry per event. \n2. **Vector data**: Zero, one, or more of an item per event. E.g., pulses. This requires both _data_ and _index_ arrays. The index array has one entry per event which indicates the _start_ and _stop_ indices of that event's data. Meanwhile, _data_ can be arbitrarily long.\n\nTo accomodate arbitrary missing or invalid data, there can be an optional _valid_ array alongside the other arrays.\n\nOne **column** consists of the container (either a directory or .npz file) and the set of the above arrays it contains. The directory version of this looks on disk like, for example for _InIcePulses_ (which is vector data) with a valid array:\n```\nInIcePulses/\n  |\n  +-- data.npy\n  |\n  +-- valid.npy\n  |\n  +-- index.npy\n```\nIf the above is compressed, it becomes a single .npz file: `InIcePulses.npz`. Within the file, the arrays can be accessed via their names \"data\", \"valid\", and/or \"index\". Here, we load all arrays that are present into a dictionary:\n```python\nwith np.load(\"InIcePulses.npz\") as npz:\n    array_d = {name: npz.get(name, None) for name in [\"data\", \"valid\", \"index\"]}\n```\nNote there is a convenience function to do this (and load any category indices) for a single item or for an entire directory, transparent to compression: `arrays, category_indexes = i3cols.cols.load(dirpath)`.\n\n#### Unsupported (or awkwardly supported) data types\n\n1. **Mixture of types**: Scalar data where the item in one event has one type while another event (for that same item) has another type; similarly for vector data where dtype changes either within or across events. E.g., if the datatype contains a different-length-per-event vector as one field within the type (while all other parts of the type are constant-length). This can be accommodated (to varying degrees of ineffficiency) by creating a type that contains all fields and just _one_ of the vector type, and then duplicating the fields that remain constant for every value in the vector.\n\n\n## Installation\n\n```\npip install i3cols\n```\n\n### For developers\n\nClone the repository and then perform an editable (`pip install -e`) installation:\n\n```\ngit clone git@github.com:jllanfranchi/i3cols.git\npip install -e ./i3cols\n```\n\n## Examples\n\n### Extracting data from I3 files\n\nAll command-line examples assume you are using BASH; adapt as necessary for\nyour favorite shell.\n\nExtract a few items from all Monte Carlo run 160000 files (nutau GENIE\nsimulation performed for oscNext), concatenating into single column per item:\n\n```bash\nfind /tmp/i3/genie/level7_v01.04/160000/ -name \"oscNext*.i3*\" | \\\n    sort -V | \\\n    i3cols extract_files_separately \\\n        --keys I3EventHeader I3MCTree I3MCWeightDict \\\n        --index-and-concatenate \\\n        --category-xform subrun \\\n        --procs 20 \\\n        --overwrite \\\n        --outdir /tmp/columnar/genie/level7_v01.04/160000\n```\n\nIf you completed the above and realize you also want the I3GENIEResultDict,\nthen you can re-run the above but just specify that key. The process should be\nmuch faster:\n\n```bash\nfind /tmp/i3/genie/level7_v01.04/160000/ -name \"oscNext*.i3*\" | \\\n    sort -V | \\\n    i3cols extract_files_separately \\\n        --keys I3GENIEResultDict \\\n        --index-and-concatenate \\\n        --category-xform subrun \\\n        --procs 20 \\\n        --overwrite \\\n        --outdir /tmp/columnar/genie/level7_v01.04/160000\n```\n\nExtract all keys from IC86.11 season. All subrun files for a given run are\ncombined transparently into one and then all runs are combined in the end into\nmonolithic columns, with a `run__category_index.npy` created in `outdir` that\nindexes the columns by run:\n\n```bash\ni3cols extract_season \\\n    /tmp/i3/data/level7_v01.04/IC86.11/ \\\n    --index-and-concatenate \\\n    --gcd /data/icecube/gcd/ \\\n    --overwrite \\\n    --outdir /tmp/columnar/data/level7_v01.04/IC86.11 \\\n    --compress\n```\n\nThings to note in the above:\n\n* You can specify paths on the command line, or you can pipe the paths to the\n   function. The former is simple for specifying one or a few paths, but UNIX\n   command lines are limited in total length, so the latter can be the only way to\n   successfully pass all paths to i3cols (and UNIX pipes are nice ayway; note the\n   numerical-version-sorting performed inline via\n   `find <...> | sort -V | i3cols ...` in the first example).\n* Optional compression of the resulting column directories (a directory + 1 or\n    more npy arrays within) can be performed after the extraction. Memory mapping\n    is not possible with the compressed files, but significant compression ratios\n    are achievable.\n* Extraction is performed in parallel where possible. Specify --procs to limit\n    the number of subroccesses; otherwise, extraction (and\n    compression/decompression) will attempt to use all cores (or hyperthreads,\n    where available) on a machine.\n\n\n### Working with the extracted data\n\nExtracted data is output to Numpy arrays, possibly with structured Numpy dtypes.\n\n```python\nimport numba\n\nfrom i3cols import cols, phys\n\n@numba.njit(fastmath=True, error_model=\"numpy\")\ndef get_tau_info(data, index):\n    \"\"\"Return indices of events which exhibit nutau regeneration and return a\n    dict of decay products of primary nutau.\n    \"\"\"\n\n    tau_regen_evt_indices = numba.typed.List()\n    tau_decay_products = numba.typed.Dict()\n    for evt_idx, index_ in enumerate(index):\n        flat_particles = data[index_[\"start\"] : index_[\"stop\"]]\n        for flat_particle in flat_particles:\n            if flat_particle[\"level\"] == 0:\n                if flat_particle[\"particle\"][\"pdg_encoding\"] not in phys.NUTAUS:\n                    break\n            else:\n                pdg = flat_particle[\"particle\"][\"pdg_encoding\"]\n                if flat_particle[\"level\"] == 1:\n                    if pdg not in tau_decay_products:\n                        tau_decay_products[pdg] = 0\n                    tau_decay_products[pdg] += 1\n                if pdg in phys.NUTAUS:\n                    tau_regen_evt_indices.append(evt_idx)\n    return tau_regen_evt_indices, tau_decay_products\n\n\n# Load just the I3MCTree (regardless of presence other columns), memory-mapped \narrays, category_indexes = cols.load(\"/tmp/columnar/genie/level7_v01.04/160000\", keys=\"I3MCTree\", mmap=True)\n\n# Get the info!\ntau_regen_evt_indices, tau_decay_products = get_tau_info_nb(**arrays[\"I3MCTree\"])\n```\n\n\n## See Also\n\nThe **i3cols** project was developed independently of but with the [Awkward\nArray project](https://github.com/scikit-hep/awkward-1.0) in mind. It is an\neventual goal that the extraction of arrays and other IceCube-specific things\nfrom this project can remain, while the backend storage and manipulation of\narrays can be done using that project.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/jllanfranchi/i3cols", "keywords": "", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "i3cols", "package_url": "https://pypi.org/project/i3cols/", "platform": "", "project_url": "https://pypi.org/project/i3cols/", "project_urls": {"Homepage": "https://github.com/jllanfranchi/i3cols"}, "release_url": "https://pypi.org/project/i3cols/0.1.3/", "requires_dist": ["numba (>=0.45)"], "requires_python": ">=2.7", "summary": "Numpy columnar storage for IceCube data", "version": "0.1.3"}, "last_serial": 7018504, "releases": {"0.1.3": [{"comment_text": "", "digests": {"md5": "db4dbefdc60aa365e6c85a516fbab00d", "sha256": "14721c86dbbcec9257cbbbba10d16d4563bec9dcf700b114013b0c7cef705eb1"}, "downloads": -1, "filename": "i3cols-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "db4dbefdc60aa365e6c85a516fbab00d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 59737, "upload_time": "2020-04-14T16:45:14", "upload_time_iso_8601": "2020-04-14T16:45:14.429583Z", "url": "https://files.pythonhosted.org/packages/db/82/e98285d7e2d381ba49b6182739442f2b4b39d8823163d8b3c25f0e009e2f/i3cols-0.1.3-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "44f8802b0db68557110281e8bd9835c2", "sha256": "a27f5775ff352630d2b2020ac1946f81843f34af736970cb3dff7f6f786c787e"}, "downloads": -1, "filename": "i3cols-0.1.3.tar.gz", "has_sig": false, "md5_digest": "44f8802b0db68557110281e8bd9835c2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 62861, "upload_time": "2020-04-14T16:45:16", "upload_time_iso_8601": "2020-04-14T16:45:16.252383Z", "url": "https://files.pythonhosted.org/packages/47/04/b8f0791a5eb29fb25544ee6c5de3fc9f46199bd086dedb837192a8fe638f/i3cols-0.1.3.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "db4dbefdc60aa365e6c85a516fbab00d", "sha256": "14721c86dbbcec9257cbbbba10d16d4563bec9dcf700b114013b0c7cef705eb1"}, "downloads": -1, "filename": "i3cols-0.1.3-py3-none-any.whl", "has_sig": false, "md5_digest": "db4dbefdc60aa365e6c85a516fbab00d", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=2.7", "size": 59737, "upload_time": "2020-04-14T16:45:14", "upload_time_iso_8601": "2020-04-14T16:45:14.429583Z", "url": "https://files.pythonhosted.org/packages/db/82/e98285d7e2d381ba49b6182739442f2b4b39d8823163d8b3c25f0e009e2f/i3cols-0.1.3-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "44f8802b0db68557110281e8bd9835c2", "sha256": "a27f5775ff352630d2b2020ac1946f81843f34af736970cb3dff7f6f786c787e"}, "downloads": -1, "filename": "i3cols-0.1.3.tar.gz", "has_sig": false, "md5_digest": "44f8802b0db68557110281e8bd9835c2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=2.7", "size": 62861, "upload_time": "2020-04-14T16:45:16", "upload_time_iso_8601": "2020-04-14T16:45:16.252383Z", "url": "https://files.pythonhosted.org/packages/47/04/b8f0791a5eb29fb25544ee6c5de3fc9f46199bd086dedb837192a8fe638f/i3cols-0.1.3.tar.gz"}]}