{"info": {"author": "Fishtown Analytics", "author_email": "info@fishtownanalytics.com", "bugtrack_url": null, "classifiers": [], "description": "## dbt-spark\n\n### Documentation\nFor more information on using Spark with dbt, consult the dbt documentation:\n- [Spark profile](https://docs.getdbt.com/docs/profile-spark)\n- [Spark specific configs](https://docs.getdbt.com/docs/spark-configs)\n\n### Installation\nThis plugin can be installed via pip:\n\n```\n# Install dbt-spark from PyPi:\n$ pip install dbt-spark\n```\n\n### Configuring your profile\n\n**Connection Method**\n\nConnections can be made to Spark in two different modes. The `http` mode is used when connecting to a managed service such as Databricks, which provides an HTTP endpoint; the `thrift` mode is used to connect directly to the master node of a cluster (either on-premise or in the cloud).\n\nA dbt profile can be configured to run against Spark using the following configuration:\n\n| Option  | Description                                        | Required?               | Example                  |\n|---------|----------------------------------------------------|-------------------------|--------------------------|\n| method    | Specify the connection method (`thrift` or `http`)   | Required   | `http`   |\n| schema  | Specify the schema (database) to build models into | Required                | `analytics`              |\n| host    | The hostname to connect to                         | Required                | `yourorg.sparkhost.com`  |\n| port    | The port to connect to the host on                 | Optional (default: 443 for `http`, 10001 for `thrift`) | `443`                    |\n| token   | The token to use for authenticating to the cluster | Required for `http`                | `abc123`                 |\n| organization | The id of the Azure Databricks workspace being used; only for  Azure Databricks | See Databricks Note | `1234567891234567` |\n| cluster | The name of the cluster to connect to              | Required for `http`               | `01234-23423-coffeetime` |\n| user    | The username to use to connect to the cluster  | Optional  | `hadoop`  |\n| connect_timeout | The number of seconds to wait before retrying to connect to a Pending Spark cluster | Optional (default: 10) | `60` |\n| connect_retries | The number of times to try connecting to a Pending Spark cluster before giving up   | Optional (default: 0)  | `5` |\n\n**Databricks Note**\n\nAWS and Azure Databricks have differences in their connections, likely due to differences in how their URLs are generated between the two services.\n\n**Organization:** To connect to an Azure Databricks cluster, you will need to obtain your organization ID, which is a unique ID Azure Databricks generates for each customer workspace.  To find the organization ID, see https://docs.microsoft.com/en-us/azure/databricks/dev-tools/databricks-connect#step-2-configure-connection-properties. This is a string field; if there is a leading zero, be sure to include it.\n\n**Port:** Please ignore all references to port 15001 in the databricks-connect docs as that is specific to that tool; port 443 is used for dbt-spark's https connection.\n\n**Host:** The host field for Databricks can be found at the start of your workspace or cluster url: `region.azuredatabricks.net` for Azure, or `account.cloud.databricks.com` for AWS. Do not include `https://`.\n\n**Usage with Amazon EMR**\n\nTo connect to Spark running on an Amazon EMR cluster, you will need to run `sudo /usr/lib/spark/sbin/start-thriftserver.sh` on the master node of the cluster to start the Thrift server (see https://aws.amazon.com/premiumsupport/knowledge-center/jdbc-connection-emr/ for further context). You will also need to connect to port `10001`, which will connect to the Spark backend Thrift server; port `10000` will instead connect to a Hive backend, which will not work correctly with dbt.\n\n\n**Example profiles.yml entries:**\n\n**http, e.g. Databricks**\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      method: http\n      type: spark\n      schema: analytics\n      host: yourorg.sparkhost.com\n      organization: 1234567891234567    # Azure Databricks ONLY\n      port: 443\n      token: abc123\n      cluster: 01234-23423-coffeetime\n      connect_retries: 5\n      connect_timeout: 60\n```\n\n**Thrift connection**\n```\nyour_profile_name:\n  target: dev\n  outputs:\n    dev:\n      method: thrift\n      type: spark\n      schema: analytics\n      host: 127.0.0.1\n      port: 10001\n      user: hadoop\n      connect_retries: 5\n      connect_timeout: 60\n```\n\n\n\n### Usage Notes\n\n**Model Configuration**\n\nThe following configurations can be supplied to models run with the dbt-spark plugin:\n\n\n| Option  | Description                                        | Required?               | Example                  |\n|---------|----------------------------------------------------|-------------------------|--------------------------|\n| file_format | The file format to use when creating tables (`parquet`, `delta`, `csv`, `json`, `text`, `jdbc`, `orc`, `hive` or `libsvm`). | Optional | `parquet`|\n| location_root  | The created table uses the specified directory to store its data. The table alias is appended to it. | Optional                | `/mnt/root`              |\n| partition_by  | Partition the created table by the specified columns. A directory is created for each partition. | Optional                | `partition_1`              |\n| clustered_by  | Each partition in the created table will be split into a fixed number of buckets by the specified columns. | Optional               | `cluster_1`              |\n| buckets  | The number of buckets to create while clustering | Required if `clustered_by` is specified                | `8`              |\n| incremental_strategy | The strategy to use for incremental models (`insert_overwrite` or `merge`). Note `merge` requires `file_format` = `delta` and `unique_key` to be specified. | Optional (default: `insert_overwrite`) | `merge` |\n| persist_docs | Whether dbt should include the model description as a table `comment` | Optional | `{'relation': true}` |\n\n\n**Incremental Models**\n\nTo use incremental models, specify a `partition_by` clause in your model config. The default incremental strategy used is `insert_overwrite`, which will overwrite the partitions included in your query. Be sure to re-select _all_ of the relevant\ndata for a partition when using the `insert_overwrite` strategy.\n\n```\n{{ config(\n    materialized='incremental',\n    partition_by=['date_day'],\n    file_format='parquet'\n) }}\n\n/*\n  Every partition returned by this query will be overwritten\n  when this model runs\n*/\n\nselect\n    date_day,\n    count(*) as users\n\nfrom {{ ref('events') }}\nwhere date_day::date >= '2019-01-01'\ngroup by 1\n```\n\nThe `merge` strategy is only supported when using file_format `delta` (supported in Databricks). It also requires you to specify a `unique key` to match existing records.\n\n```\n{{ config(\n    materialized='incremental',\n    incremental_strategy='merge',\n    partition_by=['date_day'],\n    file_format='delta'\n) }}\n\nselect *\nfrom {{ ref('events') }}\n{% if is_incremental() %}\n  where date_day > (select max(date_day) from {{ this }})\n{% endif %}\n```\n\n### Running locally\n\nA `docker-compose` environment starts a Spark Thrift server and a Postgres database as a Hive Metastore backend.\n\n```\ndocker-compose up\n```\n\nYour profile should look like this:\n\n```\nyour_profile_name:\n  target: local\n  outputs:\n    local:\n      method: thrift\n      type: spark\n      schema: analytics\n      host: 127.0.0.1\n      port: 10000\n      user: dbt\n      connect_retries: 5\n      connect_timeout: 60\n```\n\nConnecting to the local spark instance:\n\n* The Spark UI should be available at [http://localhost:4040/sqlserver/](http://localhost:4040/sqlserver/)\n* The endpoint for SQL-based testing is at `http://localhost:10000` and can be referenced with the Hive or Spark JDBC drivers using connection string `jdbc:hive2://localhost:10000` and default credentials `dbt`:`dbt`\n\nNote that the Hive metastore data is persisted under `./.hive-metastore/`, and the Spark-produced data under `./.spark-warehouse/`. To completely reset you environment run the following:\n\n```\ndocker-compose down\nrm -rf ./.hive-metastore/\nrm -rf ./.spark-warehouse/\n```\n\n### Reporting bugs and contributing code\n\n-   Want to report a bug or request a feature? Let us know on [Slack](http://slack.getdbt.com/), or open [an issue](https://github.com/fishtown-analytics/dbt-spark/issues/new).\n\n## Code of Conduct\n\nEveryone interacting in the dbt project's codebases, issue trackers, chat rooms, and mailing lists is expected to follow the [PyPA Code of Conduct](https://www.pypa.io/en/latest/code-of-conduct/).\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/fishtown-analytics/dbt-spark", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "dbt-spark", "package_url": "https://pypi.org/project/dbt-spark/", "platform": "", "project_url": "https://pypi.org/project/dbt-spark/", "project_urls": {"Homepage": "https://github.com/fishtown-analytics/dbt-spark"}, "release_url": "https://pypi.org/project/dbt-spark/0.16.0/", "requires_dist": ["dbt-core (==0.16.0)", "PyHive[hive] (<0.7.0,>=0.6.0)", "thrift (<0.12.0,>=0.11.0)"], "requires_python": "", "summary": "The SparkSQL plugin for dbt (data build tool)", "version": "0.16.0"}, "last_serial": 6979152, "releases": {"0.13.0": [{"comment_text": "", "digests": {"md5": "b4985cce5174703043df23a701f7cce3", "sha256": "65d8d9ccfd5185cfaba1652bb732d69e25eda12dbafcdb67943615d3255e6242"}, "downloads": -1, "filename": "dbt_spark-0.13.0-py3.7.egg", "has_sig": false, "md5_digest": "b4985cce5174703043df23a701f7cce3", "packagetype": "bdist_egg", "python_version": "3.7", "requires_python": null, "size": 26236, "upload_time": "2019-07-03T17:12:07", "upload_time_iso_8601": "2019-07-03T17:12:07.076459Z", "url": "https://files.pythonhosted.org/packages/6a/79/686f13b7bfa55ff80abc40c3db0a61f59fafba6c17e9b8fcebb153eed6bf/dbt_spark-0.13.0-py3.7.egg"}, {"comment_text": "", "digests": {"md5": "aba7d7199a6f4f76fcc8c0933cbc5a4d", "sha256": "d0c3255edadec5a2d423ca7fd20a4d2b0ba45c75fc0b73b554121a98f74c72c6"}, "downloads": -1, "filename": "dbt-spark-0.13.0.tar.gz", "has_sig": false, "md5_digest": "aba7d7199a6f4f76fcc8c0933cbc5a4d", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 12799, "upload_time": "2019-07-03T17:12:04", "upload_time_iso_8601": "2019-07-03T17:12:04.935194Z", "url": "https://files.pythonhosted.org/packages/bb/37/fe34166ef27c5d71022ae27ec2445c8c0227b3f17bd5999e5893e6012ca8/dbt-spark-0.13.0.tar.gz"}], "0.14.3": [{"comment_text": "", "digests": {"md5": "2086bf50c5cfd9f85a84f1e4b4eb0aa4", "sha256": "019ee2f1d81831f74d7c78ff151811811ac897b277cf8b7617fd6c7edc290bd2"}, "downloads": -1, "filename": "dbt_spark-0.14.3-py3-none-any.whl", "has_sig": false, "md5_digest": "2086bf50c5cfd9f85a84f1e4b4eb0aa4", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 15931, "upload_time": "2020-03-23T16:28:06", "upload_time_iso_8601": "2020-03-23T16:28:06.536918Z", "url": "https://files.pythonhosted.org/packages/6f/37/f2e676ca33c19f9881252ac1e83960d1125035d8521d1076f0f6041e22f4/dbt_spark-0.14.3-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "ef1baa39f359458aa1aaa6058f643ea2", "sha256": "e7d84d06a64872476617ba774e3fecde108ea25a3ace54384f50202a8bf7d5f4"}, "downloads": -1, "filename": "dbt-spark-0.14.3.tar.gz", "has_sig": false, "md5_digest": "ef1baa39f359458aa1aaa6058f643ea2", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 13254, "upload_time": "2020-03-23T16:28:07", "upload_time_iso_8601": "2020-03-23T16:28:07.983001Z", "url": "https://files.pythonhosted.org/packages/27/f6/ab35352293399dfbcbf96a7d4b93cbc801ef2e0d1527081d84a2be089d35/dbt-spark-0.14.3.tar.gz"}], "0.15.3": [{"comment_text": "", "digests": {"md5": "d2a7714ca10068aa27656de146128924", "sha256": "86968568b8abbd845843d70cc38d09992628cf350099b3f5ad05e30d02e8da45"}, "downloads": -1, "filename": "dbt_spark-0.15.3-py3-none-any.whl", "has_sig": false, "md5_digest": "d2a7714ca10068aa27656de146128924", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 22946, "upload_time": "2020-03-23T16:38:40", "upload_time_iso_8601": "2020-03-23T16:38:40.054443Z", "url": "https://files.pythonhosted.org/packages/32/90/8868bcd053e9369a5ed28099a2e879708b756227ba14b66b062480fe5314/dbt_spark-0.15.3-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "eda7c505105890445fd4961b91a6fa08", "sha256": "5782468bf7a54d7dd8fde753a16ff3196b0d36d1ff6b01b58a0830992bf96c27"}, "downloads": -1, "filename": "dbt-spark-0.15.3.tar.gz", "has_sig": false, "md5_digest": "eda7c505105890445fd4961b91a6fa08", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22617, "upload_time": "2020-03-23T16:38:41", "upload_time_iso_8601": "2020-03-23T16:38:41.088757Z", "url": "https://files.pythonhosted.org/packages/75/76/38456629b791664f3bbad23e2043a7258ba0af2d9f293ebfee3bb6920a99/dbt-spark-0.15.3.tar.gz"}], "0.16.0": [{"comment_text": "", "digests": {"md5": "0e7d20563e9606f3fbfcf2f3f3b13e14", "sha256": "660056ca84e93b2a077852979ef77d21f25d14f25769889399d984419e17d585"}, "downloads": -1, "filename": "dbt_spark-0.16.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0e7d20563e9606f3fbfcf2f3f3b13e14", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23459, "upload_time": "2020-04-08T16:53:46", "upload_time_iso_8601": "2020-04-08T16:53:46.208850Z", "url": "https://files.pythonhosted.org/packages/5c/4c/3712c5842bb6504f067749eea4e9da4b9fea59dd36e7aeb26d78bc716504/dbt_spark-0.16.0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "c6361a56ba829bc282429ad77c2180df", "sha256": "8636fbc5e10cc75cc34378b982d708ee4aeeb17bae96f1b869bb75dddc79121a"}, "downloads": -1, "filename": "dbt-spark-0.16.0.tar.gz", "has_sig": false, "md5_digest": "c6361a56ba829bc282429ad77c2180df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22969, "upload_time": "2020-04-08T16:53:47", "upload_time_iso_8601": "2020-04-08T16:53:47.575198Z", "url": "https://files.pythonhosted.org/packages/b3/a8/cccf7554fc6213658b15157c1567af0ad67d9a253b76921a4b55a098cc00/dbt-spark-0.16.0.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "0e7d20563e9606f3fbfcf2f3f3b13e14", "sha256": "660056ca84e93b2a077852979ef77d21f25d14f25769889399d984419e17d585"}, "downloads": -1, "filename": "dbt_spark-0.16.0-py3-none-any.whl", "has_sig": false, "md5_digest": "0e7d20563e9606f3fbfcf2f3f3b13e14", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 23459, "upload_time": "2020-04-08T16:53:46", "upload_time_iso_8601": "2020-04-08T16:53:46.208850Z", "url": "https://files.pythonhosted.org/packages/5c/4c/3712c5842bb6504f067749eea4e9da4b9fea59dd36e7aeb26d78bc716504/dbt_spark-0.16.0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "c6361a56ba829bc282429ad77c2180df", "sha256": "8636fbc5e10cc75cc34378b982d708ee4aeeb17bae96f1b869bb75dddc79121a"}, "downloads": -1, "filename": "dbt-spark-0.16.0.tar.gz", "has_sig": false, "md5_digest": "c6361a56ba829bc282429ad77c2180df", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22969, "upload_time": "2020-04-08T16:53:47", "upload_time_iso_8601": "2020-04-08T16:53:47.575198Z", "url": "https://files.pythonhosted.org/packages/b3/a8/cccf7554fc6213658b15157c1567af0ad67d9a253b76921a4b55a098cc00/dbt-spark-0.16.0.tar.gz"}]}