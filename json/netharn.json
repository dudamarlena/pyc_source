{"info": {"author": "Jon Crall", "author_email": "jon.crall@kitware.com", "bugtrack_url": null, "classifiers": ["Development Status :: 4 - Beta", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Scientific/Engineering", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Software Development", "Topic :: Software Development :: Libraries :: Python Modules", "Topic :: Utilities"], "description": "NetHarn - a PyTorch Network Harness\n-----------------------------------\n\n|GitlabCIPipeline| |GitlabCICoverage| |Pypi| |Downloads| \n\nThe main webpage for this project is: https://gitlab.kitware.com/computer-vision/netharn\n\nIf you want a framework for your pytorch training loop that\n(1) chooses directory names based on hashes of hyperparameters,\n(2) can write a single-file deployment of your model by statically auto-extracting the in-code definition of the model topology and zipping it with the weights, \n(3) has brief terminal output and a rich logging output, \n(4) has rule-based monitoring of validation loss and can reduce the learning rate or early stop, \n(5) has tensorboard and/or matplotlib visualizations of training statistics, and \n(6) is designed to be extended, then you might be interested in NetHarn. \n\nNAME:\n    NetHarn (pronounced \"net-harn\")\nFRAMEWORK:\n    PyTorch\nFEATURES: \n    * hyperparameter tracking\n    * training directory management\n    * callback-based public API \n    * XPU - code abstraction for [cpu, gpu, multi-gpu].\n    * single-file deployments (NEW in version ``0.1.0``).\n    * reasonable test coverage using pytest and xdoctest\n    * CI testing on appveyor and travis (note a few tests are failing due to minor issues)\n    * A rich utility set\n    * Extensions of PyTorch objects (e.g. critions, initializers, layers,\n      optimizers, schedulers)\nBUILTINS:\n   - training loop boilerplate\n   - snapshots / checkpoints\n   - progress bars (backend_choices: [progiter, tqdm])\n   - data provenance of training history in ``train_info.json``\n   - tensorboard metric visualization (optional)\nDESIGN PHILOSOPHY: \n   Avoid boilerplate, built-it yourself when you need to, and don't repeat yourself.\n   Experiments should be strongly tied to the choice of hyperparameters, and\n   the framework should be able to construct a directory heirarchy based on\n   these hyperparameters.\nSLOGAN: \n    Rein and train.\nUSAGE PATTERNS:\n    (1) Write code for a torch object  (i.e. Dataset, Model, Criterion, Initializer, and Scheduler) just as you normally would.\n    (2) Inherit from the ``netharn.FitHarn`` object, define ``run_batch``, ``on_batch``, ``on_epoch``, etc...\n    (3) Create an instance of ``netharn.HyperParams`` to specify your dataset, model, criterion, etc...\n    (4) Create an instance of your ``FitHarn`` object with those hyperparameters.\n    (5) Then execute its ``run`` method.\n    (6) ???\n    (7) profit\nEXAMPLES:\n    * ToyData2d classification with netharn.models.ToyNet2d (see doctest in netharn/fit_harn.py:__DOC__:0)\n    * MNIST digit classification with MnistNet (netharn/examples/mnist.py)\n    * Cifar10 category classification with ResNet50 / dpn91 (netharn/examples/cifar.py)\n    * Voc2007+2012 object detection with YOLOv2 (netharn/examples/yolo_voc.py)\n    * IBEIS metric learning with SiameseLP (netharn/examples/siam_ibeis.py)\nSTABILITY:\n   Mostly harmless. Most tests pass, the current failures are probably not\n   critical. I'm able to use it on my machine (tm). In this early stage of\n   development, there are still a few pain points. Issues and PRs welcome.\nKNOWN BUGS:\n   * The metrics for computing detection mAP / AP might not be correct.\n   * The YOLO example gets to about 70% mAP (using Girshik's mAP code) whereas we should be hitting 74-76%\nAUTHORS COMMENTS:\n   * The MNIST, CIFAR, and VOC examples will download the data as needed.\n   * The CIFAR example for ResNet50 achieves 95.72% accuracy, outperforming the\n     best DPN92 result (95.16%) that I'm aware of.\n     This result seems real, I do not believe I've made an error in measurement\n     (but this has need been peer-reviewed so, caveat emptor).  I've reproduced\n     this results a few times. You can use the code in examples/cifar.py to see\n     if you can too (please tell me if you cannot). \n   * The YOLO example is based of of EAVise's excellent lightnet (https://gitlab.com/EAVISE/lightnet/) package.\n   * I reimplemented the CocoAPI (see netharn.data.coco_api), because I had some\n     (probably minor) issue with the original implementation. I've extended it\n     quite a bit, and I'd recommend using it.\n   * The metric-learning example requires code requires the ibeis software:\n     `https://github.com/Erotemic/ibeis`.\nDEPENDENCIES:\n    * torch\n    * numpy\n    * Cython\n    * ubelt\n    * xdoctest\n    * ... (see requirements.txt)\n\n\nFeatures (continued)\n====================\n\n* Hyperparameter tracking: The hash of your hyperparameters determines the\n  directory data will be written to. We also allow for a \"nicer\" means to\n  manage directory structures. Given a ``HyperParams`` object, we create the\n  symlink ``{workdir}/fit/nice/{nice}`` which points to\n  ``{workdir}/fit/runs/{nice}/{hashid}``.\n\n* Automatic restarts: \n  Calling ``FitHarn.run`` twice restarts training from where you left off by\n  default (as long as the hyperparams haven't changed).\n\n* \"Smart\" Snapshot cleanup:  \n  Maintaining model weights files can be a memory hog. Depending the settings\n  of ``harn.preferences``, ``netharn.FitHarn`` will periodically remove\n  less-recent or low-scoring snapshots.\n\n* Deployment files: \n  Model weights and architecture are together written as one\n  reasonably-portable zip-file. We also package training metadata to maintain\n  data provinence and make reproducing experiments easier. \n\n* Restart from any pretrained state: \n  use ``netharn.initializers.PretainedInitializer``. \n\n* Utilities for building networks in torch:\n  Layers like ``netharn.layers.ConvNormNd`` make it easy to build networks for\n  n=1, 2, or 3 dimensional data. \n\n* Analytic output shape and receptive field:\n  Netharn defines a ``netharn.layers.AnalyticModule``, which can automatically\n  define ``forward``, ``output_shape_for`` and ``receptive_field_for`` if users\n  define a special ``_output_for`` method, written with the\n  ``netharn.analytic_for.Output``, ``netharn.analytic_for.Hidden``, and\n  ``netharn.analytic_for.OutputFor`` special callables.\n\n* Example tasks:\n  Baseline code for standard tasks like: object segmentation, classification,\n  and detection are defined in ``netharn.examples``. The examples also provide\n  example use cases for ``ndsampler``, ``kwimage``, ``kwannot``, and\n  ``kwplot``. \n\n\n\nInstallation\n============\n\nIn the future these instructions may actually be different than the developer\nsetup instructions, but for now they are the same.\n\n.. code-block:: bash\n\n    mkdir -p ~/code\n    git clone git@github.com:Erotemic/netharn.git ~/code/netharn\n    cd ~/code/netharn\n    ./run_developer_setup.sh\n\n\nWhile all netharn dependencies should be available on pypi (with manylinux2010\nwheels for binary packages), there are other packages developed concurrently\nwith netharn. To install the development version of these dependencies then run\n``python super_setup.py ensure`` to check out the repos and ensure they are on\nthe correct branch, ``python super_setup.py develop`` to build everything in\ndevelopment mode, and ``python super_setup.py pull`` to update to the latest on\nthe branch.\n\nDescription\n===========\n\nParameterized fit harnesses for PyTorch.\n\nTrains models and keeps track of your hyperparameters.\n\nThis is a clean port of the good parts developed in my research repo: ``clab``. \n\nSee the netharn/examples folder for example usage. The doctests are also a good\nresource. It would be nice if we had better docs.\n\nNetHarn is a research framework for training and deploying arbitrary PyTorch\nmodels.  It was designed for the purpose of minimizing training-loop\nboilerplate and tracking hyperparameters to encourage reproducible research.\nNetHarn separates the problem of training a model into the following core\nhyperparameter components: the datasets, model, criterion, initializer,\noptimizer, and learning rate scheduler.  Runs with different hyperparameters\nare automatically logged to separate directories which makes it simple to\ncompare the results of two experiments.  NetHarn also has the ability to create\na single-file deployment of a trained model that is independent of the system\nused to train it.  This makes it fast and simple for research results to be\nexternally verified and moved into production.\n\n\n\nDeveloper Setup:\n================\n\n\nIn the future these instructions might be different from the install\ninstructions, but for now they are the same.\n\n.. code-block:: bash\n\n    sudo apt-get install python3 python-dev python3-dev \\\n     build-essential libssl-dev libffi-dev \\\n     libxml2-dev libxslt1-dev zlib1g-dev \\\n     python-pip\n\n    mkdir -p ~/code\n    git clone git@github.com:Erotemic/netharn.git ~/code/netharn\n    cd ~/code/netharn\n\n    ./run_developer_setup.sh\n\n\nDocumentation\n=============\n\nNetharn's documentation is currently sparse. I typically do most of my\ndocumenting in the code itself using docstrings. In the future much of this\nwill likely be consolidated in a read-the-docs style documentation page, but\nfor now you'll need to look at the code to read the docs.\n\nThe main concept provided by netharn is the \"FitHarn\", which has a decent\nmodule level docstring, and a lot of good class / method level docstrings: \nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/fit_harn.py\n\nThe examples folder has better docstrings with task-level documentation: \n\nThe simplest is the mnist example:\nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/examples/mnist.py\n\nThe CIFAR example builds on the mnist example:\nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/examples/cifar.py\n\nI'd recommend going through those two examples, as they have the best documentation. \n\nThe segmentation example:\nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/examples/segmentation.py\n\nand object detection example: \nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/examples/object_detection.py\n\nhave less documentation, but provide more real-world style examples of how netharn is used. \n\nThere is an applied segmentation example that is specific to the CAMVID dataset:\nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/examples/sseg_camvid.py\n\nAnd there is an applied VOC detection example:\nhttps://gitlab.kitware.com/computer-vision/netharn/-/blob/master/netharn/examples/yolo_voc.py\n\nThis README also contains a toy example.\n\nToy Example:\n============\n\nThis following example is the doctest in ``netharn/fit_harn.py``. It\ndemonstrates how to use NetHarn to train a model to solve a toy problem.  \n\nIn this toy problem, we do not extend the netharn.FitHarn object, so we are using\nthe default behavior of ``run_batch``. The default ``on_batch``, and\n``on_epoch`` do nothing, so only loss will be the only measurement of\nperformance.\n\nFor further examples please see the examples directory. These example show how\nto extend netharn.FitHarn to measure performance wrt a particular problem.  The\nMNIST and CIFAR examples are the most simple. The YOLO example is more complex.\nThe IBEIS example depends on non-public data / software, but can still be\nuseful to look at.  Its complexity is more than CIFAR but less than YOLO.\n\n\n.. code-block:: python\n\n    >>> import netharn \n    >>> hyper = netharn.HyperParams(**{\n    >>>     # ================\n    >>>     # Environment Components\n    >>>     'workdir'     : ub.ensure_app_cache_dir('netharn/demo'),\n    >>>     'nice'        : 'demo',\n    >>>     'xpu'         : netharn.XPU.cast('auto'),\n    >>>     # workdir is a directory where intermediate results can be saved\n    >>>     # nice symlinks <workdir>/fit/nice/<nice> -> ../runs/<hashid>\n    >>>     # XPU auto select a gpu if idle and VRAM>6GB else a cpu\n    >>>     # ================\n    >>>     # Data Components\n    >>>     'datasets'    : {  # dict of plain ol torch.data.Dataset instances\n    >>>         'train': netharn.data.ToyData2d(size=3, border=1, n=256, rng=0),\n    >>>         'vali': netharn.data.ToyData2d(size=3, border=1, n=128, rng=1),\n    >>>         'test': netharn.data.ToyData2d(size=3, border=1, n=128, rng=2),\n    >>>     },\n    >>>     'loaders'     : {'batch_size': 64}, # DataLoader instances or kw\n    >>>     # ================\n    >>>     # Algorithm Components\n    >>>     # Note the (cls, kw) tuple formatting\n    >>>     'model'       : (netharn.models.ToyNet2d, {}),\n    >>>     'optimizer'   : (netharn.optimizers.SGD, {\n    >>>         'lr': 0.0001\n    >>>     }),\n    >>>     # focal loss is usually better than netharn.criterions.CrossEntropyLoss\n    >>>     'criterion'   : (netharn.criterions.FocalLoss, {}),\n    >>>     'initializer' : (netharn.initializers.KaimingNormal, {\n    >>>         'param': 0,\n    >>>     }),\n    >>>     # these may receive an overhaul soon\n    >>>     'scheduler'   : (netharn.schedulers.ListedScheduler, {\n    >>>         'points': {'lr': {0: .0001, 2: .01, 5: .015, 6: .005, 9: .001}},\n    >>>         'interpolation': 'linear',\n    >>>     }),\n    >>>     'monitor'     : (netharn.Monitor, {\n    >>>         'max_epoch': 10,\n    >>>     }),\n    >>>     # dynamics are a config option that modify the behavior of the main\n    >>>     # training loop. These parameters effect the learned model.\n    >>>     'dynamics'   : {'batch_step': 4},\n    >>> })\n    >>> harn = netharn.FitHarn(hyper)\n    >>> # non-algorithmic behavior configs (do not change learned models)\n    >>> harn.preferences['prog_backend'] = 'progiter'  # alternative: 'tqdm'\n    >>> harn.preferences['num_keep'] = 10\n    >>> # start training.\n    >>> harn.initialize(reset='delete')\n    >>> harn.run()  # note: run calls initialize it hasn't already been called.\n    >>> # xdoc: +IGNORE_WANT\n\nRunning this code produes the following output:\n\n.. code-block:: \n\n   RESET HARNESS BY DELETING EVERYTHING IN TRAINING DIR\n   Symlink: /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum -> /home/joncrall/.cache/netharn/demo/_mru\n   ... already exists\n   Symlink: /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum -> /home/joncrall/.cache/netharn/demo/fit/nice/demo\n   ... already exists\n   ... and points to the right place\n   INFO: Initializing tensorboard (dont forget to start the tensorboard server)\n   INFO: Model has 824 parameters\n   INFO: Mounting ToyNet2d model on GPU(0)\n   INFO: Exported model topology to /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/ToyNet2d_2a3f49.py\n   INFO: Initializing model weights with: <netharn.initializers.nninit_core.KaimingNormal object at 0x7fc67eff0278>\n   INFO:  * harn.train_dpath = '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum'\n   INFO:  * harn.nice_dpath  = '/home/joncrall/.cache/netharn/demo/fit/nice/demo'\n   INFO: Snapshots will save to harn.snapshot_dpath = '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots'\n   INFO: ARGV:\n       /home/joncrall/.local/conda/envs/py36/bin/python /home/joncrall/.local/conda/envs/py36/bin/ipython\n   INFO: dont forget to start:\n       tensorboard --logdir ~/.cache/netharn/demo/fit/nice\n   INFO: === begin training 0 / 10 : demo ===\n   epoch lr:0.0001 \u2502 vloss is unevaluated  0/10... rate=0 Hz, eta=?, total=0:00:00, wall=19:36 EST\n   train loss:0.173 \u2502 100.00% of 64x8... rate=11762.01 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   vali loss:0.170 \u2502 100.00% of 64x4... rate=9991.94 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   test loss:0.170 \u2502 100.00% of 64x4... rate=24809.37 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   INFO: === finish epoch 0 / 10 : demo ===\n   epoch lr:0.00505 \u2502 vloss: 0.1696 (n_bad=00, best=0.1696)  1/10... rate=1.24 Hz, eta=0:00:07, total=0:00:00, wall=19:36 EST\n   train loss:0.175 \u2502 100.00% of 64x8... rate=13522.14 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   vali loss:0.167 \u2502 100.00% of 64x4... rate=23598.31 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   test loss:0.167 \u2502 100.00% of 64x4... rate=20354.22 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   INFO: === finish epoch 1 / 10 : demo ===\n   epoch lr:0.01 \u2502 vloss: 0.1685 (n_bad=00, best=0.1685)  2/10... rate=1.28 Hz, eta=0:00:06, total=0:00:01, wall=19:36 EST\n   train loss:0.177 \u2502 100.00% of 64x8... rate=15723.99 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   vali loss:0.163 \u2502 100.00% of 64x4... rate=29375.56 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   test loss:0.163 \u2502 100.00% of 64x4... rate=29664.69 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   INFO: === finish epoch 2 / 10 : demo ===\n\n   <JUST MORE OF THE SAME; REMOVED FOR BREVITY>\n\n   epoch lr:0.001 \u2502 vloss: 0.1552 (n_bad=00, best=0.1552)  9/10... rate=1.11 Hz, eta=0:00:00, total=0:00:08, wall=19:36 EST\n   train loss:0.164 \u2502 100.00% of 64x8... rate=13795.93 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   vali loss:0.154 \u2502 100.00% of 64x4... rate=19796.72 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   test loss:0.154 \u2502 100.00% of 64x4... rate=21396.73 Hz, eta=0:00:00, total=0:00:00, wall=19:36 EST\n   INFO: === finish epoch 9 / 10 : demo ===\n   epoch lr:0.001 \u2502 vloss: 0.1547 (n_bad=00, best=0.1547) 10/10... rate=1.13 Hz, eta=0:00:00, total=0:00:08, wall=19:36 EST\n\n\n\n\n   INFO: Maximum harn.epoch reached, terminating ...\n   INFO: \n\n\n\n   INFO: training completed\n   INFO: harn.train_dpath = '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum'\n   INFO: harn.nice_dpath  = '/home/joncrall/.cache/netharn/demo/fit/nice/demo'\n   INFO: view tensorboard results for this run via:\n       tensorboard --logdir ~/.cache/netharn/demo/fit/nice\n   [DEPLOYER] Deployed zipfpath=/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/deploy_ToyNet2d_lnejaaum_009_GAEYQT.zip\n   INFO: wrote single-file deployment to: '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/deploy_ToyNet2d_lnejaaum_009_GAEYQT.zip'\n   INFO: exiting fit harness.\n\nFurthermore, if you were to run that code when ``'--verbose' in sys.argv``,\nthen it would produce this more detailed description of what it was doing:\n\n.. code-block:: \n\n   RESET HARNESS BY DELETING EVERYTHING IN TRAINING DIR\n   Symlink: /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum -> /home/joncrall/.cache/netharn/demo/_mru\n   ... already exists\n   Symlink: /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum -> /home/joncrall/.cache/netharn/demo/fit/nice/demo\n   ... already exists\n   ... and points to the right place\n   DEBUG: Initialized logging\n   INFO: Initializing tensorboard (dont forget to start the tensorboard server)\n   DEBUG: harn.train_info[hyper] = {\n       'model': (\n           'netharn.models.toynet.ToyNet2d',\n           {\n               'input_channels': 1,\n               'num_classes': 2,\n           },\n       ),\n       'initializer': (\n           'netharn.initializers.nninit_core.KaimingNormal',\n           {\n               'mode': 'fan_in',\n               'param': 0,\n           },\n       ),\n       'optimizer': (\n           'torch.optim.sgd.SGD',\n           {\n               'dampening': 0,\n               'lr': 0.0001,\n               'momentum': 0,\n               'nesterov': False,\n               'weight_decay': 0,\n           },\n       ),\n       'scheduler': (\n           'netharn.schedulers.scheduler_redesign.ListedScheduler',\n           {\n               'interpolation': 'linear',\n               'optimizer': None,\n               'points': {'lr': {0: 0.0001, 2: 0.01, 5: 0.015, 6: 0.005, 9: 0.001}},\n           },\n       ),\n       'criterion': (\n           'netharn.criterions.focal.FocalLoss',\n           {\n               'focus': 2,\n               'ignore_index': -100,\n               'reduce': None,\n               'reduction': 'mean',\n               'size_average': None,\n               'weight': None,\n           },\n       ),\n       'loader': (\n           'torch.utils.data.dataloader.DataLoader',\n           {\n               'batch_size': 64,\n           },\n       ),\n       'dynamics': (\n           'Dynamics',\n           {\n               'batch_step': 4,\n               'grad_norm_max': None,\n           },\n       ),\n   }\n   DEBUG: harn.hyper = <netharn.hyperparams.HyperParams object at 0x7fb19b4b8748>\n   DEBUG: make XPU\n   DEBUG: harn.xpu = <XPU(GPU(0)) at 0x7fb12af24668>\n   DEBUG: Criterion: FocalLoss\n   DEBUG: Optimizer: SGD\n   DEBUG: Scheduler: ListedScheduler\n   DEBUG: Making loaders\n   DEBUG: Making model\n   DEBUG: ToyNet2d(\n     (layers): Sequential(\n       (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (2): ReLU(inplace)\n       (3): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n       (4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n       (5): ReLU(inplace)\n       (6): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n     )\n     (softmax): Softmax()\n   )\n   INFO: Model has 824 parameters\n   INFO: Mounting ToyNet2d model on GPU(0)\n   DEBUG: Making initializer\n   DEBUG: Move FocalLoss() model to GPU(0)\n   DEBUG: Make optimizer\n   DEBUG: Make scheduler\n   DEBUG: Make monitor\n   DEBUG: Make dynamics\n   INFO: Exported model topology to /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/ToyNet2d_2a3f49.py\n   INFO: Initializing model weights with: <netharn.initializers.nninit_core.KaimingNormal object at 0x7fb129e732b0>\n   DEBUG: calling harn.initializer=<netharn.initializers.nninit_core.KaimingNormal object at 0x7fb129e732b0>\n   INFO:  * harn.train_dpath = '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum'\n   INFO:  * harn.nice_dpath  = '/home/joncrall/.cache/netharn/demo/fit/nice/demo'\n   INFO: Snapshots will save to harn.snapshot_dpath = '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots'\n   INFO: ARGV:\n       /home/joncrall/.local/conda/envs/py36/bin/python /home/joncrall/.local/conda/envs/py36/bin/ipython --verbose\n   INFO: dont forget to start:\n       tensorboard --logdir ~/.cache/netharn/demo/fit/nice\n   INFO: === begin training 0 / 10 : demo ===\n   DEBUG: epoch lr:0.0001 \u2502 vloss is unevaluated\n   epoch lr:0.0001 \u2502 vloss is unevaluated  0/10... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 EST\n   DEBUG: === start epoch 0 ===\n   DEBUG: log_value(epoch lr, 0.0001, 0\n   DEBUG: log_value(epoch momentum, 0, 0\n   DEBUG: _run_epoch 0, tag=train, learn=True\n   DEBUG:  * len(loader) = 8\n   DEBUG:  * loader.batch_size = 64\n   train loss:-1.000 \u2502 0.00% of 64x8... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 ESTDEBUG: Making batch iterator\n   DEBUG: Starting batch iteration for tag=train, epoch=0\n   train loss:0.224 \u2502 100.00% of 64x8... rate=12052.25 Hz, eta=0:00:00, total=0:00:00, wall=19:56 EST\n   DEBUG: log_value(train epoch loss, 0.22378234565258026, 0\n   DEBUG: Finished batch iteration for tag=train, epoch=0\n   DEBUG: _run_epoch 0, tag=vali, learn=False\n   DEBUG:  * len(loader) = 4\n   DEBUG:  * loader.batch_size = 64\n   vali loss:-1.000 \u2502 0.00% of 64x4... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 ESTDEBUG: Making batch iterator\n   DEBUG: Starting batch iteration for tag=vali, epoch=0\n   vali loss:0.175 \u2502 100.00% of 64x4... rate=23830.75 Hz, eta=0:00:00, total=0:00:00, wall=19:56 EST\n   DEBUG: log_value(vali epoch loss, 0.1749105490744114, 0\n   DEBUG: Finished batch iteration for tag=vali, epoch=0\n   DEBUG: epoch lr:0.0001 \u2502 vloss: 0.1749 (n_bad=00, best=0.1749)\n   DEBUG: _run_epoch 0, tag=test, learn=False\n   DEBUG:  * len(loader) = 4\n   DEBUG:  * loader.batch_size = 64\n   test loss:-1.000 \u2502 0.00% of 64x4... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 ESTDEBUG: Making batch iterator\n   DEBUG: Starting batch iteration for tag=test, epoch=0\n   test loss:0.176 \u2502 100.00% of 64x4... rate=28606.65 Hz, eta=0:00:00, total=0:00:00, wall=19:56 EST\n   DEBUG: log_value(test epoch loss, 0.17605290189385414, 0\n   DEBUG: Finished batch iteration for tag=test, epoch=0\n   DEBUG: Saving snapshot to /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots/_epoch_00000000.pt\n   DEBUG: Snapshot saved to /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots/_epoch_00000000.pt\n   DEBUG: new best_snapshot /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots/_epoch_00000000.pt\n   DEBUG: Plotting tensorboard data\n   Populating the interactive namespace from numpy and matplotlib\n   INFO: === finish epoch 0 / 10 : demo ===\n\n   <JUST MORE OF THE SAME; REMOVED FOR BREVITY>\n\n   INFO: === finish epoch 8 / 10 : demo ===\n   DEBUG: epoch lr:0.001 \u2502 vloss: 0.2146 (n_bad=08, best=0.1749)\n   epoch lr:0.001 \u2502 vloss: 0.2146 (n_bad=08, best=0.1749)  9/10... rate=1.20 Hz, eta=0:00:00, total=0:00:07, wall=19:56 EST\n   DEBUG: === start epoch 9 ===\n   DEBUG: log_value(epoch lr, 0.001, 9\n   DEBUG: log_value(epoch momentum, 0, 9\n   DEBUG: _run_epoch 9, tag=train, learn=True\n   DEBUG:  * len(loader) = 8\n   DEBUG:  * loader.batch_size = 64\n   train loss:-1.000 \u2502 0.00% of 64x8... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 ESTDEBUG: Making batch iterator\n   DEBUG: Starting batch iteration for tag=train, epoch=9\n   train loss:0.207 \u2502 100.00% of 64x8... rate=13580.13 Hz, eta=0:00:00, total=0:00:00, wall=19:56 EST\n   DEBUG: log_value(train epoch loss, 0.2070118673145771, 9\n   DEBUG: Finished batch iteration for tag=train, epoch=9\n   DEBUG: _run_epoch 9, tag=vali, learn=False\n   DEBUG:  * len(loader) = 4\n   DEBUG:  * loader.batch_size = 64\n   vali loss:-1.000 \u2502 0.00% of 64x4... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 ESTDEBUG: Making batch iterator\n   DEBUG: Starting batch iteration for tag=vali, epoch=9\n   vali loss:0.215 \u2502 100.00% of 64x4... rate=29412.91 Hz, eta=0:00:00, total=0:00:00, wall=19:56 EST\n   DEBUG: log_value(vali epoch loss, 0.21514184772968292, 9\n   DEBUG: Finished batch iteration for tag=vali, epoch=9\n   DEBUG: epoch lr:0.001 \u2502 vloss: 0.2148 (n_bad=09, best=0.1749)\n   DEBUG: _run_epoch 9, tag=test, learn=False\n   DEBUG:  * len(loader) = 4\n   DEBUG:  * loader.batch_size = 64\n   test loss:-1.000 \u2502 0.00% of 64x4... rate=0 Hz, eta=?, total=0:00:00, wall=19:56 ESTDEBUG: Making batch iterator\n   DEBUG: Starting batch iteration for tag=test, epoch=9\n   test loss:0.216 \u2502 100.00% of 64x4... rate=25906.58 Hz, eta=0:00:00, total=0:00:00, wall=19:56 EST\n   DEBUG: log_value(test epoch loss, 0.21618007868528366, 9\n   DEBUG: Finished batch iteration for tag=test, epoch=9\n   DEBUG: Saving snapshot to /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots/_epoch_00000009.pt\n   DEBUG: Snapshot saved to /home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/torch_snapshots/_epoch_00000009.pt\n   DEBUG: Plotting tensorboard data\n   INFO: === finish epoch 9 / 10 : demo ===\n   DEBUG: epoch lr:0.001 \u2502 vloss: 0.2148 (n_bad=09, best=0.1749)\n   epoch lr:0.001 \u2502 vloss: 0.2148 (n_bad=09, best=0.1749) 10/10... rate=1.21 Hz, eta=0:00:00, total=0:00:08, wall=19:56 EST\n\n\n\n\n   INFO: Maximum harn.epoch reached, terminating ...\n   INFO: \n\n\n\n   INFO: training completed\n   INFO: harn.train_dpath = '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum'\n   INFO: harn.nice_dpath  = '/home/joncrall/.cache/netharn/demo/fit/nice/demo'\n   INFO: view tensorboard results for this run via:\n       tensorboard --logdir ~/.cache/netharn/demo/fit/nice\n   [DEPLOYER] Deployed zipfpath=/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/deploy_ToyNet2d_lnejaaum_000_JWPNDC.zip\n   INFO: wrote single-file deployment to: '/home/joncrall/.cache/netharn/demo/fit/runs/demo/lnejaaum/deploy_ToyNet2d_lnejaaum_000_JWPNDC.zip'\n   INFO: exiting fit harness.\n\n\n.. |Pypi| image:: https://img.shields.io/pypi/v/netharn.svg\n   :target: https://pypi.python.org/pypi/netharn\n\n.. |Downloads| image:: https://img.shields.io/pypi/dm/netharn.svg\n   :target: https://pypistats.org/packages/netharn\n\n.. |ReadTheDocs| image:: https://readthedocs.org/projects/netharn/badge/?version=latest\n    :target: http://netharn.readthedocs.io/en/latest/\n\n.. # See: https://ci.appveyor.com/project/jon.crall/netharn/settings/badges\n.. .. |Appveyor| image:: https://ci.appveyor.com/api/projects/status/py3s2d6tyfjc8lm3/branch/master?svg=true\n.. :target: https://ci.appveyor.com/project/jon.crall/netharn/branch/master\n\n.. |GitlabCIPipeline| image:: https://gitlab.kitware.com/computer-vision/netharn/badges/master/pipeline.svg\n   :target: https://gitlab.kitware.com/computer-vision/netharn/-/jobs\n\n.. |GitlabCICoverage| image:: https://gitlab.kitware.com/computer-vision/netharn/badges/master/coverage.svg?job=coverage\n    :target: https://gitlab.kitware.com/computer-vision/netharn/commits/master", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://gitlab.kitware.com/computer-vision/netharn", "keywords": "", "license": "Apache 2", "maintainer": "", "maintainer_email": "", "name": "netharn", "package_url": "https://pypi.org/project/netharn/", "platform": "", "project_url": "https://pypi.org/project/netharn/", "project_urls": {"Homepage": "https://gitlab.kitware.com/computer-vision/netharn"}, "release_url": "https://pypi.org/project/netharn/0.5.6/", "requires_dist": null, "requires_python": "", "summary": "Train and deploy pytorch models", "version": "0.5.6"}, "last_serial": 7018527, "releases": {"0.1.0": [{"comment_text": "", "digests": {"md5": "de828b21605ff5479bac5ee63781a0c0", "sha256": "038cb6d1bf08dcf98e3bbce28a26ef319d0193d7027199f75b5dee3844e2063e"}, "downloads": -1, "filename": "netharn-0.1.0-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "de828b21605ff5479bac5ee63781a0c0", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 255395, "upload_time": "2018-09-09T21:18:18", "upload_time_iso_8601": "2018-09-09T21:18:18.679202Z", "url": "https://files.pythonhosted.org/packages/28/55/c7dfe80db34b004aea09de73ff33aeb031f165ad9c5fa8323162d482a17c/netharn-0.1.0-py2.py3-none-any.whl"}], "0.1.1": [{"comment_text": "", "digests": {"md5": "a530a6f7979001575a5522d3caeeedc2", "sha256": "e8c984327a8f37dd0783dd904b74ab3b759ebd5225a26d34805278c32096b6ac"}, "downloads": -1, "filename": "netharn-0.1.1-py2.py3-none-any.whl", "has_sig": false, "md5_digest": "a530a6f7979001575a5522d3caeeedc2", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 262097, "upload_time": "2018-12-01T00:51:10", "upload_time_iso_8601": "2018-12-01T00:51:10.842822Z", "url": "https://files.pythonhosted.org/packages/18/46/930536b524a9b117d5bb0c1d0e56f861ec4ce7be250f8ca3befbbdaa6874/netharn-0.1.1-py2.py3-none-any.whl"}], "0.5.0": [{"comment_text": "", "digests": {"md5": "c750df02432eb4ead59b52a04617ac6f", "sha256": "af1a7e68257e8952eda2d53737813aaeffe0a06aa14195f71f99267533a33422"}, "downloads": -1, "filename": "netharn-0.5.0-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "c750df02432eb4ead59b52a04617ac6f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 350369, "upload_time": "2019-11-14T17:31:57", "upload_time_iso_8601": "2019-11-14T17:31:57.651934Z", "url": "https://files.pythonhosted.org/packages/21/5b/6abbce7ee70ec02c419e3ac451358c0555926e276a06b20bf42394740a2e/netharn-0.5.0-py2.py3-none-any.whl"}], "0.5.2": [{"comment_text": "", "digests": {"md5": "4394b22cc12a59bbdce085d335aeff1f", "sha256": "972ab26e57a436065fe14724db47065bc7ee7d5c7f25a33ffe4462f317994f39"}, "downloads": -1, "filename": "netharn-0.5.2-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "4394b22cc12a59bbdce085d335aeff1f", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 351922, "upload_time": "2019-11-25T20:04:04", "upload_time_iso_8601": "2019-11-25T20:04:04.332562Z", "url": "https://files.pythonhosted.org/packages/a8/f3/5bbcd591213084bd5350f3ec34c6fae538f43a4baa6b51b6f8dc8d958ca4/netharn-0.5.2-py2.py3-none-any.whl"}], "0.5.3": [{"comment_text": "", "digests": {"md5": "6cce6af92a750be72d291df549d4a12a", "sha256": "10dfd4e1263f3cc95d4882351bc6c9a4d3750683c0ee62e062184a85633d5601"}, "downloads": -1, "filename": "netharn-0.5.3-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "6cce6af92a750be72d291df549d4a12a", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 358363, "upload_time": "2020-01-31T03:26:46", "upload_time_iso_8601": "2020-01-31T03:26:46.140768Z", "url": "https://files.pythonhosted.org/packages/92/d7/1c29cdec97cab042b8f41aec229e859c049f2615f4e13eb6cf17c2b100f7/netharn-0.5.3-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "4a500dda2051a88df5dac5fbd4400337", "sha256": "f38f1ecf1512c302c9ac1d502084b95f1b180d530a638850bfb290844684efc3"}, "downloads": -1, "filename": "netharn-0.5.3.tar.gz", "has_sig": true, "md5_digest": "4a500dda2051a88df5dac5fbd4400337", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 313789, "upload_time": "2020-01-31T03:26:43", "upload_time_iso_8601": "2020-01-31T03:26:43.790607Z", "url": "https://files.pythonhosted.org/packages/a8/9d/79ea7f8f92935d81bfb19e151c8cb7417a72fec564d66542719250a506cd/netharn-0.5.3.tar.gz"}], "0.5.4": [{"comment_text": "", "digests": {"md5": "7bf2c863d436a61a058dd6a9a4c283b4", "sha256": "32fae5a2a589ae7cd8a8d69dbfd585edf4c770b988f32fa26d883dcbf8bea551"}, "downloads": -1, "filename": "netharn-0.5.4-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "7bf2c863d436a61a058dd6a9a4c283b4", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 430373, "upload_time": "2020-02-19T18:02:06", "upload_time_iso_8601": "2020-02-19T18:02:06.230659Z", "url": "https://files.pythonhosted.org/packages/01/4e/bf27728839a02c4cc2feba373ad10e522dd17042f485e0eb75dfb21541e5/netharn-0.5.4-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "6cab3d54d8011a41fda553d4f20ade77", "sha256": "5f28eb2f93cb637d5a3388dc0f8d3443b33f6e572f72992c513e1651ee5a36c0"}, "downloads": -1, "filename": "netharn-0.5.4.tar.gz", "has_sig": true, "md5_digest": "6cab3d54d8011a41fda553d4f20ade77", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 378322, "upload_time": "2020-02-19T18:02:03", "upload_time_iso_8601": "2020-02-19T18:02:03.683033Z", "url": "https://files.pythonhosted.org/packages/05/eb/be56fb10e0633582a1b7081ac88c20f8785feba8cf730972aee93206e683/netharn-0.5.4.tar.gz"}], "0.5.5": [{"comment_text": "", "digests": {"md5": "b7b9626b1ae309837219dc97cb707278", "sha256": "5152ee74e80add8fe3123d03d8d39491f4fdb04ae417240a57bbdc47755ba8ff"}, "downloads": -1, "filename": "netharn-0.5.5-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "b7b9626b1ae309837219dc97cb707278", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 454372, "upload_time": "2020-04-08T18:22:29", "upload_time_iso_8601": "2020-04-08T18:22:29.569508Z", "url": "https://files.pythonhosted.org/packages/82/49/4f9d8b33c8aa60a0024dba19a62639b1f9fe4bb6c975ad746834850de72e/netharn-0.5.5-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "f7820739e317f3a6a8f754c1db0359a8", "sha256": "b47bff018aca6fb072ecbac8bd43f6da96e5b1041b0b441fb9e1ea2797c7c972"}, "downloads": -1, "filename": "netharn-0.5.5.tar.gz", "has_sig": true, "md5_digest": "f7820739e317f3a6a8f754c1db0359a8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 394772, "upload_time": "2020-04-08T18:22:27", "upload_time_iso_8601": "2020-04-08T18:22:27.442171Z", "url": "https://files.pythonhosted.org/packages/8f/b6/0b8d291da896a23bbe21e882d61ea56e23ae8c577989e4ad39340c62d100/netharn-0.5.5.tar.gz"}], "0.5.6": [{"comment_text": "", "digests": {"md5": "facb214c1ee63f8c82627d2bcc1452ba", "sha256": "5c734de6decf7c260edbb19b76b7db7f4450d2264109ea34e93ff65ed66d73bc"}, "downloads": -1, "filename": "netharn-0.5.6-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "facb214c1ee63f8c82627d2bcc1452ba", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 455340, "upload_time": "2020-04-14T16:48:50", "upload_time_iso_8601": "2020-04-14T16:48:50.200373Z", "url": "https://files.pythonhosted.org/packages/a1/af/1f83e371e370e0be9f12975dd410e3f8b85516bcf7432ee7c3740adc0476/netharn-0.5.6-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "db2ea923752fb9c3b6155803a7afad4a", "sha256": "179b08193f38a1dae471c343327e0ef5949846f61daecda6bf7c47b5146aaf80"}, "downloads": -1, "filename": "netharn-0.5.6.tar.gz", "has_sig": true, "md5_digest": "db2ea923752fb9c3b6155803a7afad4a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 399617, "upload_time": "2020-04-14T16:48:47", "upload_time_iso_8601": "2020-04-14T16:48:47.385048Z", "url": "https://files.pythonhosted.org/packages/31/08/82ae08cfea501c25293d51076425ba487c394e0fe0babb00188d40ab8f99/netharn-0.5.6.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "facb214c1ee63f8c82627d2bcc1452ba", "sha256": "5c734de6decf7c260edbb19b76b7db7f4450d2264109ea34e93ff65ed66d73bc"}, "downloads": -1, "filename": "netharn-0.5.6-py2.py3-none-any.whl", "has_sig": true, "md5_digest": "facb214c1ee63f8c82627d2bcc1452ba", "packagetype": "bdist_wheel", "python_version": "py2.py3", "requires_python": null, "size": 455340, "upload_time": "2020-04-14T16:48:50", "upload_time_iso_8601": "2020-04-14T16:48:50.200373Z", "url": "https://files.pythonhosted.org/packages/a1/af/1f83e371e370e0be9f12975dd410e3f8b85516bcf7432ee7c3740adc0476/netharn-0.5.6-py2.py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "db2ea923752fb9c3b6155803a7afad4a", "sha256": "179b08193f38a1dae471c343327e0ef5949846f61daecda6bf7c47b5146aaf80"}, "downloads": -1, "filename": "netharn-0.5.6.tar.gz", "has_sig": true, "md5_digest": "db2ea923752fb9c3b6155803a7afad4a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 399617, "upload_time": "2020-04-14T16:48:47", "upload_time_iso_8601": "2020-04-14T16:48:47.385048Z", "url": "https://files.pythonhosted.org/packages/31/08/82ae08cfea501c25293d51076425ba487c394e0fe0babb00188d40ab8f99/netharn-0.5.6.tar.gz"}]}