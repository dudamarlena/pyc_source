{"info": {"author": "Thamme Gowda", "author_email": "tgowdan@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 5 - Production/Stable", "Intended Audience :: Developers", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3", "Topic :: Text Processing", "Topic :: Text Processing :: Filters", "Topic :: Text Processing :: General", "Topic :: Text Processing :: Linguistic", "Topic :: Utilities"], "description": "# NLCodec\nA set of (low level) Natural Language Encoder-Decoders (codecs), that are useful in preprocessing stage of \nNLP pipeline. These codecs include encoding of sequences into one of the following:\n1. Character\n2. Word\n3. BPE based subword\n\nIt provides python (so embed into your app) and CLI APIs (use it as stand alone tool).\n\nThere are many BPE implementations available already, but this one provides differs:\n1. Pure python implementation that is easy to modify anything to try new ideas. \n  (other implementations require c++ expertise to modify the core) \n2. BPE model is a simple text that can be inspected with `less` or `cut`. It includes info on which pieces were put together and what frequencies etc. \n3. Reasonably faster than the other pure python implementations -- speed in python comes with the cost of extra memory due to indexing.\n\n\n# Installation \nPlease run only one of these\n```bash\n# Clone repo for development mode (preferred  mode)\ngit clone https://github.com/isi-nlp/nlcodec\ncd nlcodec\npip install --editable . \n\n# Install from github, directly\n$ pip install git+https://github.com/isi-nlp/nlcodec.git\n\n\n# Install from pypi\n$ pip install nlcodec\n```\npip installer registers a cli tool named `nlcodec` in PATH\n which serves is the command line interface.\n  You can always trigger either via `python -m nlcodec` or \n `python path/to/nlcodec/__main__.py` if you wish!\n\n\n## Usage \n```bash\n$ python -m nlcodec -h\nusage: __main__.py [-h] [-i INP] [-o OUT] -m MODEL [-idx] [-vs VOCAB_SIZE]\n                   [-l {char,word,bpe}] [-mf MIN_FREQ]\n                   {learn,encode,decode,estimate}\n\npositional arguments:\n  {learn,encode,decode,estimate}\n                        \"task\" or sub-command.\n                            \"learn\" - learns vocabulary. use --level and vocab_size for type and size \n                            \"encode\" - encodes a dataset \n                            \"decode\" - decodes an already encoded dataset\n                            \"estimate\" - estimates quality attributes of an encoding\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -i INP, --inp INP     Input file path (default: <_io.TextIOWrapper\n                        name='<stdin>' mode='r' encoding='UTF-8'>)\n  -o OUT, --out OUT     Output file path. Not valid for \"learn\" or \"estimate\"\n                        task (default: <_io.TextIOWrapper name='<stdout>'\n                        mode='w' encoding='UTF-8'>)\n  -m MODEL, --model MODEL\n                        Path to model aka vocabulary file (default: None)\n  -idx, --indices       Indices instead of strings. Valid for task=encode and\n                        task=decode (default: None)\n\nargs for task=learn:\n  -vs VOCAB_SIZE, --vocab_size VOCAB_SIZE\n                        Vocabulary size. Valid only for task=learn. This is\n                        required for \"bpe\", but optional for \"word\" and \"char\"\n                        models, specifying it will trim the vocabulary at\n                        given top most frequent types. (default: -1)\n  -l {char,word,bpe}, --level {char,word,bpe}\n                        Encoding Level; Valid only for task=learn (default:\n                        None)\n  -mf MIN_FREQ, --min_freq MIN_FREQ\n                        Minimum frequency of types for considering inclusion\n                        in vocabulary. Types fewer than this frequency will be\n                        ignored. For --level=word, freq is type freq and\n                        default is 2.for --level=char or --level=bpe,\n                        characters fewer than this value will be excluded.\n                        default=20 (default: None)\n\n```\n\nExample: \n\n```\n# learn\nhead -2000 somefile.tok | nlcodec learn -l bpe -m bpe.model --vocab_size 2000\n\n# encode  with text pieces\nhead  somefile.tok  | nlcodec encode -m bpe.model\n\n# encode with indexes\nhead  somefile.tok  | nlcodec encode -m bpe.model -idx\n\n# decode -- undo encoding\nhead  somefile.tok  | nlcodec decode -m bpe.model\nhead  somefile.tok  | nlcodec decode -m bpe.model -idx\n\n# estimate quality \nhead  somefile.tok  | nlcodec estimate -m bpe.model\n\n```\n\n## Python API\n\n### Using a vocabulary\n```python\nfrom nlcodec import  load_scheme\npath = 'path/to/vocab.model'\nvocab = load_scheme(path)\n\nline = 'this is a sample sentence'\n# encode a line of text into list of ids\nvocab.encode(line)\n\n# parallel encode a bunch of lines using multiple cpus\nvocab.encode_parallel(seqs=[line], n_cpus=2)\n\n# encode a line of text into pieces \nvocab.encode_str(line)\n\n# decode\nvocab.decode(vocab.encode(line))\nvocab.decode_str(vocab.encode_str(line))\n```\n\n### Creating a vocabulary\n```python\nfrom nlcodec import learn_vocab\ninp = ['line 1', 'line 2']\nlevel = 'bpe' # other options = char, word\nmodel = 'path/to/vocab.model'\nlearn_vocab(inp, level, model, vocab_size=8000, min_freq=1, char_coverage=0.9995)\n```\n\n\n### BPE Subword sub optimal splits for regularization\n\n```python\nfrom nlcodec import load_scheme, BPEScheme\npath = 'path/to/bpe-vocab.model'\nbpe: BPEScheme = load_scheme(path)\nsome_type = bpe.table[1000] # select some bpe piece type\n\n# get stochastic split\nsome_type.get_stochastic_split(split_ratio=0.5, name=False)\n# get all possible permutations \nsome_type.get_permutations(name=False)\n\n```\n\n\n# Authors \n+ [Thamme Gowda](https://twitter.com/thammegowda) \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/thammegowda/bpepp", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/thammegowda/bpepp", "keywords": "", "license": "Apache Software License 2.0", "maintainer": "", "maintainer_email": "", "name": "nlcodec", "package_url": "https://pypi.org/project/nlcodec/", "platform": "any", "project_url": "https://pypi.org/project/nlcodec/", "project_urls": {"Download": "https://github.com/thammegowda/bpepp", "Homepage": "https://github.com/thammegowda/bpepp"}, "release_url": "https://pypi.org/project/nlcodec/0.2.0/", "requires_dist": ["tqdm"], "requires_python": ">=3.7", "summary": "nlcodec is a collection of encoding schemes for natural language sequences", "version": "0.2.0"}, "last_serial": 7038219, "releases": {"0.2.0": [{"comment_text": "", "digests": {"md5": "759fdfbb68a554aee2e63b1ba09d60d0", "sha256": "8b7e42fa0980da324d63f2699808a5ed2ad9bda139f80e2e264df5b2cd42bc20"}, "downloads": -1, "filename": "nlcodec-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "759fdfbb68a554aee2e63b1ba09d60d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 25912, "upload_time": "2020-04-17T07:30:06", "upload_time_iso_8601": "2020-04-17T07:30:06.470539Z", "url": "https://files.pythonhosted.org/packages/87/96/24a57da91a3e5fd5a9f878a877acf01004d6bea431a32f5f76ed5f9aa144/nlcodec-0.2.0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "97415757dd63f22b945d43f1ec269d28", "sha256": "52e99b817a449ce1b42b7a7b4b502873a7af2e6a1987a0d32c0ef71098b8db38"}, "downloads": -1, "filename": "nlcodec-0.2.0.tar.gz", "has_sig": false, "md5_digest": "97415757dd63f22b945d43f1ec269d28", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 20695, "upload_time": "2020-04-17T07:30:09", "upload_time_iso_8601": "2020-04-17T07:30:09.035508Z", "url": "https://files.pythonhosted.org/packages/8c/fd/cb5dab68dacf599668a852df986942b3d970cf676c879d44e0aae6f08866/nlcodec-0.2.0.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "759fdfbb68a554aee2e63b1ba09d60d0", "sha256": "8b7e42fa0980da324d63f2699808a5ed2ad9bda139f80e2e264df5b2cd42bc20"}, "downloads": -1, "filename": "nlcodec-0.2.0-py3-none-any.whl", "has_sig": false, "md5_digest": "759fdfbb68a554aee2e63b1ba09d60d0", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.7", "size": 25912, "upload_time": "2020-04-17T07:30:06", "upload_time_iso_8601": "2020-04-17T07:30:06.470539Z", "url": "https://files.pythonhosted.org/packages/87/96/24a57da91a3e5fd5a9f878a877acf01004d6bea431a32f5f76ed5f9aa144/nlcodec-0.2.0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "97415757dd63f22b945d43f1ec269d28", "sha256": "52e99b817a449ce1b42b7a7b4b502873a7af2e6a1987a0d32c0ef71098b8db38"}, "downloads": -1, "filename": "nlcodec-0.2.0.tar.gz", "has_sig": false, "md5_digest": "97415757dd63f22b945d43f1ec269d28", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.7", "size": 20695, "upload_time": "2020-04-17T07:30:09", "upload_time_iso_8601": "2020-04-17T07:30:09.035508Z", "url": "https://files.pythonhosted.org/packages/8c/fd/cb5dab68dacf599668a852df986942b3d970cf676c879d44e0aae6f08866/nlcodec-0.2.0.tar.gz"}]}