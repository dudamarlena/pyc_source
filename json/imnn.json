{"info": {"author": "Tom Charnock", "author_email": "charnock@iap.fr", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3.7"], "description": "\n\n```python\n%load_ext autoreload\n%autoreload 2\n```\n\n# Information maximiser\n\nUsing neural networks, sufficient statistics can be obtained from data by maximising the Fisher information.\n\nThe neural network takes some data ${\\bf d}$ and maps it to a compressed summary $\\mathscr{f}:{\\bf d}\\to{\\bf x}$ where ${\\bf x}$ can have the same size as the dimensionality of the parameter space, rather than the data space.\n\nTo train the neural network a batch of simulations ${\\bf d}_{\\sf sim}^{\\sf fid}$ created at a fiducial parameter value $\\boldsymbol{\\theta}^{\\rm fid}$ are compressed by the neural network to obtain ${\\bf x}_{\\sf sim}^{\\sf fid}$. From this we can calculate the covariance ${\\bf C_\\mathscr{f}}$ of the compressed summaries. We learn about model parameter distributions using the derivative of the simulation. This can be provided analytically or numercially using ${\\bf d}_{\\sf sim}^{\\sf fid+}$ created above the fiducial parameter value $\\boldsymbol{\\theta}^{\\sf fid+}$ and ${\\bf d}_{\\sf sim}^{\\sf fid-}$ created below the fiducial parameter value $\\boldsymbol{\\theta}^{\\sf fid-}$. The simulations are compressed using the network and used to find mean of the summaries\n$$\\frac{\\partial\\boldsymbol{\\mu}_\\mathscr{f}}{\\partial\\theta_\\alpha}=\\frac{1}{n_\\textrm{derivs}}\\sum_{i=1}^{n_\\textrm{derivs}}\\frac{{\\bf x}_{\\sf sim}^{\\sf fid+}-{\\bf x}_{\\sf sim}^{\\sf fid-}}{\\boldsymbol{\\theta}^{\\sf fid+}-\\boldsymbol{\\theta}^{\\sf fid-}}.$$\nIf the derivative of the simulations with respect to the parameters can be calculated analytically (or via autograd, etc.) then that can be used directly using the chain rule since the derivative of the network outputs with respect to the network input can be calculated easily\n$$\\frac{\\partial\\mu_\\mathscr{f}}{\\partial\\theta_\\alpha} = \\frac{1}{n_{\\textrm{sims}}}\\sum_{i=1}^{n_{\\textrm{sims}}}\\frac{\\partial{\\bf x}_i}{\\partial{\\bf d}_j}\\frac{\\partial{\\bf d}_j}{\\partial\\theta_\\alpha}\\delta_{ij}.$$\nWe then use ${\\bf C}_\\mathscr{f}$ and $\\boldsymbol{\\mu}_\\mathscr{f},_\\alpha$ to calculate the Fisher information\n$${\\bf F}_{\\alpha\\beta} = \\boldsymbol{\\mu}_\\mathscr{f},^T_{\\alpha}{\\bf C}^{-1}_\\mathscr{f}\\boldsymbol{\\mu}_\\mathscr{f},_{\\beta}.$$\nWe want to maximise the Fisher information, and we want the summaries to be orthogonal so to train the network we minimise the loss function\n$$\\Lambda = -\\ln|{\\bf F}_{\\alpha\\beta}|+r(\\Lambda_2)\\Lambda_2$$\nwhere\n$$\\Lambda_2 = ||{\\bf C}_\\mathscr{f}-\\mathbb{1}||_2 + ||{\\bf C}_\\mathscr{f}^{-1}-\\mathbb{1}||_2$$\nis a regularisation term whose strength is dictated by\n$$r(\\Lambda_2) = \\frac{\\lambda\\Lambda_2}{\\Lambda_2 + \\exp(-\\alpha\\Lambda_2)}$$\nwhere $\\lambda$ is a strength and $\\alpha$ is a rate parameter which can be determined from a closeness condition on the Frobenius norm of the difference between the convariance (and inverse covariance) from the identity matrix.\n\nWhen using this code please cite <a href=\"https://arxiv.org/abs/1802.03537\">arXiv:1802.03537</a>.<br><br>\nThe code in the paper can be downloaded as v1 or v1.1 of the code kept on zenodo:<br><br>\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.1175196.svg)](https://doi.org/10.5281/zenodo.1175196)\n<br>\n\nThis code is run using<br>\n>`python v3.7.4`\n\n>`tensorflow==2.0.0`\n\n>`numpy==1.16.2`\n\n>`tqdm==4.31.1`\n\nAlthough these precise versions may not be necessary, I have put them here to avoid possible conflicts.\n\n## Load modules\n\n\n```python\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom IMNN import IMNN\nfrom IMNN.ABC import ABC, priors\n```\n\n# Summarising the mean and the variance\n\nFor this example we are going to use $n_{\\bf d}=10$ data points of a 1D field of Gaussian noise with unknown mean and variance to see if the network can learn to summarise them.<br><br>\n\nThe likelihood is given by\n$$\\mathcal{L} = \\prod_i^{n_{\\bf d}}\\frac{1}{\\sqrt{2\\pi|\\Sigma|}}\\exp\\left[-\\frac{1}{2}\\frac{(d_i-\\mu)^2}{\\Sigma}\\right]$$\n\nWe can solve this problem analytically, so it is useful to check how well the network does. There is a single sufficient statistic which describes each the mean and the variance, which can be found by finding the maximum of the probability. We find that\n$$\\sum_i^{n_{\\bf d}}d_i = \\mu\\textrm{ and }\\sum_i^{n_{\\bf d}}(d_i-\\mu)^2=n_{\\bf d}\\Sigma$$\n\nWe can calculate the Fisher information by taking the negative of second derivative of the likelihood taking the expectation by inserting the above relations at examining at some fiducial parameter values\n$${\\bf F}_{\\alpha\\beta} = -\\left.\\left(\\begin{array}{cc}\\displaystyle-\\frac{n_{\\bf d}}{\\Sigma}&0\\\\0&\\displaystyle-\\frac{n_{\\bf d}}{2\\Sigma^2}\\end{array}\\right)\\right|_{\\textrm{fiducial}}.$$\nIf we choose a fiducial mean of $\\mu^{\\textrm{fid}}=0$ and variance of $\\Sigma^{\\textrm{fid}} = 1$ then we obtain a Fisher information matrix of\n\n\n\n\n\n```python\ninput_shape = (10,)\n\nn_params = 2\n\u03b8_fid = np.array([0, 1.])\n```\n\n\n```python\nexact_fisher = -np.array([[-input_shape[0] / \u03b8_fid[1], 0.], [0. , - 0.5 * input_shape[0] / \u03b8_fid[1]**2.]])\ndeterminant_exact_fisher = np.linalg.det(exact_fisher)\nprint(\"determinant of the Fisher information\", determinant_exact_fisher)\nplt.imshow(np.linalg.inv(exact_fisher))\nplt.title(\"Inverse Fisher matrix\")\nplt.xticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.yticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.colorbar();\n```\n\n    determinant of the Fisher information 50.000000000000014\n\n\n\n![png](figures/output_9_1.png)\n\n\nLet us observe our _real_ data which happens to have true parameters $\\mu=3$ and $\\Sigma=2$\n\n\n```python\nreal_data = np.random.normal(3., np.sqrt(2.), size = (1,) + input_shape)\n```\n\n\n```python\nfig, ax = plt.subplots(1, 1, figsize = (10, 6))\nax.plot(real_data[0], label = \"observed data\")\nax.legend(frameon = False)\nax.set_xlim([0, 9])\nax.set_xticks([])\nax.set_ylabel(\"Data amplitude\");\n```\n\n\n![png](figures/output_12_0.png)\n\n\nThe posterior distribution for this data (normalised to integrate to 1) is\n\n\n```python\n\u03bc_array = np.linspace(-10, 10, 1000)\n\u03a3_array = np.linspace(0.001, 10, 1000)\n\nparameter_grid = np.array(np.meshgrid(\u03bc_array, \u03a3_array, indexing = \"ij\"))\ndx = (\u03bc_array[1] - \u03bc_array[0]) * (\u03a3_array[1] - \u03a3_array[0])\n\nanalytic_posterior = np.exp(-0.5 * (np.sum((real_data[0][:, np.newaxis] - parameter_grid[0, :, 0][np.newaxis, :])**2., axis = 0)[:, np.newaxis] / parameter_grid[1, 0, :][np.newaxis, :] + real_data.shape[1] * np.log(2. * np.pi * parameter_grid[1, 0, :][np.newaxis, :])))\nanalytic_posterior = analytic_posterior.T / np.sum(analytic_posterior * dx)\n```\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior, axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 1].plot(np.sum(analytic_posterior, axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_15_0.png)\n\n\nNow lets see how the information maximising neural network can recover this posterior.\n\n## Generate data\n\nWe start by defining a function to generate the data with the correct shape. The shape must be\n```\ndata_shape = (None,) + input_shape\n```\n\nIt is useful to define the generating function so that it only takes in the value of the parameter as its input since the function can then be used for ABC later.<br><br>\nThe data needs to be generated at a fiducial parameter value and at perturbed values just below and above the fiducial parameter for the numerical derivative.\n\n\n```python\n\u03b4\u03b8 = np.array([0.1, 0.1])\n```\n\nThe training and validation data should be generated at the perturbed values with shape\n```\nperturbed_data_shape = (None, 2, n_params) + input_shape\n```\nif using numerical derivatives, where `[:, 0, ...]` is the parameter at `\u0394\u03b8pm` below `\u03b8_fid` and `[:, 1, ...]` is the parameter at `\u0394\u03b8pm` above `\u03b8_fid`. This is done for each parameter keeping ever non-perturbed parameter at its fiducial parameter.\n\nIf the true derivative of the simulations with respect to the parameters is available then this can be calculated for each fiducial simulation in the dataset with shape\n```\nderivative_data_shape = (None, n_params) + input_shape\n```\n\nThe generating function is defined so that the fiducial parameter is passed as a list so that many simulations can be made at once. This is very useful for the ABC function later.\n\n\n```python\ndef simulator(\u03b8, seed, simulator_args):\n    if seed is not None:\n        np.random.seed(seed)\n    if len(\u03b8.shape) > 1:\n        \u03bc = \u03b8[:, 0]\n        \u03a3 = \u03b8[:, 1]\n    else:\n        \u03bc = 0.\n        \u03a3 = \u03b8\n    return np.moveaxis(np.random.normal(\u03bc, np.sqrt(\u03a3), simulator_args[\"input shape\"] + (\u03b8.shape[0],)), -1, 0)\n```\n\n### Training data\nEnough data needs to be made to approximate the covariance matrix of the output summaries. The number of simulations needed to approximate the covariance is `n_s`. We can make stochastic updates by making `n_train` more simulations than are needed to approximate the covariance. `n_train` should be an integer number.\n\n\n```python\nn_s = 1000\nn_train = 1\nseed = np.random.randint(1e6)\n```\n\nThe training data can now be made\n\n\n```python\nd = simulator(\u03b8 = np.tile(\u03b8_fid, [n_train * n_s, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\n```\n\nIdeally we would be able to take the derivative of our simulations with respect to the model parameters. We can indeed do that in this case, but since this is possibly a rare occurance I will show an example where the derivatives are calculated numerically. By suppressing the sample variance between the simulations created at some lower and upper varied parameter values, far fewer simulations are needed. We should choose the extra `n_train` simulations for the stochastic updates to be the same as for the fiducial simulations.\n\n\n```python\nn_d = 1000\n```\n\nThe sample variance is supressed by choosing the same initial seed when creating the upper and lower simulations.\n\n\n```python\ndd_m_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0.1, 0.]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ndd_p_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0.1, 0.]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ndd_m_d\u03b8 = np.stack([dd_m_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0., 0.1]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ndd_p_d\u03b8 = np.stack([dd_p_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0., 0.1]), [n_train * n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ndd_d\u03b8_num = np.concatenate([dd_m_d\u03b8[:, np.newaxis, ...], dd_p_d\u03b8[:, np.newaxis, ...]], axis=1)\ndd_d\u03b8 = (dd_p_d\u03b8 - dd_m_d\u03b8) / (2. * \u03b4\u03b8)[np.newaxis, :, np.newaxis]\n```\n\n### Test data\nWe should also make some test data, but here we will use only one combination. This needs adding to the dictionary\n\n\n```python\nseed = np.random.randint(1e6)\ntd = simulator(\u03b8 = np.tile(\u03b8_fid, [n_s, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ntdd_m_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0.1, 0.]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ntdd_p_d\u03b8 = simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0.1, 0.]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})\ntdd_m_d\u03b8 = np.stack([tdd_m_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid - np.array([0., 0.1]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ntdd_p_d\u03b8 = np.stack([tdd_p_d\u03b8, simulator(\u03b8 = np.tile(\u03b8_fid + np.array([0., 0.1]), [n_d, 1]), seed = seed, simulator_args = {\"input shape\": input_shape})], axis = 1)\ntdd_d\u03b8_num = np.concatenate([tdd_m_d\u03b8[:, np.newaxis, ...], tdd_p_d\u03b8[:, np.newaxis, ...]], axis=1)\ntdd_d\u03b8 = (tdd_p_d\u03b8 - tdd_m_d\u03b8) / (2. * \u03b4\u03b8)[np.newaxis, :, np.newaxis]\n```\n\n### Data visualisation\nWe can plot the data to see what it looks like.\n\n\n```python\nfig, ax = plt.subplots(1, 1, figsize = (10, 6))\nax.plot(d[np.random.randint(n_train * n_s)], label = \"training data\")\nax.plot(td[np.random.randint(n_s)], label = \"test data\")\nax.legend(frameon = False)\nax.set_xlim([0, 9])\nax.set_xticks([])\nax.set_ylabel(\"Data amplitude\");\n```\n\n\n![png](figures/output_33_0.png)\n\n\nIt is also very useful to plot the upper and lower derivatives to check that the sample variance is actually supressed since the network learns extremely slowly if this isn't done properly.\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (20, 10))\nplt.subplots_adjust(hspace = 0)\ntraining_index = np.random.randint(n_train * n_d)\ntest_index = np.random.randint(n_d)\n\nax[0, 0].plot(dd_m_d\u03b8[training_index, 0], label = \"lower training data\", color = \"C0\", linestyle = \"dashed\")\nax[0, 0].plot(dd_p_d\u03b8[training_index, 0], label = \"upper training data\", color = \"C0\")\nax[0, 0].plot(tdd_m_d\u03b8[test_index, 0], label = \"lower validation data\", color = \"C1\", linestyle = \"dashed\")\nax[0, 0].plot(tdd_p_d\u03b8[test_index, 0], label = \"upper validation data\", color = \"C1\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([0, 9])\nax[0, 0].set_xticks([])\nax[0, 0].set_ylabel(\"Data amplitude with varied mean\")\nax[1, 0].plot(dd_d\u03b8[training_index, 0], label = \"derivative training data\", color = \"C0\")\nax[1, 0].plot(tdd_d\u03b8[test_index, 0], label = \"derivative validation data\", color = \"C1\")\nax[1, 0].set_xlim([0, 9])\nax[1, 0].set_xticks([])\nax[1, 0].legend(frameon = False)\nax[1, 0].set_ylabel(\"Amplitude of the derivative of the data\\nwith respect to the mean\");\n\nax[0, 1].plot(dd_m_d\u03b8[training_index, 1], label = \"lower training data\", color = \"C0\", linestyle = \"dashed\")\nax[0, 1].plot(dd_p_d\u03b8[training_index, 1], label = \"upper training data\", color = \"C0\")\nax[0, 1].plot(tdd_m_d\u03b8[test_index, 1], label = \"lower validation data\", color = \"C1\", linestyle = \"dashed\")\nax[0, 1].plot(tdd_p_d\u03b8[test_index, 1], label = \"upper validation data\", color = \"C1\")\nax[0, 1].legend(frameon = False)\nax[0, 1].set_xlim([0, 9])\nax[0, 1].set_xticks([])\nax[0, 1].set_ylabel(\"Data amplitude with varied covariance\")\nax[1, 1].plot(dd_d\u03b8[training_index, 1], label = \"derivative training data\", color = \"C0\")\nax[1, 1].plot(tdd_d\u03b8[test_index, 1], label = \"derivative validation data\", color = \"C1\")\nax[1, 1].set_xlim([0, 9])\nax[1, 1].set_xticks([])\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylabel(\"Amplitude of the derivative of the data\\nwith respect to covariance\");\n```\n\n\n![png](figures/output_35_0.png)\n\n\n## Initiliase the neural network\n\nWe will choose a fairly generic network as an example here. These networks can be written in TensorFlow or keras. It must be able to take in `(None,) + input_shape` and `(None, n_params) + input_shape` or `(None, 2, n_params) + input_shape` and output a flat array with `n_summaries` outputs, i.e. `(None, n_summaries` and `(None, n_params, n_summaries)` or `(None, 2, n_params, n_summaries)`.\n\nIn principle `n_summaries` can be any number, but information loss is guaranteed if `n_summaries < n_params` and we overparameterise the summaries if we use `n_summaries>n_params`. Therefore, in principle, we should use `n_summaries=n_params`. Note that this isn't necessarily true if external informative summaries are included in the training.\n\n\n```python\nn_summaries = n_params\n```\n\nIn keras we shall define a network with two hidden layers with 128 hidden nodes in each and every layer apart from the output activated using `tanh`. We shall optimise the network using the `Adam` optimiser on its default settings\n\n\n```python\nmodel = tf.keras.Sequential(\n    [tf.keras.Input(shape=input_shape),\n     tf.keras.layers.Dense(128),\n     tf.keras.layers.Activation(\"tanh\"),\n     tf.keras.layers.Dense(128),\n     tf.keras.layers.Activation(\"tanh\"),\n     tf.keras.layers.Dense(n_summaries),\n    ])\nopt = tf.keras.optimizers.Adam()\n```\n\n## Initialise the IMNN module\n\nThe IMNN only needs to be provided with the number of parameters in the model, the number of summaries output by the network and the number of simulations needed to approximate the covariance of the summaries and to approximate the mean of the derivative of the summaries with respect to the parameters. Optionally we can also choose whether to use 32 or 64 bit floats (and ints) in TensorFlow and choose whether or not to get verbose size checking whilst loading the model and the data.\n\n\n```python\nimnn = IMNN.IMNN(n_params=n_params, n_summaries=n_summaries, n_covariance_sims=n_s, n_derivative_sims=n_d, dtype=tf.float32, verbose=True)\n```\n\n    Using single dataset\n\n\nThe network is then passed to the module using\n\n\n```python\nimnn.set_model(model=model, optimiser=opt)\n```\n\n    Checking is not currently done on the model. Make sure that its output has shape (None, 2) for the fiducial values a nd (None, 2, 2, 2) for the derivative values.\n\n\n## Load the data\n\nThe data is passed as a TensorFlow data set for extremely quick and efficient transfer to GPUs, etc. For most reasonable sized datasets (GPU/CPU memory dependent of course) we can load this using\n\n\n```python\nimnn.setup_dataset(\u03b8_fid=\u03b8_fid, d=d, dd_d\u03b8=dd_d\u03b8_num, \u03b4\u03b8=\u03b4\u03b8, external_summaries=None, external_derivatives=None)\n```\n\nSince we might already know some very informative summaries of the data, we can include these. To do so the external summaries can be passed to the above function as numpy arrays. The summaries must have the same structure as the data (and aligned with the data) where `input_shape` is replaced with a flat array with `n_external` elements.\n\nThe dataset can be setup externally by setting the data attributes\n```python\nimnn.dataset = tf.data.Dataset.from_tensor_slices(data)\nimnn.dataset = imnn.dataset.batch(imnn.n_s)\nimnn.dataset = imnn.dataset.shuffle(n_train * imnn.n_s)\n```\nwhere `data=(fiducial_data, derivative_data)` is a tuple containing the fiducial and the derivative simulations. If the number of parameters for the mean of the derivatives is different to the number needed for the covariance then `data` should not be a tuple and instead just contain the fiducial simulations and instead the derivatives can be passed as\n```python\nimnn.derivative_dataset = tf.data.Dataset.from_tensor_slices(derivative_data)\nimnn.dataset = imnn.dataset.batch(imnn.n_d)\nimnn.dataset = imnn.dataset.shuffle(n_train * imnn.n_d)\n```\nIf the derivative is done numerically then `imnn.numerical=True` must be set, and likewise `imnn.numerical=False` must be used otherwise. The fiducial parameters should be set using\n```python\nimnn.\u03b8_fid = tf.Variable(\u03b8_fid, dtype=imnn.dtype)\n```\nand if using the numerical derivative we also need to set\n```python\nimnn.\u03b4\u03b8 = tf.Variable(1. / (2. * \u03b4\u03b8), dtype=imnn.dtype)\n```\nIf external informative summaries are going to be used then the when the number of simulations for the derivative of the mean of the summaries and the number of simulations for the covariance of the summaries is the same the data tuple should be `data=(fiducial_data, derivative_data, external_summaries, external_derivatives)` and when then number is different the tuples should be different `data=(fiducial_data, external_summaries`) and `derivatives=(derivative_data, external_derivatives)`.\n\nAlthough there is little point in doing the above externally, this could be extremely useful for large datasets which are written as TFRecords.\n\nThe validation data can be passed using\n\n\n```python\nimnn.setup_dataset(\u03b8_fid=\u03b8_fid, d=td, dd_d\u03b8=tdd_d\u03b8_num, \u03b4\u03b8=\u03b4\u03b8, external_summaries=None, external_derivatives=None, train=False)\n```\n\nThe same external setup can be be performed exactly as above but with `test_` appended before each attribute name.\n\n## Train the network\n\nSince we need to constrain the scale of the summaries (remember, every linear rescaling of a summary is also a summary) we choose to constrain the summaries to have a covariance close to the identity matrix. This makes the summaries somewhat (and ideally exactly) orthogonal. To enforce this scale we have a regulariser which is the Frobenius norm of the difference between the covariance matrix and the identity matrix and also the difference betwee the inverse covariance matrix and the identity matrix. How close the covariance gets to the identity is controlled by a strength `\u03bb` parameter and a closeness `\u03f5` parameter. These determine a rate of convergence \u03b1 which determines how sharp the loss function is (if it's too sharp then training can be unstable). We choose an `\u03f5=0.01` and a strength of `\u03bb=10`\n\n\n```python\nimnn.set_regularisation_strength(\u03f5=0.01, \u03bb=10.)\n```\n\nWe can now train the network. We just need to choose a number iterations of training updates (equivalent to full passes through the dataset). To validate we just pass the `validate=True` option.\n\n\n```python\nimnn.fit(n_iterations=1000, validate=True)\n```\n\n\n    HBox(children=(IntProgress(value=0, description='Iterations', max=1000, style=ProgressStyle(description_width=\u2026\n\n\nTraining can be continued simply by running the fit again.\n\nThe network can also be reinitialised before training if something goes awry by running\n```python\nn.fit(n_iterations=1000, validate=True, reset=True)\n```\n\nNote that it is normal for the network to initially run slow and quite quickly speed up as the data flow from the dataset to the CPU is properly filled.\n\nA history (imnn.history) dictionary is collecting diagnostics. It contains\n- `det_F` : determinant of the Fisher information\n- `det_C` : determinant of the covariance of the summaries\n- `det_Cinv` : determinant of the inverse covariance of the summaries\n- `det_d\u03bc_d\u03b8` : derivative of the mean of the summaries with respect to the parameters\n- `reg` : value of the regulariser\n- `r` : value of the regulariser strength\n\nand the same for the validation set\n\n- `val_det_F` : determinant of the Fisher information for the validation set\n- `val_det_C` : determinant of the covariance of the summaries from the validation set\n- `val_det_Cinv` : determinant of the inverse covariance of the summaries from the validation set\n- `val_det_d\u03bc_d\u03b8` : derivative of the mean of the summaries from the validation set with respect to the parameters\n- `val_reg` : value of the regulariser for the validation set\n- `val_r` : value of the regulariser strength for the validation set\n\n\n```python\nfig, ax = plt.subplots(4, 1, sharex = True, figsize = (10, 20))\nplt.subplots_adjust(hspace = 0)\nepochs = np.arange(1, len(imnn.history[\"det_F\"]) + 1)\nax[0].plot(epochs, imnn.history[\"det_F\"], color=\"C0\",\n           label=r'$|{\\bf F}_{\\alpha\\beta}|$ from training data')\nax[0].plot(epochs, imnn.history[\"val_det_F\"], color=\"C1\",\n           label=r'$|{\\bf F}_{\\alpha\\beta}|$ from validation data')\nax[0].axhline(determinant_exact_fisher, color=\"black\", linestyle=\"dashed\")\nax[0].legend(frameon=False)\nax[0].set_xlim([1, epochs[-1]])\nax[0].set_ylabel(r\"$|{\\bf F}_{\\alpha\\beta}|$\")\nax[1].plot(epochs, imnn.history[\"det_C\"], color=\"C0\",\n           label=r'$|{\\bf C}|$ from training data')\nax[1].plot(epochs, imnn.history[\"val_det_C\"], color=\"C1\",\n           label=r'$|{\\bf C}|$ from validation data')\nax[1].plot(epochs, imnn.history[\"det_Cinv\"], color=\"C0\", linestyle=\"dashed\",\n           label=r'$|{\\bf C}^{-1}|$ from training data')\nax[1].plot(epochs, imnn.history[\"val_det_Cinv\"], color=\"C1\", linestyle=\"dashed\",\n           label=r'$|{\\bf C}^{-1}|$ from validation data')\nax[1].axhline(1., color=\"black\", linestyle=\"dashed\")\nax[1].legend(frameon=False, loc=\"best\")\nax[1].set_ylabel(r\"$|{\\bf C}|$ and $|{\\bf C}^{-1}|$\")\nax[1].set_xlim([1, epochs[-1]])\nax[1].set_yscale(\"log\")\nax[2].plot(epochs, imnn.history[\"reg\"],\n           label=r'$\\Lambda_2$ from training data')\nax[2].plot(epochs, imnn.history[\"val_reg\"],\n           label=r'$\\Lambda_2$ from validation data')\nax[2].legend(frameon=False)\nax[2].set_ylabel(r\"$\\Lambda_2$\")\nax[2].set_xlim([1, epochs[-1]])\nax[3].plot(epochs, imnn.history[\"r\"],\n           label=r'$r$ from training data')\nax[3].plot(epochs, imnn.history[\"val_r\"],\n           label=r'$r$ from validation data')\nax[3].legend(frameon=False)\nax[3].set_ylabel(r\"$r$\")\nax[3].set_xlim([1, epochs[-1]])\nax[3].set_xlabel(\"Number of epochs\");\n```\n\n\n![png](figures/output_58_0.png)\n\n\n## Maximum likelihood estimates\n\nThe IMNN can provide maximum likelihood estimates of model parameters by initialising\n\n\n```python\nimnn.setup_MLE()\n```\n\nNote this only works when a validation set is already loaded. It uses the validation set to calculate the Fisher information, covariance, derivative of the mean summaries and therefore the compression and transformation into MLE space. If a different dataset is to be used then it can be provided by setting `dataset=False` and providing all the necessary extra data\n```python\nimnn.setup_MLE(dataset=True, \u03b8_fid=None, d=None, dd_d\u03b8=None, \u03b4\u03b8=None, s=None, ds_d\u03b8=None)\n```\n\nThe maxmimum likelihood estimate is then obtained by running `imnn.get_MLE(d)` on data `d`\n\n\n```python\nprint(\"The maximum likelihood estimate of the real data is \" + str(imnn.get_MLE(real_data)[0].numpy()))\n```\n\n    The maximum likelihood estimate of the real data is [1.8213251 4.3077555]\n\n\n## Approximate Bayesian computation\n\nWe can now do ABC (or PMC-ABC) with our calculated summary. From the samples we create simulations at each parameter value and feed each simulation through the network to get summaries. The summaries are compared to the summary of the real data to find the distances which can be used to accept or reject points.\n\nWe start by defining our prior as a truncated Gaussian (uniform is also available). The uniform function is taken from delfi by Justin Alsing. At some point in the near future (for a given value of \"near\") this whole module will upgraded to TensorFlow Probability and complete TensorFlow 2 implementation.\n\nWe are going to choose the mean value of the variance to be 1 with a variance of the variance of 10 cut at 0 and 10.\n\n\n```python\nprior = priors.TruncatedGaussian(np.array([0., 1.]), np.array([[10., 0.], [0., 10.]]), np.array([-10., 0.]), np.array([10., 10.]))\n```\n\nThe ABC module takes in the _observed_ data, the prior and the function for obtaining the MLE from the IMNN as well as the Fisher information matrix used for obtaining the MLE. It also takes in the simulator and its arguments.\n\n\n```python\nabc = ABC(real_data=real_data, prior=prior, F=imnn.MLE_F, get_MLE=imnn.get_MLE, simulator=simulator, seed=None, simulator_args={\"input shape\": input_shape})\n```\n\n## Gaussian approximation\nBefore running all the simulations need for approximate Bayesian computation, we can get the Gaussian approximation of the posterior from the MLE and the inverse Fisher information.\n\n\n```python\nprint(\"maximum likelihood estimate\", abc.MLE[0])\nprint(\"determinant of the Fisher information\", np.linalg.det(abc.F))\nplt.imshow(np.linalg.inv(abc.Finv))\nplt.title(\"Inverse Fisher matrix\")\nplt.xticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.yticks([0, 1], [r\"$\\mu$\", r\"$\\Sigma$\"])\nplt.colorbar();\n```\n\n    maximum likelihood estimate [1.8213251 4.3077555]\n    determinant of the Fisher information 67.50628\n\n\n\n![png](figures/output_68_1.png)\n\n\n\n```python\ngaussian_approximation, grid = abc.gaussian_approximation(gridsize = 100)\n```\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior * (parameter_grid[0, 1, 0] - parameter_grid[0, 0, 0]), axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].plot(grid[0, :, 0], np.sum(gaussian_approximation * (grid[0, 1, 0] - grid[0, 0, 0]), axis = 0), color = \"C1\", label = \"Gaussian approximation\")\nax[0, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 0].contour(grid[0, :, 0], grid[1, 0, :], gaussian_approximation, colors = \"C1\")\nax[1, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[1, 0].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].plot(np.sum(analytic_posterior * (parameter_grid[1, 0, 1] - parameter_grid[1, 0, 0]), axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].plot(np.sum(gaussian_approximation * (grid[1, 0, 1] - grid[1, 0, 0]), axis = 1), grid[1, 0, :], color = \"C1\", label = \"Gaussian approximation\")\nax[1, 1].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_70_0.png)\n\n\nWe can see that the maximum likelihood estimate for the mean is almost perfect whilst it is incorrect for the variance. However, we can now see the ABC does in its place.\n\n### ABC\nThe most simple ABC takes the number of draws and a switch to state whether to run all the simulations in parallel or sequentially. The full simulations can also be saved by passing a file name. The draws are stored in the class attribute `ABC_dict`.\n\n\n```python\nabc.ABC(draws=100000, at_once=True, save_sims=None)\n```\n\nIn ABC, draws are accepted if the distance between the simulation summary and the simulation of the real data are \"close\", i.e. smaller than some \u03f5 value, which is chosen somewhat arbitrarily.\n\n\n```python\n\u03f5 = .5\naccept_indices = np.argwhere(abc.ABC_dict[\"distances\"] < \u03f5)[:, 0]\nreject_indices = np.argwhere(abc.ABC_dict[\"distances\"] >= \u03f5)[:, 0]\nprint(\"Number of accepted samples = \", accept_indices.shape[0])\n```\n\n    Number of accepted samples =  2319\n\n\n### Plot samples\nWe can plot the output samples and the histogram of the accepted samples, which should peak around `\u03b8 = 1` (where we generated the real data). The monotonic function of all the output samples shows that the network has learned how to summarise the data.\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (14, 10))\nplt.subplots_adjust(hspace = 0, wspace = 0.2)\nax[0, 0].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 0], abc.ABC_dict[\"MLE\"][reject_indices, 0], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[0, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0] , abc.ABC_dict[\"MLE\"][accept_indices, 0], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[0, 0].axhline(abc.MLE[0, 0], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\mu}_{obs}$\")\nax[0, 0].legend(frameon=False)\nax[0, 0].set_ylabel('$\\hat{\\mu}$', labelpad = 0)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_xticks([])\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 0], abc.ABC_dict[\"MLE\"][reject_indices, 1], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0] , abc.ABC_dict[\"MLE\"][accept_indices, 1], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[1, 0].axhline(abc.MLE[0, 1], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\Sigma}_{obs}$\")\nax[1, 0].legend(frameon=False)\nax[1, 0].set_ylabel('$\\hat{\\Sigma}$', labelpad = 0)\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].set_xlabel(\"$\\mu$\")\nax[0, 1].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 1], abc.ABC_dict[\"MLE\"][reject_indices, 0], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[0, 1].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 1] , abc.ABC_dict[\"MLE\"][accept_indices, 0], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[0, 1].axhline(abc.MLE[0, 0], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\mu}_{obs}$\")\nax[0, 1].legend(frameon=False)\nax[0, 1].set_ylabel('$\\hat{\\mu}$', labelpad = 0)\nax[0, 1].set_xlim([0, 10])\nax[0, 1].set_xticks([])\nax[1, 1].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 1], abc.ABC_dict[\"MLE\"][reject_indices, 1], s = 1, alpha = 0.1, label = \"Rejected samples\", color = \"C3\")\nax[1, 1].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 1] , abc.ABC_dict[\"MLE\"][accept_indices, 1], s = 1, label = \"Accepted samples\", color = \"C6\", alpha = 0.5)\nax[1, 1].axhline(abc.MLE[0, 1], color = 'black', linestyle = 'dashed', label = \"$\\hat{\\Sigma}_{obs}$\")\nax[1, 1].legend(frameon=False)\nax[1, 1].set_ylabel('$\\hat{\\Sigma}$', labelpad = 0)\nax[1, 1].set_xlim([0, 10])\nax[1, 1].set_xlabel(\"$\\Sigma$\");\n```\n\n\n![png](figures/output_77_0.png)\n\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize = (16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior * (parameter_grid[0, 1, 0] - parameter_grid[0, 0, 0]), axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].plot(grid[0, :, 0], np.sum(gaussian_approximation * (grid[0, 1, 0] - grid[0, 0, 0]), axis = 0), color = \"C1\", label = \"Gaussian approximation\")\nax[0, 0].hist(abc.ABC_dict[\"parameters\"][accept_indices, 0], np.linspace(-10, 10, 100), histtype = u'step', density = True, linewidth = 1.5, color = \"C6\", label = \"ABC posterior\");\nax[0, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0], abc.ABC_dict[\"parameters\"][accept_indices, 1], color = \"C6\", s = 1, alpha = 0.5)\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][reject_indices, 0], abc.ABC_dict[\"parameters\"][reject_indices, 1], color = \"C3\", s = 1, alpha = 0.01)\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 0].contour(grid[0, :, 0], grid[1, 0, :], gaussian_approximation, colors = \"C1\")\nax[1, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[1, 0].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].hist(abc.ABC_dict[\"parameters\"][accept_indices, 1], np.linspace(0, 10, 100), histtype = u'step', orientation=\"horizontal\", density = True, linewidth = 1.5, color = \"C6\", label = \"ABC posterior\");\nax[1, 1].plot(np.sum(analytic_posterior * (parameter_grid[1, 0, 1] - parameter_grid[1, 0, 0]), axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].plot(np.sum(gaussian_approximation * (grid[1, 0, 1] - grid[1, 0, 0]), axis = 1), grid[1, 0, :], color = \"C1\", label = \"Gaussian approximation\")\nax[1, 1].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_78_0.png)\n\n\nWe now get samples from the posterior disrtibution which is not too far from the analytic posterior, and is at least unbiased. However, many samples are rejected to achieve this, and the rejection is defined somewhat arbitrarily, making it very computationally heavy and uncertain. We can improve on this using a PMC.\n\n## PMC-ABC\nPopulation Monte Carlo ABC is a way of reducing the number of draws by first sampling from a prior, accepting the closest 75% of the samples and weighting all the rest of the samples to create a new proposal distribution. The furthest 25% of the original samples are redrawn from the new proposal distribution. The furthest 25% of the simulation summaries are continually rejected and the proposal distribution updated until the number of draws needed accept all the 25% of the samples is much greater than this number of samples. This ratio is called the criterion.\n\nIf we want 1000 samples from the approximate distribution at the end of the PMC we need to set `posterior = 1000`. The initial random draw (as in ABC above) initialises with `draws`, the larger this is the better proposal distribution will be on the first iteration.\n\nThe `PMC` can be continued by running again with a smaller criterion.\n\n\n```python\nabc.PMC(draws=2000, posterior=2000, criterion=0.01, at_once=True, save_sims=None)\n```\n\n    iteration = 28, current criterion = 0.009747776288534179, total draws = 920480, \u03f5 = 0.08228597976267338.\n\nTo restart the PMC from scratch then one can run\n```python\nabc.PMC(draws=1000, posterior=1000, criterion=0.01, at_once=True, save_sims=None, restart=True)\n```\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize=(14, 10))\nplt.subplots_adjust(hspace=0, wspace=0.2)\nax[0, 0].scatter(abc.PMC_dict[\"parameters\"][:, 0] , abc.PMC_dict[\"MLE\"][:, 0], s=1, label=\"Accepted samples\", color=\"C4\", alpha=0.5)\nax[0, 0].axhline(abc.MLE[0, 0], color='black', linestyle='dashed', label=\"$\\hat{\\mu}_{obs}$\")\nax[0, 0].legend(frameon=False)\nax[0, 0].set_ylabel('$\\hat{\\mu}$', labelpad=0)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_xticks([])\nax[1, 0].scatter(abc.PMC_dict[\"parameters\"][:, 0], abc.PMC_dict[\"MLE\"][:, 1], s=1, alpha=0.5, label=\"Accepted samples\", color=\"C4\")\nax[1, 0].axhline(abc.MLE[0, 1], color='black', linestyle='dashed', label=\"$\\hat{\\Sigma}_{obs}$\")\nax[1, 0].legend(frameon=False)\nax[1, 0].set_ylabel('$\\hat{\\Sigma}$', labelpad=0)\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].set_xlabel(\"$\\mu$\")\nax[0, 1].scatter(abc.PMC_dict[\"parameters\"][:, 1], abc.PMC_dict[\"MLE\"][:, 0], s=1, alpha=0.5, label=\"Accepted samples\", color=\"C4\")\nax[0, 1].axhline(abc.MLE[0, 0], color='black', linestyle='dashed', label=\"$\\hat{\\mu}_{obs}$\")\nax[0, 1].legend(frameon=False)\nax[0, 1].set_ylabel('$\\hat{\\mu}$', labelpad=0)\nax[0, 1].set_xlim([0, 10])\nax[0, 1].set_xticks([])\nax[1, 1].scatter(abc.PMC_dict[\"parameters\"][:, 1], abc.PMC_dict[\"MLE\"][:, 1], s=1, alpha=0.5, label=\"Accepted samples\", color=\"C4\")\nax[1, 1].axhline(abc.MLE[0, 1], color='black', linestyle='dashed', label=\"\\hat{\\Sigma}_{obs}\")\nax[1, 1].legend(frameon=False)\nax[1, 1].set_ylabel('$\\hat{\\Sigma}$', labelpad=0)\nax[1, 1].set_xlim([0, 10])\nax[1, 1].set_xlabel(\"$\\Sigma$\");\n```\n\n\n![png](figures/output_83_0.png)\n\n\n\n```python\nfig, ax = plt.subplots(2, 2, figsize=(16, 10))\nplt.subplots_adjust(wspace = 0, hspace = 0)\nax[0, 0].plot(parameter_grid[0, :, 0], np.sum(analytic_posterior * (parameter_grid[0, 1, 0] - parameter_grid[0, 0, 0]), axis = 0), linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[0, 0].plot(grid[0, :, 0], np.sum(gaussian_approximation * (grid[0, 1, 0] - grid[0, 0, 0]), axis = 0), color = \"C1\", label = \"Gaussian approximation\")\nax[0, 0].hist(abc.ABC_dict[\"parameters\"][accept_indices, 0], np.linspace(-10, 10, 100), histtype = u'step', density = True, linewidth = 1.5, color = \"C6\", alpha = 0.3, label = \"ABC posterior\");\nax[0, 0].hist(abc.PMC_dict[\"parameters\"][:, 0], np.linspace(-10, 10, 100), histtype = u'step', density = True, linewidth = 1.5, color = \"C4\", label = \"PMC posterior\");\nax[0, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[0, 0].legend(frameon = False)\nax[0, 0].set_xlim([-10, 10])\nax[0, 0].set_ylabel('$\\\\mathcal{P}(\\\\mu|{\\\\bf d})$')\nax[0, 0].set_yticks([])\nax[0, 0].set_xticks([])\nax[1, 0].set_xlabel('$\\mu$');\nax[1, 0].set_ylim([0, 10])\nax[1, 0].set_ylabel('$\\Sigma$')\nax[1, 0].set_xlim([-10, 10])\nax[1, 0].scatter(abc.ABC_dict[\"parameters\"][accept_indices, 0], abc.ABC_dict[\"parameters\"][accept_indices, 1], color = \"C6\", s = 1, alpha = 0.2)\nax[1, 0].scatter(abc.PMC_dict[\"parameters\"][:, 0], abc.PMC_dict[\"parameters\"][:, 1], color = \"C4\", s = 1, alpha = 0.7)\nax[1, 0].contour(parameter_grid[0, :, 0], parameter_grid[1, 0, :], analytic_posterior, colors = \"C2\")\nax[1, 0].contour(grid[0, :, 0], grid[1, 0, :], gaussian_approximation, colors = \"C1\")\nax[1, 0].axvline(abc.MLE[0, 0], linestyle = \"dashed\", color = \"black\", label = \"Maximum likelihood estimate of mean\")\nax[1, 0].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].hist(abc.ABC_dict[\"parameters\"][accept_indices, 1], np.linspace(0, 10, 100), histtype = u'step', orientation=\"horizontal\", density = True, linewidth = 1.5, color = \"C6\", alpha = 0.3, label = \"ABC posterior\");\nax[1, 1].plot(np.sum(analytic_posterior * (parameter_grid[1, 0, 1] - parameter_grid[1, 0, 0]), axis = 1), parameter_grid[1, 0, :], linewidth = 1.5, color = 'C2', label = \"Analytic marginalised posterior\")\nax[1, 1].plot(np.sum(gaussian_approximation * (grid[1, 0, 1] - grid[1, 0, 0]), axis = 1), grid[1, 0, :], color = \"C1\", label = \"Gaussian approximation\")\nax[1, 1].hist(abc.PMC_dict[\"parameters\"][:, 1], np.linspace(0, 10, 100), histtype = u'step', orientation=\"horizontal\", density = True, linewidth = 1.5, color = \"C4\", label = \"PMC posterior\");\nax[1, 1].axhline(abc.MLE[0, 1], linestyle = \"dotted\", color = \"black\", label = \"Maximum likelihood estimate of covariance\")\nax[1, 1].legend(frameon = False)\nax[1, 1].set_ylim([0, 10])\nax[1, 1].set_xlabel('$\\\\mathcal{P}(\\\\Sigma|{\\\\bf d})$')\nax[1, 1].set_xticks([])\nax[1, 1].set_yticks([])\nax[0, 1].axis(\"off\");\n```\n\n\n![png](figures/output_84_0.png)\n\n\nWe can see that the IMNN can recover great posteriors even when the data is extremely far from the fiducial parameter value at which the network was trained! Woohoo - give yourself a pat on the back!\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/tomcharnock/information_maximiser.git", "keywords": "", "license": "", "maintainer": "", "maintainer_email": "", "name": "IMNN", "package_url": "https://pypi.org/project/IMNN/", "platform": "", "project_url": "https://pypi.org/project/IMNN/", "project_urls": {"Homepage": "https://github.com/tomcharnock/information_maximiser.git"}, "release_url": "https://pypi.org/project/IMNN/0.2a2/", "requires_dist": ["tensorflow (>=2.0.0)", "tqdm (>=4.31.0)", "numpy (>=1.16.0)"], "requires_python": ">=3", "summary": "Using neural networks to extract sufficient statistics from         data by maximising the Fisher information", "version": "0.2a2"}, "last_serial": 6208296, "releases": {"0.1.dev0": [{"comment_text": "", "digests": {"md5": "3e1977e6bac0f3329c48ca0f15ccf10b", "sha256": "2503e8aeb9bf4413fb4c8fbe7ce87c6ab86544afa1685ffcec3ae2128d63add0"}, "downloads": -1, "filename": "IMNN-0.1.dev0-py3-none-any.whl", "has_sig": false, "md5_digest": "3e1977e6bac0f3329c48ca0f15ccf10b", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35250, "upload_time": "2019-01-23T17:36:04", "upload_time_iso_8601": "2019-01-23T17:36:04.982909Z", "url": "https://files.pythonhosted.org/packages/d6/4d/92e3b83a60d39127f8af8f5597448057de8060f39822d78a352001a306e8/IMNN-0.1.dev0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "be51168f81db7066ac4b0356b000ba74", "sha256": "e718b51b8f67365ea0069240091040807259d42fca158a3f073572d22461c573"}, "downloads": -1, "filename": "IMNN-0.1.dev0.tar.gz", "has_sig": false, "md5_digest": "be51168f81db7066ac4b0356b000ba74", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 49339, "upload_time": "2019-01-23T17:36:06", "upload_time_iso_8601": "2019-01-23T17:36:06.898708Z", "url": "https://files.pythonhosted.org/packages/0d/ad/ea5a670be10cd97bf81eb4033bdd8ca963c4656f1f06322d4a4094dd119a/IMNN-0.1.dev0.tar.gz"}], "0.1.dev2": [{"comment_text": "", "digests": {"md5": "2fe58c400d0a3de14f7f16ed0f04f6ee", "sha256": "4eeb4ed8d347b9eedf72356be589101d9e857eaa4a57030fba528e0ba022a76a"}, "downloads": -1, "filename": "IMNN-0.1.dev2-py3-none-any.whl", "has_sig": false, "md5_digest": "2fe58c400d0a3de14f7f16ed0f04f6ee", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 34996, "upload_time": "2019-01-28T11:07:11", "upload_time_iso_8601": "2019-01-28T11:07:11.004882Z", "url": "https://files.pythonhosted.org/packages/d4/27/d7e51b66acf3c7b8b3a2f8077f7e012d83cd9809281f4220cd794bb66131/IMNN-0.1.dev2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "2c0008b8d90d369680cb1094e126fdfd", "sha256": "f962aa006ba1776a317ddae0b5fe836e5840590b5cfbce4ef89742e2b33988c0"}, "downloads": -1, "filename": "IMNN-0.1.dev2.tar.gz", "has_sig": false, "md5_digest": "2c0008b8d90d369680cb1094e126fdfd", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48651, "upload_time": "2019-01-28T11:07:13", "upload_time_iso_8601": "2019-01-28T11:07:13.194089Z", "url": "https://files.pythonhosted.org/packages/bf/13/bf5345e3b85e425735949638c8e75c2b13c0c746cb04543008aaebf21533/IMNN-0.1.dev2.tar.gz"}], "0.1.dev3": [{"comment_text": "", "digests": {"md5": "a51c6ead7513f57e0722cdf02e268e01", "sha256": "5a1dc1d06db2e7809904a3926b457cf9d7669a4e3bea40d551d4d1dadfc7d032"}, "downloads": -1, "filename": "IMNN-0.1.dev3-py3-none-any.whl", "has_sig": false, "md5_digest": "a51c6ead7513f57e0722cdf02e268e01", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35003, "upload_time": "2019-01-28T11:58:49", "upload_time_iso_8601": "2019-01-28T11:58:49.574781Z", "url": "https://files.pythonhosted.org/packages/2f/97/ad16b748782bb56040583ed3c4e27a12c8241c4e8c2abbecaad0c5557c38/IMNN-0.1.dev3-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "d9aae80c21d9b6902d2d5e95060ae409", "sha256": "4499e43651c2585e76a9fc5eed5e1ee55b3c606eed3c59e7fb35a2d9f76795d9"}, "downloads": -1, "filename": "IMNN-0.1.dev3.tar.gz", "has_sig": false, "md5_digest": "d9aae80c21d9b6902d2d5e95060ae409", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48653, "upload_time": "2019-01-28T11:58:51", "upload_time_iso_8601": "2019-01-28T11:58:51.261890Z", "url": "https://files.pythonhosted.org/packages/2e/fe/6f6e666d7a444d755250bf45c035bc6c2e2959a36feb6218ea58c013e22e/IMNN-0.1.dev3.tar.gz"}], "0.1.dev4": [{"comment_text": "", "digests": {"md5": "76ecccc76a96f13e18e47b83729c4b7a", "sha256": "ff814a7096f97c3a5aa71d87492cc88c610a5707830e44a941954eba1e4f4b0d"}, "downloads": -1, "filename": "IMNN-0.1.dev4-py3-none-any.whl", "has_sig": false, "md5_digest": "76ecccc76a96f13e18e47b83729c4b7a", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35002, "upload_time": "2019-01-28T12:13:11", "upload_time_iso_8601": "2019-01-28T12:13:11.866826Z", "url": "https://files.pythonhosted.org/packages/ab/36/c67ad160b987c50110e0960d48b688bad0445e76d90a8849ceb5c5d6bdcb/IMNN-0.1.dev4-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "39f419fc52499f8ebe0e03ba1208f1e2", "sha256": "4b87c285b6a7bc54283d20fe8d3cfdcd8ab8c39274163c08cf8ddc1b546304fc"}, "downloads": -1, "filename": "IMNN-0.1.dev4.tar.gz", "has_sig": false, "md5_digest": "39f419fc52499f8ebe0e03ba1208f1e2", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48653, "upload_time": "2019-01-28T12:13:13", "upload_time_iso_8601": "2019-01-28T12:13:13.456697Z", "url": "https://files.pythonhosted.org/packages/16/41/50978445be50fae20ccaacccbb77e3188cf6cd9f39853b45f38e098ebdb8/IMNN-0.1.dev4.tar.gz"}], "0.1.dev5": [{"comment_text": "", "digests": {"md5": "18296fe144d015772ca5576c1a9a3e3c", "sha256": "9957217601cd6d7c2818cd12d6735c957e9a892f45b8b10830baf27c129485ba"}, "downloads": -1, "filename": "IMNN-0.1.dev5-py3-none-any.whl", "has_sig": false, "md5_digest": "18296fe144d015772ca5576c1a9a3e3c", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 35001, "upload_time": "2019-01-28T12:17:08", "upload_time_iso_8601": "2019-01-28T12:17:08.815098Z", "url": "https://files.pythonhosted.org/packages/ef/96/6bba42cac447e1623a64d3a79d6f78c298e0784a18b9a7e3e1e2693bae9a/IMNN-0.1.dev5-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "3022e29661f800bf3312ef81ffce0cb1", "sha256": "6132855954e3d0ee4599c7c41076cb02f8bc061c41226e3630f0467a1f4374d3"}, "downloads": -1, "filename": "IMNN-0.1.dev5.tar.gz", "has_sig": false, "md5_digest": "3022e29661f800bf3312ef81ffce0cb1", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48648, "upload_time": "2019-01-28T12:17:10", "upload_time_iso_8601": "2019-01-28T12:17:10.398221Z", "url": "https://files.pythonhosted.org/packages/07/38/92cc32817d3452f2c9a3faccb85107509e02bd3e25a197aaf09f83431998/IMNN-0.1.dev5.tar.gz"}], "0.1.dev6": [{"comment_text": "", "digests": {"md5": "5e918e9e534ae7b7be2d08a536ccc9b8", "sha256": "f3b048958ac811e33de46d12317b0f2099da72670e5b50b6bc0fd5b883134444"}, "downloads": -1, "filename": "IMNN-0.1.dev6-py3-none-any.whl", "has_sig": false, "md5_digest": "5e918e9e534ae7b7be2d08a536ccc9b8", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 34946, "upload_time": "2019-01-28T18:25:49", "upload_time_iso_8601": "2019-01-28T18:25:49.491243Z", "url": "https://files.pythonhosted.org/packages/9f/65/01c081cb92464740725b698bf884210bb29936312f87c2a9bb19a4cbc07e/IMNN-0.1.dev6-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "d8a17a51a224a45f05ad4394eb65131c", "sha256": "76c8e412a9bfa0a79d4fc55690fa98aa7762759a1e89d72d4671d51ede8ece40"}, "downloads": -1, "filename": "IMNN-0.1.dev6.tar.gz", "has_sig": false, "md5_digest": "d8a17a51a224a45f05ad4394eb65131c", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 48604, "upload_time": "2019-01-28T18:25:53", "upload_time_iso_8601": "2019-01-28T18:25:53.261825Z", "url": "https://files.pythonhosted.org/packages/45/d1/97929f102dedc740ae0f985ee5cd40b4b8dc4cbe2d08040468429d01901c/IMNN-0.1.dev6.tar.gz"}], "0.1.dev8": [{"comment_text": "", "digests": {"md5": "b7ce6e5aa451b9219500449934154656", "sha256": "4da6851163337e0a355e399ed7482f571c3ee6b864e95113e959b4e785cad79f"}, "downloads": -1, "filename": "IMNN-0.1.dev8-py3-none-any.whl", "has_sig": false, "md5_digest": "b7ce6e5aa451b9219500449934154656", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 38707, "upload_time": "2019-03-07T15:25:54", "upload_time_iso_8601": "2019-03-07T15:25:54.283004Z", "url": "https://files.pythonhosted.org/packages/66/7a/9974dd11345297ccf8848bcb0fe23b87dba1c779da41cfa44fe785663e6c/IMNN-0.1.dev8-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "e2b45deca9f2265d0ed5d54f9923fece", "sha256": "ee7add025a03cdaf48308c3f5d341771d4c751729970a283034998a073b887a5"}, "downloads": -1, "filename": "IMNN-0.1.dev8.tar.gz", "has_sig": false, "md5_digest": "e2b45deca9f2265d0ed5d54f9923fece", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54410, "upload_time": "2019-03-07T15:25:56", "upload_time_iso_8601": "2019-03-07T15:25:56.074474Z", "url": "https://files.pythonhosted.org/packages/63/c4/6a1254c4766211f4225b7999102c835fb79813594f6e1629dee963fabfc8/IMNN-0.1.dev8.tar.gz"}], "0.1rc1": [{"comment_text": "", "digests": {"md5": "6fbf225f0f3fb80b5004503f9242a754", "sha256": "4c2271a2d7d7d4c563bc15a98bafd3221486ceaf50ea28b25a1093498d60315d"}, "downloads": -1, "filename": "IMNN-0.1rc1-py3-none-any.whl", "has_sig": false, "md5_digest": "6fbf225f0f3fb80b5004503f9242a754", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.6", "size": 37879, "upload_time": "2019-02-01T18:11:40", "upload_time_iso_8601": "2019-02-01T18:11:40.230801Z", "url": "https://files.pythonhosted.org/packages/6e/45/fcef3827bbec6acf00af9c2e1dd77223ba03c0f6d7e673a83e7ec3162a07/IMNN-0.1rc1-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9bc2f2f3d09085b862335c9d6d388cca", "sha256": "8d810c18e7f4f5104f0968ace045712253b3b1abfdfe35c3858d9b67667bde63"}, "downloads": -1, "filename": "IMNN-0.1rc1.tar.gz", "has_sig": false, "md5_digest": "9bc2f2f3d09085b862335c9d6d388cca", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 54298, "upload_time": "2019-02-01T18:11:43", "upload_time_iso_8601": "2019-02-01T18:11:43.168104Z", "url": "https://files.pythonhosted.org/packages/e7/71/31bb47c26cbf7bef9d91a8a0c2a03f687ece2f422334070beeaff0eb21de/IMNN-0.1rc1.tar.gz"}], "0.2a1": [{"comment_text": "", "digests": {"md5": "940a6fd908c2bd0dfcd3112aab0dbcfd", "sha256": "7593bbc6816b707074b2276af5e7e082465e70af622dc694428a5bb29e3f8c7a"}, "downloads": -1, "filename": "IMNN-0.2a1-py3-none-any.whl", "has_sig": false, "md5_digest": "940a6fd908c2bd0dfcd3112aab0dbcfd", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 32276, "upload_time": "2019-11-26T15:19:31", "upload_time_iso_8601": "2019-11-26T15:19:31.286790Z", "url": "https://files.pythonhosted.org/packages/d7/57/3d270f9d2e7141552bc5d9413b0cce0a87c192fd4373604405fa4fcc7c83/IMNN-0.2a1-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "526daba1ebd6f63f60bb964dd1a52ba4", "sha256": "55f1d635693df3690371c21ef563c5fa4f8819a9bd1ac26086c25facffa810d5"}, "downloads": -1, "filename": "IMNN-0.2a1.tar.gz", "has_sig": false, "md5_digest": "526daba1ebd6f63f60bb964dd1a52ba4", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 48047, "upload_time": "2019-11-26T15:19:36", "upload_time_iso_8601": "2019-11-26T15:19:36.032897Z", "url": "https://files.pythonhosted.org/packages/83/5c/6391c3042068e62730140616d1f736a7a74da67de31492a7ddb166139514/IMNN-0.2a1.tar.gz"}], "0.2a2": [{"comment_text": "", "digests": {"md5": "766bfcc197ac7e41e0ca8388e8ce56bf", "sha256": "e1a4a5faf42622af9d04391b596ec210a28c1ca6c1f58e797e625c8743c973f4"}, "downloads": -1, "filename": "IMNN-0.2a2-py3-none-any.whl", "has_sig": false, "md5_digest": "766bfcc197ac7e41e0ca8388e8ce56bf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 32261, "upload_time": "2019-11-27T14:28:37", "upload_time_iso_8601": "2019-11-27T14:28:37.046727Z", "url": "https://files.pythonhosted.org/packages/f6/2f/47385598b4b6911b554a11259504950a8cadeac92bc890deb438a8da43be/IMNN-0.2a2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "c535b6e474bc8902dd62637227046b3f", "sha256": "0ddbe7b4de6934163aaeed5d97fb319c815085e83247a945afdfc49b11f63e4b"}, "downloads": -1, "filename": "IMNN-0.2a2.tar.gz", "has_sig": false, "md5_digest": "c535b6e474bc8902dd62637227046b3f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 48033, "upload_time": "2019-11-27T14:28:39", "upload_time_iso_8601": "2019-11-27T14:28:39.938806Z", "url": "https://files.pythonhosted.org/packages/62/2c/3e3350a7478e36546219aada011929adf66e793c61f8416ae680cf8b608b/IMNN-0.2a2.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "766bfcc197ac7e41e0ca8388e8ce56bf", "sha256": "e1a4a5faf42622af9d04391b596ec210a28c1ca6c1f58e797e625c8743c973f4"}, "downloads": -1, "filename": "IMNN-0.2a2-py3-none-any.whl", "has_sig": false, "md5_digest": "766bfcc197ac7e41e0ca8388e8ce56bf", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3", "size": 32261, "upload_time": "2019-11-27T14:28:37", "upload_time_iso_8601": "2019-11-27T14:28:37.046727Z", "url": "https://files.pythonhosted.org/packages/f6/2f/47385598b4b6911b554a11259504950a8cadeac92bc890deb438a8da43be/IMNN-0.2a2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "c535b6e474bc8902dd62637227046b3f", "sha256": "0ddbe7b4de6934163aaeed5d97fb319c815085e83247a945afdfc49b11f63e4b"}, "downloads": -1, "filename": "IMNN-0.2a2.tar.gz", "has_sig": false, "md5_digest": "c535b6e474bc8902dd62637227046b3f", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3", "size": 48033, "upload_time": "2019-11-27T14:28:39", "upload_time_iso_8601": "2019-11-27T14:28:39.938806Z", "url": "https://files.pythonhosted.org/packages/62/2c/3e3350a7478e36546219aada011929adf66e793c61f8416ae680cf8b608b/IMNN-0.2a2.tar.gz"}]}