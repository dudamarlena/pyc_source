{"info": {"author": "The Happy Transformer Development Team", "author_email": "happytransformer@gmail.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: Apache Software License", "Programming Language :: Python :: 3.6", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence", "Topic :: Text Processing :: Linguistic"], "description": "[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0) [![Downloads](https://pepy.tech/badge/happytransformer)](https://pepy.tech/project/happytransformer)\n\n# Happy Transformer \n\n![HappyTransformer](img/HappyTransformer.png)\n\nHappy Transformer is an API built on top of [PyTorch's transformer library](https://pytorch.org/hub/huggingface_pytorch-transformers/) that makes it easy to utilize state-of-the-art NLP models. \n\n## Key Features\n  - **New: Finetuning Masked Language Models**\n  - Available language models: XLNET, BERT and ROBERTA.\n  - Predict a masked word within a sentence.\n  - Fine tune binary sequence classification models to solve problems like sentiment analysis.\n  - Predict the likelihood that sentence B follows sentence A within a paragraph. \n\n\n| Public Methods                     | HappyROBERTA | HappyXLNET | HappyBERT |\n|------------------------------------|--------------|------------|-----------|\n| Masked Word Prediction             | \u2714            | \u2714          | \u2714         |\n| Sequence Classification            | \u2714            | \u2714          | \u2714         |\n| Next Sentence Prediction           |              |            | \u2714         |\n| Question Answering                 |              |            | \u2714         |\n| Masked Word Prediction Finetuning  | \u2714            |            | \u2714         |\n\n## Installation\n\n```sh\npip install happytransformer\n```\n## Initialization \n\nBy default base models are used. They are smaller, faster and require significantly less training time\nto obtain decent results.\n\nLarge models are recommended for tasks that do not require fine tuning such as some word prediction tasks. \n\nBase models are recommended for tasks that require fine tuning with limited available training data. \n\nUncased models do not differentiate between cased and uncased words. For example, the words\n\"empire\" and \"Empire\" would be reduced to the same token. In comparison, cased models do differentiate between cased and uncased words. \n\n#### HappyXLNET:\n\n```sh\nfrom happytransformer import HappyXLNET\n#--------------------------------------#\nxl_base_uncased = HappyXLNET(\"xlnet-base-uncased\")\nxl_base_cased = HappyXLNET(\"xlnet-base-cased\")\nxl_large_uncased = HappyXLNET(\"xlnet-large-uncased\")\nxl_large_cased = HappyXLNET(\"xlnet-large-cased\")\n```\n#### HappyROBERTA:\n```sh\nfrom happytransformer import HappyROBERTA\n#--------------------------------------#\nhappy_roberta_base = HappyROBERTA(\"roberta-base\")\nhappy_roberta_large = HappyROBERTA(\"roberta-large\")\n\n```\n#### HappyBERT :\n```sh\nfrom happytransformer import HappyBERT\n#--------------------------------------#\nbert_base_uncased = HappyBERT(\"bert-base-uncased\")\nbert_base_cased = HappyBERT(\"bert-base-cased\")\nbert_large_uncased = HappyBERT(\"bert-large-uncased\")\nbert_large_cased = HappyBERT(\"bert-large-cased\")\n```\n## Word Prediction \n\nEach Happy Transformer has a public  method called \"predict_mask(text, options, num_results)\" with the following input arguments.\n1. Text: the text you wish to predict including a single masked token.\n2. options (default = every word): A limited set of words the model can return.\n3. num_results (default = 1): The number of returned predictions.\n\nFor all Happy Transformers, the masked token is **\"[MASK]\"**\n\n\"predict_mask(text, options, num_results)\" returns a list of dictionaries which is exemplified in Example 1 .\n\nIt is recommended that you use HappyROBERTA(\"roberta-large\") for masked word prediction.\nAvoid using HappyBERT for masked word prediction. \nIf you do decide to use HappyXLNET or HappyBERT, then also use their corresponding \"large cased model'. \n\n\n#### Example 1 :\n```sh\nfrom happytransformer import HappyROBERTA\n#--------------------------------------#\nhappy_roberta = HappyROBERTA(\"roberta-large\")\ntext = \"I think therefore I [MASK]\"\nresults = happy_roberta.predict_mask(text)\n\nprint(type(results)) # prints: <class 'list'>\nprint(results) # prints: [{'word': 'am', 'softmax': 0.24738965928554535}]\n\nprint(type(results[0])) # prints: <class 'dict'>\nprint(results[0]) # prints: {'word': 'am', 'softmax': 0.24738965928554535}\n\n\n```\n\n#### Example 2 :\n```sh\nfrom happytransformer import HappyROBERTA\n#--------------------------------------#\nhappy_roberta = HappROBERTA(\"roberta-large\")\ntext = \"To solve world poverty we must invest in [MASK]\"\nresults = happy_roberta.predict_mask(text, num_results = 2)\n\nprint(type(results)) # prints: <class 'list'>\nprint(results) # prints: [{'word': 'education', 'softmax': 0.34365904331207275}, {'word': 'children', 'softmax': 0.03996562585234642}]\n\nprint(type(results[0])) # prints: <class 'dict'>\nprint(results[0]) # prints: {'word': 'education', 'softmax': 0.34365904331207275}\n\n\n```\n\n#### Example 3 :\n```sh\nfrom happytransformer import HappyXLNET\n#--------------------------------------#\nhappy_xlnet = HappyXLNET(\"xlnet-large-cased\")\ntext = \"Can you please pass the [MASK] \"\noptions = [\"pizza\", \"rice\", \"tofu\", 'eggs', 'milk']\nresults = happy_xlnet.predict_mask(text, options=options, num_results=3)\n\nprint(type(results)) # prints: <class 'list'>\nprint(results) # prints: [{'word': 'tofu', 'softmax': 0.007073382}, {'word': 'pizza', 'softmax': 0.00017212195}, {'word': 'rice', 'softmax': 2.843065e-07}]\n\n\nprint(type(results[1])) # prints: <class 'dict'>\nprint(results[1]) # prints: {'word': 'pizza', 'softmax': 0.00017212195}\n\n```\n## Binary Sequence Classification \n\nBinary sequence classification (BSC) has many applications. For example, by using BSC, you can train a model to predict if a yelp review is positive or negative. \nAnother example includes determining if an email is spam or ham. \n\nEach Happy Transformer has four methods that are utilized for binary sequence classification:\n\n1. init_sequence_classifier()\n2. custom_init_sequence_classifier(args)\n3. train_sequence_classifier(train_csv_path)\n4. eval_sequence_classifier(eval_csv_path)\n\n\n### init_sequence_classifier()\nInitialize binary sequence classification for the HappyTransformer object with the default settings.\n\n\n### train_sequence_classifier(train_csv_path):\nTrains the HappyTransformer's sequence classifier.\n\nOne of the two init sequence classifier methods must be called before this method can be called.\n\nArgument:\n\n    1. train_csv_path: A string directory path to the csv that contains the training data.\n\n##### train_csv requirements: \n    1. The csv must contain *NO* header. \n    2. Each row contains a training case. \n    3. The first column contains either a 0 or a 1 to indicate whether the training case is for case \"0\" or case \"1\". \n    4. The second column contains the text for the training case\n#### Example 1\n|   |                                                              | \n|---|--------------------------------------------------------------| \n| 0 |  Terrible service and awful food                             | \n| 1 |  My new favourite Chinese restaurant!!!!                     | \n| 1 |  Amazing food and okay service. Overall a great place to eat | \n| 0 |  The restaurant smells horrible.                             | \n\nThis method does not return anything \n\n\n### eval_sequence_classifier(eval_csv_path):\nEvaluates the trained model against an input.\n\ntrain_sequence_classifier(train_csv_path): must be called before this method can be called.\n\nArgument:\n\n    1. eval_csv_path: A string directory path to the csv that contains the evaluating data.\n\n##### eval_csv requirements: (same as train_csv requirements) \n    1. The csv must contain *NO* header. \n    2. Each row contains a training case. \n    3. The first column contains either a 0 or a 1 to indicate whether the training case is for case \"0\" or case \"1\". \n    4. The second column contains the text for the training case\n\n**Returns** a python dictionary that contains a count for the following values\n\n*true_positive:* The model correctly predicted the value 1 .\n*true_negative:* The model correctly predicted the value 0.\n*false_positive':* The model incorrectly predicted the value 1.\n*false_negative* The model incorrectly predicted the value 0.\n\n\n### test_sequence_classifier(test_csv_path):\nTests the trained model against an input.\n\ntrain_sequence_classifier(train_csv_path): must be called before this method can be called.\n\nArgument:\n    1. test_csv_path: A string directory path to the csv that contains the testing data\n\n##### test_csv requirements: \n    1. The csv must contain *NO* header. \n    2. Each row contains a single test case. \n    3. The csv contains a single column with the text for each test case.\n\n#### Example 2:\n\n|                                           | \n|-------------------------------------------| \n| 5 stars!!!                                | \n| Cheap food at an expensive price          | \n| Great location and nice view of the ocean | \n| two thumbs down                           | \n\n**Returns** a list of integer values in ascending order by test case row index.\nFor example, for the csv file shown in Example 2, the result would be [1, 0, 1, 0]. \nWhere the first index in the list  corresponds to \"5 stars!!!\" \nand the last index corresponds to \"two thumbs down.\"\n\n\n#### Example 3:\n```sh\nfrom happytransformer import HappyROBERTA\n#------------------------------------------#\nhappy_roberta = HappyROBERTA()\nhappy_roberta.init_sequence_classifier()\n\ntrain_csv_path = \"data/train.csv\"\nhappy_roberta.train_sequence_classifier(train_csv_path)\n\neval_csv_path = \"data/eval.csv\"\neval_results = happy_roberta.eval_sequence_classifier(eval_csv_path)\nprint(type(eval_results)) # prints: <class 'dict'>\nprint(eval_results) # prints: {'true_positive': 300', 'true_negative': 250, 'false_positive': 40, 'false_negative': 55}\n\ntest_csv_path = \"data/test.csv\"\ntest_results = happy_roberta.test_sequence_classifier(test_csv_path)\nprint(type(test_results)) # prints: <class 'list'>\nprint(test_results) # prints: [1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0 ]\n```\n\n\n### custom_init_sequence_classifier(args)\n\nInitializing the sequence classifier with custom settings. \nCalled instead of init_sequence_classifier(). \nargument:\n    1. args: a python dictionary that contains all of the same fields as the default arguments\n\n### default classifier arguments\n\n```\n# found under \"from happytransformer.classifier_args\"\nclassifier_args = {\n    # Basic fine tuning parameters\n    'learning_rate': 1e-5,\n    'num_epochs': 2,\n    'batch_size': 8,\n\n    # More advanced fine tuning parameters\n    'max_seq_length': 128,  #  Max number of tokens per input. Max value = 512\n    'adam_epsilon': 1e-5,\n    'gradient_accumulation_steps': 1,\n    'weight_decay': 0,\n    'warmup_ratio': 0.06,\n    'warmup_steps': 0,\n    'max_grad_norm': 1.0,\n\n    # More modes will become available in future releases\n    'task_mode': 'binary',\n    }\n ```\n#### Example 4:\n ```sh\nfrom happytransformer import HappyROBERTA\nfrom happytransformer import classifier_args\n#------------------------------------------#\nhappy_xlnet = HappyXLNET()\n\ncustom_args = classifier_args.copy()\ncustom_args[\"learning_rate\"] = 2e-5\ncustom_args['num_epochs'] = 4\ncustom_args[\"batch_size\"] = 3\n\nhappy_xlnet.custom_init_sequence_classifier(custom_args)\n# Continue from example 1 after \"happy_roberta.init_sequence_classifier()\"\"\n\n```\n\n\n## Next Sentence Prediction\n\n*Determine the likelihood that sentence B follows sentence A.*\n\n\n**HappyBERT** has a method called \"predict_next_sentence\" which is used for next sentence prediction tasks.\nThe method takes the following arguments:\n\n    1. sentence_a: A **single** sentence in a body of text\n    2. sentence_b: A **single** sentence that may or may not follow sentence sentence_a\n\nThis likelihood that sentence_b follows sentenced_a is returned as a boolean value that is either True or False indicating if it is true that sentence B follows sentence A.  \n\n###### Example 1:\n```sh\nfrom happytransformer import HappyBERT\n#--------------------------------------#\nhappy_bert = HappyBERT()\nsentence_a = \"How old are you?\"\nsentence_b = \"I am 93 years old.\"\nsentence_c = \"The Eiffel Tower is in Paris.\"\nresult = happy_bert.predict_next_sentence(sentence_a, sentence_b)\nprint(type(result)) # prints: <class 'bool'>\nprint(result) # prints: True\nresult = happy_bert.predict_next_sentence(sentence_a, sentence_c)\nprint(type(result)) # prints: <class 'bool'>\nprint(result) # prints: False\n```\n\n\n## Question Answering\n\n*Determine the answer to a given question using a body of supplied text.*\n\n\n**HappyBERT** has a method called \"answer_question\" which is used for question answering tasks.\nThe method takes the following arguments:\n\n    1. question: The question to be answered\n    2. text: The text containing the answer to the question\n\nThe output from the method is the answer to the question, returned as a string.\n\n###### Example 1:\n```sh\nfrom happytransformer import HappyBERT\n#--------------------------------------#\nhappy_bert = HappyBERT()\nquestion = \"Who does Ernie live with?\"\ntext = \"Ernie is an orange Muppet character on the long running PBS and HBO children's television show Sesame Street. He and his roommate Bert form the comic duo Bert and Ernie, one of the program's centerpieces, with Ernie acting the role of the na\u00efve troublemaker and Bert the world weary foil.\"  # Source: https://en.wikipedia.org/wiki/Ernie_(Sesame_Street)\nresult = happy_bert.answer_question(question, text)\nprint(type(result)) # prints: <class 'str'>\nprint(result) # prints: bert\n```\n\n\n## Masked Word Prediction Fine-Tuning\n\n*Fine-tune a state-of-the-art masked word prediction model with just a text file*\n\nEach HappyBERT and HappyROBERTA both have 4 methods that are associated with masked word prediction fine-tuning\nThey are:\n\n```\n1. init_mwp(args)\n2. train_mwp(training_path)\n3. eval_mwp(testing_path,batch_size)\n4. predict_mask(text, options, num_results)\n```\n\n### init_mwp(args)\n\n*Initialize the model for masked word prediction training.*\n\n#### Example 1 \n```python\nfrom happytransformer import HappyROBERTA\n#----------------------------------------#\n\nRoberta = HappyROBERTA()\n\nRoberta.init_mwp() # Initialize the training model with default settings\n\n```\n\nYou can also customize the training parameters by inputting a dictionary with specific training parameters.  The dictionary must have the same keys as the dictionary shown below. \n```python\nword_prediction_args = {\n\n\"batch_size\": 1,\n\n\"epochs\": 1,\n\n\"lr\": 5e-5,\n\n\"adam_epsilon\": 1e-8\n\n} \n```\nThe args are:\n\n- batch_size: How many sequences the model processes on one iteration.\n\n- epochs: This refers to how many times the model will train on the same dataset.\n\n-  lr (learning rate): How quickly the model learns.\n\n-  adam_epsilon: This is used to avoid diving by zero when gradient is almost zero.\n\n\nThe recommended for the parameters are:\n\n- lr: 1e-4 used in BERT and ROBERTA [1]\n\n- Adam Epsilon: 1e-6 used by Huggingface team [2]\n\n- batch_size: Depend on the user's vram, Typically 2 to 3\n\n\n#### Example 2 \n\n```python\nfrom happytransformer import HappyROBERTA\n#----------------------------------------#\n\nhappy_roberta = HappyROBERTA()\n\nword_prediction_args = {\n\"batch_size\": 4,\n\n\"epochs\": 2,\n\n\"lr\": 3e-5,\n\n\"adam_epsilon\": 1e-8\n\n} \n\nhappy_roberta.init_mwp(word_prediction_args)\n\n```\n\n\n### train_mwp(training_path)\n*Trains the model on Masked Language Modelling Loss.*\n\nArgument:\n1. testing_path: A string directory path to the .txt that contains the testing data.\n\nExample training.txt :\n```\nI want to get healthy in 2011 .\nI want to boost my immune system , cut that nasty dairy out , and start exercising on a regular basis .\nThat doesn 't seem to hard to follow does it ?\n```\n\n### eval_mwp(testing_path,batch_size) <br />\n*Evaluates the model on Masked Language Modelling loss and return both perplexity and masked language modelling loss.*\n\n\nPerplexity: Mathematicall it is ![equation](https://latex.codecogs.com/gif.latex?2^{Entropy}) where Entropy is the disorder in the system. Lower the perplexity the better the model is performing.\n\nMasked language modelling loss:\tsee [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/perplexity-intuition-and-derivation-105dd481c8f3) for the explanation.\n\nArguments:\n```\n1. testing_path: A string directory path to the .txt that contains the testing data.\n2. batch_size: An integer. Will default to 2.\n```\n\nExample testing.txt :\n```\nIn the few short months since Dan 's mother had moved to town , Saturday had gone from being my favourite day of the week to the one I looked forward to the least.\nAlthough she came when Dan was at work , she invariably stayed all day .\nIt was like living in a goldfish bowl and Dan was on edge the minute he came through the door.\n```\n\n\nNote 2: Evaluating on Cpu is not recommended as it will take considerably longer.\n\n## Example 3:\n\n```python\nfrom happytransformer import HappyROBERTA\n#----------------------------------------#\n\nhappy_roberta = HappyROBERTA()\nhappy_roberta.init_mwp(word_prediction_args)\n\ntrain_path = \"data/train.txt\"\nhappy_roberta.train_mwp(train_path)\n\neval_path = \"data/eval.txt\"\neval_results = happy_roberta.eval_mwp(eval_path)\n\nprint(type(eval_results)) # prints: <class 'dict'>\nprint(eval_results) # prints: {'perplexity': 7.863316059112549, 'eval_loss': 2.0622084404198864}\n\n```\n\n\n### Predicting masked word with fine-tuned model\n\n```python\ntext = \"Linear algebra is a branch of [MASK]\"\n\noptions = [\"music\", \"mathematics\", \"geography\"]\n\nresults = happy_roberta.predict_mask(text, options=options, num_results=3)\n\nprint(type(results)) # prints: <class 'list'>\n\nprint(results) # prints: [{'word': 'mathematics', 'softmax': 0.16551}, {'word': 'music', 'softmax': 3.91739e-05}, {'word': 'geography', 'softmax': 2.9731e-05}]\n\n```\n\n## Tech\n\n Happy Transformer uses a number of open source projects:\n\n* [transformers](https://github.com/huggingface/transformers/stargazers) - State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch!\n*  [pytorch](https://github.com/pytorch/pytorch) - Tensors and Dynamic neural networks in Python\n* [scikit-learn](https://github.com/scikit-learn/scikit-learn) - A set of python modules for machine learning and data mining\n* [numpy](https://github.com/numpy/numpy) - Array computation \n* [pandas](https://github.com/pandas-dev/pandas) - Powerful data structures for data analysis, time series, and statistics\n* [tqdm](https://github.com/tqdm/tqdm) - A Fast, Extensible Progress Bar for Python and CLI\n*  [pytorch-transformers-classification](https://github.com/ThilinaRajapakse/pytorch-transformers-classification) - Text classification for BERT, RoBERTa, XLNet and XLM\n\n HappyTransformer is also an open source project with this [public repository](https://github.com/EricFillion/happy-transformer)\n on GitHub. \n\n### Call for contributors \n Happy Transformer is a new and growing API. We're seeking more contributors to help accomplish our mission of making state-of-the-art AI easier to use.  \n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "https://github.com/EricFillion/happy-transformer/archive/1.1.0.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/EricFillion/happy-transformer", "keywords": "bert,roberta,xlnet,word,predictionmasked,transformer,happy,HappyTransformer,binary,sequence,classification,pytorch,nlp,nlu,natural,language,processing,understanding", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "happytransformer", "package_url": "https://pypi.org/project/happytransformer/", "platform": "", "project_url": "https://pypi.org/project/happytransformer/", "project_urls": {"Download": "https://github.com/EricFillion/happy-transformer/archive/1.1.0.tar.gz", "Homepage": "https://github.com/EricFillion/happy-transformer"}, "release_url": "https://pypi.org/project/happytransformer/1.1.0/", "requires_dist": ["numpy", "torch", "pandas", "tqdm", "scikit-learn", "transformers"], "requires_python": "", "summary": "Happy Transformer is an API built on top of PyTorch's transformer library that makes it easy to utilize state-of-the-art NLP models.", "version": "1.1.0"}, "last_serial": 6649789, "releases": {"1.0.0": [{"comment_text": "", "digests": {"md5": "c969c00ea63495361458e40e3d2d5f1a", "sha256": "e3c3906b0d27372fbcb479dcc7518266c5cc9f869149ff864e932177ef36d75d"}, "downloads": -1, "filename": "happytransformer-1.0.0.tar.gz", "has_sig": false, "md5_digest": "c969c00ea63495361458e40e3d2d5f1a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22191, "upload_time": "2020-01-11T19:47:02", "upload_time_iso_8601": "2020-01-11T19:47:02.721033Z", "url": "https://files.pythonhosted.org/packages/c7/75/2de121bb521b51abd9f66c35d20a5d327d697a15acb8d2c03a5fda0bc012/happytransformer-1.0.0.tar.gz"}], "1.0.1": [{"comment_text": "", "digests": {"md5": "56a3ba0c2c0533bac796a899d8af0fc8", "sha256": "b0b7617430a98ac88623c085f7f32f1c3a0ea27504a6e4b9dab82032aea0d78b"}, "downloads": -1, "filename": "happytransformer-1.0.1.tar.gz", "has_sig": false, "md5_digest": "56a3ba0c2c0533bac796a899d8af0fc8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22189, "upload_time": "2020-01-11T20:17:23", "upload_time_iso_8601": "2020-01-11T20:17:23.234710Z", "url": "https://files.pythonhosted.org/packages/2f/ec/00f29acc3e412473b5723b64733f45b070137f4d3aba256b1b2daead53ab/happytransformer-1.0.1.tar.gz"}], "1.0.2": [{"comment_text": "", "digests": {"md5": "48ecc676a8e772a55504bad09625d59a", "sha256": "8bd0e96fdb7823e9160cc4219bfbffe52c59e7ec6cbef2d3cd708abb863b327b"}, "downloads": -1, "filename": "happytransformer-1.0.2.tar.gz", "has_sig": false, "md5_digest": "48ecc676a8e772a55504bad09625d59a", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22411, "upload_time": "2020-01-24T23:39:39", "upload_time_iso_8601": "2020-01-24T23:39:39.020082Z", "url": "https://files.pythonhosted.org/packages/b9/f5/83c3f177b4bac8a895cb50583a9046d66b82434d45991568e636682ef5c5/happytransformer-1.0.2.tar.gz"}], "1.0.3": [{"comment_text": "", "digests": {"md5": "e583f1c83ad927d3fa5f321706de18b8", "sha256": "06b1bff4eaf3552b3730c407355ba76d00315628cd33b5b81fdf5db544ce0fc8"}, "downloads": -1, "filename": "happytransformer-1.0.3.tar.gz", "has_sig": false, "md5_digest": "e583f1c83ad927d3fa5f321706de18b8", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 22424, "upload_time": "2020-01-25T00:20:06", "upload_time_iso_8601": "2020-01-25T00:20:06.590912Z", "url": "https://files.pythonhosted.org/packages/cb/02/1b0256dbc53ed30e39ba47554b3ff487ec27e25b1249f85d1adb30caf6a1/happytransformer-1.0.3.tar.gz"}], "1.0.4": [{"comment_text": "", "digests": {"md5": "68d382b72428b818dadd4a4777232008", "sha256": "9b23e862c2c183f1608ba56eb078b6507c8d3b918f3091a24671a4087d5b3364"}, "downloads": -1, "filename": "happytransformer-1.0.4.tar.gz", "has_sig": false, "md5_digest": "68d382b72428b818dadd4a4777232008", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 23539, "upload_time": "2020-01-27T01:00:24", "upload_time_iso_8601": "2020-01-27T01:00:24.898501Z", "url": "https://files.pythonhosted.org/packages/c1/49/b015e6ca91e52ff3140b8f87e0e7b1f8c83608369f18a23b16628eabb28f/happytransformer-1.0.4.tar.gz"}], "1.1.0": [{"comment_text": "", "digests": {"md5": "1d0e70d01ff8fcbf48afa247f154f125", "sha256": "ddb91c42663144784015b869559cfc27af81a4e8a2f0a3fe21c31ada7faa30c9"}, "downloads": -1, "filename": "happytransformer-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1d0e70d01ff8fcbf48afa247f154f125", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 32339, "upload_time": "2020-02-17T22:42:59", "upload_time_iso_8601": "2020-02-17T22:42:59.351443Z", "url": "https://files.pythonhosted.org/packages/09/d6/daac39fd9f20015be74f373990cd43275a3d7ef33dccf62e5d8d99c29fca/happytransformer-1.1.0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "14f8a0a17307c902795b14ce4a03d119", "sha256": "9e912dbb6e39b0950b1e6ed4c4cb4ce67d9fa4958649fda1de650c76eaeb10aa"}, "downloads": -1, "filename": "happytransformer-1.1.0.tar.gz", "has_sig": false, "md5_digest": "14f8a0a17307c902795b14ce4a03d119", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30047, "upload_time": "2020-02-17T22:43:00", "upload_time_iso_8601": "2020-02-17T22:43:00.691472Z", "url": "https://files.pythonhosted.org/packages/c0/75/92d5ae8b00bc281e444086edaad6b808d7ba0ea732cde18327451cf522f4/happytransformer-1.1.0.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "1d0e70d01ff8fcbf48afa247f154f125", "sha256": "ddb91c42663144784015b869559cfc27af81a4e8a2f0a3fe21c31ada7faa30c9"}, "downloads": -1, "filename": "happytransformer-1.1.0-py3-none-any.whl", "has_sig": false, "md5_digest": "1d0e70d01ff8fcbf48afa247f154f125", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 32339, "upload_time": "2020-02-17T22:42:59", "upload_time_iso_8601": "2020-02-17T22:42:59.351443Z", "url": "https://files.pythonhosted.org/packages/09/d6/daac39fd9f20015be74f373990cd43275a3d7ef33dccf62e5d8d99c29fca/happytransformer-1.1.0-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "14f8a0a17307c902795b14ce4a03d119", "sha256": "9e912dbb6e39b0950b1e6ed4c4cb4ce67d9fa4958649fda1de650c76eaeb10aa"}, "downloads": -1, "filename": "happytransformer-1.1.0.tar.gz", "has_sig": false, "md5_digest": "14f8a0a17307c902795b14ce4a03d119", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 30047, "upload_time": "2020-02-17T22:43:00", "upload_time_iso_8601": "2020-02-17T22:43:00.691472Z", "url": "https://files.pythonhosted.org/packages/c0/75/92d5ae8b00bc281e444086edaad6b808d7ba0ea732cde18327451cf522f4/happytransformer-1.1.0.tar.gz"}]}