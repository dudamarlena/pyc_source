{"info": {"author": "Ramaravind Mothilal, Amit Sharma, Chenhao Tan", "author_email": "raam.arvind93@gmail.com", "bugtrack_url": null, "classifiers": ["License :: OSI Approved :: MIT License", "Operating System :: OS Independent", "Programming Language :: Python :: 3"], "description": "Diverse Counterfactual Explanations (DiCE) for ML\n======================================================================\n\n*How to explain a machine learning model such that the explanation is truthful to the model and yet interpretable to people?*\n\n`Ramaravind K. Mothilal <https://www.linkedin.com/in/ramaravindkm/>`_, `Amit Sharma <http://www.amitsharma.in/>`_, `Chenhao Tan <https://chenhaot.com/>`_\n\n`FAT* '20 paper <https://arxiv.org/abs/1905.07697>`_ | `Docs <https://microsoft.github.io/DiCE>`_ | Live Jupyter notebook |Binder|_\n\n.. |Binder| image:: https://mybinder.org/badge_logo.svg\n.. _Binder:  https://mybinder.org/v2/gh/microsoft/DiCE/master?filepath=notebooks\n\nExplanations are critical for machine learning, especially as machine learning-based systems are being used to inform decisions in societally critical domains such as finance, healthcare, education, and criminal justice.\nHowever, most explanation methods depend on an approximation of the ML model to\ncreate an interpretable explanation. For example,\nconsider a person who applied for a loan and was rejected by the loan distribution algorithm of a financial company. Typically, the company may provide an explanation on why the loan was rejected, for example, due to \"poor credit history\". However, such an explanation does not help the person decide *what they do should next* to improve their chances of being approved in the future. Critically, the most important feature may not be enough to flip the decision of the algorithm, and in practice, may not even be changeable such as gender and race.\n\n\nDiCE implements `counterfactual (CF) explanations <https://arxiv.org/abs/1711.00399>`_  that provide this information by showing feature-perturbed versions of the same person who would have received the loan, e.g., ``you would have received the loan if your income was higher by $10,000``. In other words, it provides \"what-if\" explanations for model output and can be a useful complement to other explanation methods, both for end-users and model developers.\n\nBarring simple linear models, however, it is difficult to generate CF examples that work for any machine learning model. DiCE is based on `recent research <https://arxiv.org/abs/1905.07697>`_ that generates CF explanations for any ML model. The core idea is to setup finding such explanations as an optimization problem, similar to finding adversarial examples. The critical difference is that for explanations, we need perturbations that change the output of a machine learning model, but are also diverse and feasible to change. Therefore, DiCE supports generating a set of counterfactual explanations  and has tunable parameters for diversity and proximity of the explanations to the original input. It also supports simple constraints on features to ensure feasibility of the generated counterfactual examples.\n\n\nInstalling DICE\n-----------------\nDiCE supports Python 3+. To install DiCE and its dependencies, run this from the top-most folder of the repo:\n\n.. code:: bash\n\n    pip install -e .\n\nIf you face any problems, try installing dependencies manually.\n\n.. code:: bash\n\n    pip install -r requirements.txt\n\nDiCE requires the following packages:\n\n* numpy\n* scikit-learn\n* pandas\n* h5py\n* tensorflow (We use DiCE with `TensorFlow 1.13.0-rc1 <https://github.com/tensorflow/tensorflow/releases/tag/v1.13.0-rc1>`_ in the notebooks, however, it works with Tensorflow>=1.13 as well.)\n\nGetting started with DiCE\n-------------------------\nWith DiCE, generating explanations is a simple three-step  process: train\nmode and then invoke DiCE to generate counterfactual examples for any input.\n\n.. code:: python\n\n    import dice_ml\n    from dice_ml.utils import helpers # helper functions\n    # Dataset for training an ML model\n    d = dice_ml.Data(dataframe=helpers.load_adult_income_dataset(),\n                     continuous_features=['age', 'hours_per_week'],\n                     outcome_name='income')\n    # Pre-trained ML model\n    m = dice_ml.Model(model_path=dice_ml.utils.helpers.get_adult_income_modelpath())\n    # DiCE explanation instance\n    exp = dice_ml.Dice(d,m)\n\nFor any given input, we can now generate counterfactual explanations. For\nexample, the following input leads to class 0 (low income).\n\n.. code:: python\n\n    query_instance = {'age':22,\n        'workclass':'Private',\n        'education':'HS-grad',\n        'marital_status':'Single',\n        'occupation':'Service',\n        'race': 'White',\n        'gender':'Female',\n        'hours_per_week': 45}\n\nUsing DiCE, we can now generate examples that would have been classified as class 1 (high income).\n\n.. code:: python\n\n    # Generate counterfactual examples\n    dice_exp = exp.generate_counterfactuals(query_instance, total_CFs=4, desired_class=\"opposite\")\n    # Visualize counterfactual explanation\n    dice_exp.visualize_as_dataframe()\n\n.. image:: docs/_static/getting_started_output.png\n  :width: 400\n  :alt: List of counterfactual examples\n\nFor more details, check out the `Getting Started <https://github.com/microsoft/DiCE/blob/master/notebooks/DiCE_getting_started.ipynb>`_ notebook.\n\nSupported use-cases\n-------------------\n**Data**\n\nDiCE does not need access to the full dataset. It only requires metadata properties for each feature (min, max for continuous features and levels for categorical features). Thus, for sensitive data, the dataset can be provided as:\n\n.. code:: python\n\n    d = data.Data(features={\n                       'age':[17, 90],\n                       'workclass': ['Government', 'Other/Unknown', 'Private', 'Self-Employed'],\n                       'education': ['Assoc', 'Bachelors', 'Doctorate', 'HS-grad', 'Masters', 'Prof-school', 'School', 'Some-college'],\n                       'marital_status': ['Divorced', 'Married', 'Separated', 'Single', 'Widowed'],\n                       'occupation':['Blue-Collar', 'Other/Unknown', 'Professional', 'Sales', 'Service', 'White-Collar'],\n                       'race': ['Other', 'White'],\n                       'gender':['Female', 'Male'],\n                       'hours_per_week': [1, 99]},\n             outcome_name='income')\n\n**Model**\n\nWe support pre-trained models as well as training a model using Tensorflow. Here's a simple example.\n\n.. code:: python\n\n    sess = tf.InteractiveSession()\n    # Generating train and test data\n    train, _ = d.split_data(d.normalize_data(d.one_hot_encoded_data))\n    X_train = train.loc[:, train.columns != 'income']\n    y_train = train.loc[:, train.columns == 'income']\n    # Fitting a dense neural network model\n    ann_model = keras.Sequential()\n    ann_model.add(keras.layers.Dense(20, input_shape=(X_train.shape[1],), kernel_regularizer=keras.regularizers.l1(0.001), activation=tf.nn.relu))\n    ann_model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))\n    ann_model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.01), metrics=['accuracy'])\n    ann_model.fit(X_train, y_train, validation_split=0.20, epochs=100, verbose=0, class_weight={0:1,1:2})\n\n    # Generate the DiCE model for explanation\n    m = model.Model(model=ann_model)\n\nCheck out the last section in `Getting Started <https://github.com/microsoft/DiCE/blob/master/notebooks/DiCE_getting_started.ipynb>`_ notebook to use DiCE with PyTorch.\n\n**Explanations**\n\nWe visualize explanations through a table highlighting the change in features. We plan to support an English language explanation too!\n\nFeasibility of counterfactual explanations\n-------------------------------------------\nWe acknowledge that not all counterfactual explanations may be feasible for a\nuser. In general, counterfactuals closer to an individual's profile will be\nmore feasible. Diversity is also important to help an individual choose between\nmultiple possible options.\n\nDiCE provides tunable parameters for diversity and proximity to generate\ndifferent kinds of explanations.\n\n.. code:: python\n\n    dice_exp = exp.generate_counterfactuals(query_instance,\n                    total_CFs=4, desired_class=\"opposite\",\n                    proximity_weight=1.5, diversity_weight=1.0)\n\nAdditionally, it may be the case that some features are harder to change than\nothers (e.g., education level is harder to change than working hours per week). DiCE allows input of relative difficulty in changing a feature through specifying *feature weights*. A higher feature weight means that the feature is harder to change than others. For instance, one way is to use the mean absolute deviation from the median as a measure of relative difficulty of changing a continuous feature. By default, DiCE computes this internally and divides the distance between continuous features by the MAD of the feature's values in the training set. We can also assign different values through the *feature_weights* parameter. \n\n.. code:: python\n\n    # assigning new weights\n    feature_weights = {'age': 10, 'hours_per_week': 5}\n    # Now generating explanations using the new feature weights\n    dice_exp = exp.generate_counterfactuals(query_instance,\n                    total_CFs=4, desired_class=\"opposite\",\n                    feature_weights=feature_weights)\n\nFinally, some features are impossible to change such as one's age or race. Therefore, DiCE also allows inputting a\nlist of features to vary.\n\n.. code:: python\n\n    dice_exp = exp.generate_counterfactuals(query_instance,\n                    total_CFs=4, desired_class=\"opposite\",\n                    features_to_vary=['age','workclass','education','occupation','hours_per_week'])\n\nIt also supports simple constraints on\nfeatures that reflect practical constraints (e.g., working hours per week\ncannot be more than 50).\n\nFor more details, check out `this <https://github.com/microsoft/DiCE/blob/master/notebooks/DiCE_with_advanced_options.ipynb>`_ notebook.\n\nThe promise of counterfactual explanations\n-------------------------------------------\nBeing truthful to the model, counterfactual explanations can be useful to all stakeholders for a decision made by a machine learning model that makes decisions.\n\n* **Decision subjects**: Counterfactual explanations can be used to explore actionable recourse for a person based on a decision received by a ML model. DiCE shows decision outcomes with *actionable* alternative profiles, to help people understand what they could have done to change their model outcome.\n\n* **ML model developers**: Counterfactual explanations are also useful for model developers to debug their model for potential problems. DiCE can be used to show CF explanations for a selection of inputs that can uncover if there are any problematic (in)dependences on some features (e.g., for 95% of inputs, changing features X and Y change the outcome, but not for the other 5%). We aim to support aggregate metrics to help developers debug ML models.\n\n* **Decision makers**: Counterfactual explanations may be useful to\n  decision-makers such as doctors or judges who may use ML models to make decisions. For a particular individual, DiCE allows probing the ML model to see the possible changes that lead to a different ML outcome, thus enabling decision-makers to assess their trust in the prediction.\n\n* **Decision evaluators**: Finally, counterfactual explanations can be useful\n  to decision evaluators who may be interested in fairness or other desirable\n  properties of an ML model. We plan to add support for this in the future.\n\n\nRoadmap\n-------\nIdeally, counterfactual explanations should balance between a wide range of suggested changes (*diversity*), and the relative ease of adopting those changes (*proximity* to the original input), and also follow the causal laws of the world, e.g., one can hardly lower their educational degree or change their race.\n\nWe are working on adding the following features to DiCE:\n\n* Incorporating causal constraints when generating counterfactual explanations (check out `feasible-CF branch <https://github.com/microsoft/DiCE/tree/feasible-cf>`_)\n* Support for PyTorch and scikit-learn models\n* Support for using DiCE for debugging machine learning models\n* Support for other algorithms for generating counterfactual explanations\n\n\n\nContributing\n------------\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the `Microsoft Open Source Code of Conduct <https://opensource.microsoft.com/codeofconduct/>`_.\nFor more information see the `Code of Conduct FAQ <https://opensource.microsoft.com/codeofconduct/faq/>`_ or\ncontact `opencode@microsoft.com <mailto:opencode@microsoft.com>`_ with any additional questions or comments.\n\n\n", "description_content_type": "text/x-rst", "docs_url": null, "download_url": "https://github.com/microsoft/DiCE/archive/v0.2.tar.gz", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/microsoft/DiCE", "keywords": "machine-learning explanation interpretability counterfactual", "license": "MIT", "maintainer": "", "maintainer_email": "", "name": "dice-ml", "package_url": "https://pypi.org/project/dice-ml/", "platform": "", "project_url": "https://pypi.org/project/dice-ml/", "project_urls": {"Download": "https://github.com/microsoft/DiCE/archive/v0.2.tar.gz", "Homepage": "https://github.com/microsoft/DiCE"}, "release_url": "https://pypi.org/project/dice-ml/0.2/", "requires_dist": ["numpy", "pandas", "scikit-learn", "tensorflow (>=1.13)", "h5py"], "requires_python": ">=3.4", "summary": "Generate Diverse Counterfactual Explanations for any machine learning model.", "version": "0.2"}, "last_serial": 6828003, "releases": {"0.2": [{"comment_text": "", "digests": {"md5": "02dfb92de4f78af54c8e127b09dda600", "sha256": "fddf63e69aa29784a1054951b4d345833f2b40e57b8277766627b0f323b2fd66"}, "downloads": -1, "filename": "dice_ml-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "02dfb92de4f78af54c8e127b09dda600", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 53746, "upload_time": "2020-03-17T10:49:01", "upload_time_iso_8601": "2020-03-17T10:49:01.360854Z", "url": "https://files.pythonhosted.org/packages/bc/99/9a1f296c6cbde33bcfbe6e2eb82ba4f079d25584cd8acc1bbd296afa0c19/dice_ml-0.2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9a036d7805a9eb035118904fa8bd61fb", "sha256": "f1b9175d014330f1275c361faea2a76b44fce2595841fe803e1c9014071cbfdc"}, "downloads": -1, "filename": "dice_ml-0.2.tar.gz", "has_sig": false, "md5_digest": "9a036d7805a9eb035118904fa8bd61fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 43085, "upload_time": "2020-03-17T10:49:03", "upload_time_iso_8601": "2020-03-17T10:49:03.721452Z", "url": "https://files.pythonhosted.org/packages/02/7e/49830f440f1d8f41d760a585ef3099b2174717b6c786bba640b026926dc9/dice_ml-0.2.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "02dfb92de4f78af54c8e127b09dda600", "sha256": "fddf63e69aa29784a1054951b4d345833f2b40e57b8277766627b0f323b2fd66"}, "downloads": -1, "filename": "dice_ml-0.2-py3-none-any.whl", "has_sig": false, "md5_digest": "02dfb92de4f78af54c8e127b09dda600", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": ">=3.4", "size": 53746, "upload_time": "2020-03-17T10:49:01", "upload_time_iso_8601": "2020-03-17T10:49:01.360854Z", "url": "https://files.pythonhosted.org/packages/bc/99/9a1f296c6cbde33bcfbe6e2eb82ba4f079d25584cd8acc1bbd296afa0c19/dice_ml-0.2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9a036d7805a9eb035118904fa8bd61fb", "sha256": "f1b9175d014330f1275c361faea2a76b44fce2595841fe803e1c9014071cbfdc"}, "downloads": -1, "filename": "dice_ml-0.2.tar.gz", "has_sig": false, "md5_digest": "9a036d7805a9eb035118904fa8bd61fb", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.4", "size": 43085, "upload_time": "2020-03-17T10:49:03", "upload_time_iso_8601": "2020-03-17T10:49:03.721452Z", "url": "https://files.pythonhosted.org/packages/02/7e/49830f440f1d8f41d760a585ef3099b2174717b6c786bba640b026926dc9/dice_ml-0.2.tar.gz"}]}