{"info": {"author": "Damavis", "author_email": "info@damavis.com", "bugtrack_url": null, "classifiers": ["Environment :: Plugins", "License :: OSI Approved :: Apache Software License", "Operating System :: Unix", "Programming Language :: Python :: 3"], "description": "# Pentaho Airflow plugin\n\n[![Build Status](https://travis-ci.org/damavis/airflow-pentaho-plugin.svg?branch=master)](https://travis-ci.org/damavis/airflow-pentaho-plugin)\n[![codecov](https://codecov.io/gh/damavis/airflow-pentaho-plugin/branch/master/graph/badge.svg)](https://codecov.io/gh/damavis/airflow-pentaho-plugin)\n[![PyPI](https://img.shields.io/pypi/v/airflow-pentaho-plugin)](https://pypi.org/project/airflow-pentaho-plugin/)\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/airflow-pentaho-plugin)](https://pypi.org/project/airflow-pentaho-plugin/)\n\nThis plugins runs Jobs and Transformations through Carte servers.\nIt allows to orchestrate a massive number of trans/jobs taking care \nof the dependencies between them, even between different instances.\nThis is done by using `CarteJobOperator` and `CarteTransOperator`\n\nIt also runs Pan (transformations) and Kitchen (Jobs) in local mode,\nboth from repository and local XML files. For this approach, use\n`KitchenOperator` and `PanOperator`\n\n## Requirements\n\n1. A Apache Airflow system deployed.\n2. One or many working PDI CE installations.\n3. A Carte server for Carte Operators.\n\n## Setup\n\nThe same setup process must be performed on webserver, scheduler\nand workers (that runs this tasks) to get it working. If you want to\ndeploy specific workers to run this kind of tasks, see \n[Queues](https://airflow.apache.org/docs/stable/concepts.html#queues),\nin **Airflow** *Concepts* section.\n\n### Pip package\n\nFirst of all, the package should be installed via `pip install` command.\n\n```bash\npip install airflow-pentaho-plugin\n```\n\n### Airflow connection\n\nThen, a new connection needs to be added to Airflow Connections, to do this,\ngo to Airflow web UI, and click on `Admin -> Connections` on the top menu.\nNow, click on `Create` tab.\n\nEnter the **Conn Id**, this plugin uses `pdi_default` by default, the username\nand the password for your Pentaho Repository.\n\nAt the bottom of the form, fill the **Extra** field with `pentaho_home`, the\npath where your pdi-ce is placed, and `rep`, the repository name for this\nconnection, using a json formatted string like it follows.\n\n```json\n{\n    \"pentaho_home\": \"/opt/pentaho\",\n    \"rep\": \"Default\"\n}\n```\n\n### Carte\n\nIn order to use `CarteJobOperator`, the connection should be set different. Fill\n`host` and `port` for Carte hostname and port, `username` and `password` for PDI\nrepository, and `extra` as it follows.\n\n```json\n{\n    \"rep\": \"Default\",\n    \"carte_username\": \"cluster\",\n    \"carte_password\": \"cluster\"\n}\n```\n\n## Usage\n\n### CarteJobOperator\n\nCarteJobOperator is responsible for running jobs in remote slave servers. Here\nit is an example of `CarteJobOperator` usage.\n\n```python\nfrom airflow.operators.pentaho import CarteJobOperator\n\n# ... #\n\n# Define the task using the CarteJobOperator\navg_spent = CarteJobOperator(\n    conn_id='pdi_default',\n    task_id=\"average_spent\",\n    job=\"/home/bi/average_spent\",\n    params={\"date\": \"{{ ds }}\"},  # Date in yyyy-mm-dd format\n    dag=dag)\n\n# ... #\n\nsome_task >> avg_spent >> another_task\n```\n\n### KitchenOperator\n\nKitchen operator is responsible for running Jobs. Lets suppose that we have\na defined *Job* saved on `/home/bi/average_spent` in our repository with\nthe argument `date` as input parameter. Lets define the task using the\n`KitchenOperator`.\n\n```python\nfrom airflow.operators.pentaho import KitchenOperator\n\n# ... #\n\n# Define the task using the KitchenOperator\navg_spent = KitchenOperator(\n    conn_id='pdi_default',\n    queue=\"pdi\",\n    task_id=\"average_spent\",\n    directory=\"/home/bi\",\n    job=\"average_spent\",\n    params={\"date\": \"{{ ds }}\"},  # Date in yyyy-mm-dd format\n    dag=dag)\n\n# ... #\n\nsome_task >> avg_spent >> another_task\n```\n\n### CarteTransOperator\n\nCarteTransOperator is responsible for running transformations in remote slave\nservers. Here it is an example of `CarteTransOperator` usage.\n\n```python\nfrom airflow.operators.pentaho import CarteTransOperator\n\n# ... #\n\n# Define the task using the CarteJobOperator\nenriche_customers = CarteTransOperator(\n    conn_id='pdi_default',\n    task_id=\"enrich_customer_data\",\n    job=\"/home/bi/enrich_customer_data\",\n    params={\"date\": \"{{ ds }}\"},  # Date in yyyy-mm-dd format\n    dag=dag)\n\n# ... #\n\nsome_task >> enrich_customers >> another_task\n```\n\n### PanOperator\n\nPan operator is responsible for running transformations. Lets suppose that\nwe have one saved on `/home/bi/clean_somedata`. Lets define the task using the\n`PanOperator`. In this case, the transformation receives a parameter that\ndetermines the file to be cleaned.\n\n```python\nfrom airflow.operators.pentaho import PanOperator\n\n# ... #\n\n# Define the task using the KitchenOperator\nclean_input = PanOperator(\n    conn_id='pdi_default',\n    queue=\"pdi\",\n    task_id=\"cleanup\",\n    directory=\"/home/bi\",\n    trans=\"clean_somedata\",\n    params={\"file\": \"/tmp/input_data/{{ ds }}/sells.csv\"},\n    dag=dag)\n\n# ... #\n\nsome_task >> clean_input >> another_task\n```\n\nFor more information, please see `sample_dags/pdi_flow.py`", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/damavis/airflow-pentaho-plugin", "keywords": "", "license": "Apache 2.0", "maintainer": "", "maintainer_email": "", "name": "airflow-pentaho-plugin", "package_url": "https://pypi.org/project/airflow-pentaho-plugin/", "platform": "", "project_url": "https://pypi.org/project/airflow-pentaho-plugin/", "project_urls": {"Homepage": "https://github.com/damavis/airflow-pentaho-plugin"}, "release_url": "https://pypi.org/project/airflow-pentaho-plugin/0.1.0b3.post1/", "requires_dist": null, "requires_python": ">=3.6", "summary": "", "version": "0.1.0b3.post1"}, "last_serial": 6927819, "releases": {"0.1.0b3": [{"comment_text": "", "digests": {"md5": "912d120b38a0d067fd6e19bcf792d56d", "sha256": "55f8b448bab59aac96e3076a8b1c84997b7efe5e0e5631255e4f383ba451dad2"}, "downloads": -1, "filename": "airflow-pentaho-plugin-0.1.0b3.tar.gz", "has_sig": false, "md5_digest": "912d120b38a0d067fd6e19bcf792d56d", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8206, "upload_time": "2020-04-01T07:51:25", "upload_time_iso_8601": "2020-04-01T07:51:25.902371Z", "url": "https://files.pythonhosted.org/packages/7b/28/a0b62a733089f9b9929fe45af5a5a712d0eedf87258aaff2c2be229475f0/airflow-pentaho-plugin-0.1.0b3.tar.gz"}], "0.1.0b3.post1": [{"comment_text": "", "digests": {"md5": "6ed8886908049a84fe262dd597cb57f6", "sha256": "02f47f72883b2da33f6415b883c88187a8debedfabd9b3157c5cf2a9b42a2a65"}, "downloads": -1, "filename": "airflow-pentaho-plugin-0.1.0b3.post1.tar.gz", "has_sig": false, "md5_digest": "6ed8886908049a84fe262dd597cb57f6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8989, "upload_time": "2020-04-01T14:00:55", "upload_time_iso_8601": "2020-04-01T14:00:55.078050Z", "url": "https://files.pythonhosted.org/packages/49/12/d6df05b2748c06321d93094d05a019134b16bc739e9a23e46acca7364f73/airflow-pentaho-plugin-0.1.0b3.post1.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "6ed8886908049a84fe262dd597cb57f6", "sha256": "02f47f72883b2da33f6415b883c88187a8debedfabd9b3157c5cf2a9b42a2a65"}, "downloads": -1, "filename": "airflow-pentaho-plugin-0.1.0b3.post1.tar.gz", "has_sig": false, "md5_digest": "6ed8886908049a84fe262dd597cb57f6", "packagetype": "sdist", "python_version": "source", "requires_python": ">=3.6", "size": 8989, "upload_time": "2020-04-01T14:00:55", "upload_time_iso_8601": "2020-04-01T14:00:55.078050Z", "url": "https://files.pythonhosted.org/packages/49/12/d6df05b2748c06321d93094d05a019134b16bc739e9a23e46acca7364f73/airflow-pentaho-plugin-0.1.0b3.post1.tar.gz"}]}