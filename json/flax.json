{"info": {"author": "Flax team", "author_email": "flax-dev@google.com", "bugtrack_url": null, "classifiers": ["Development Status :: 3 - Alpha", "Intended Audience :: Developers", "Intended Audience :: Science/Research", "License :: OSI Approved :: MIT License", "Programming Language :: Python :: 3.7", "Topic :: Scientific/Engineering :: Artificial Intelligence"], "description": "# Flax: A neural network library for JAX designed for flexibility\n\n**NOTE**: This is alpha software, but we encourage trying it out.\nChanges will come to the API, but we'll use deprecation warnings when we can, and\nkeep track of them our [Changelog](CHANGELOG.md).\n\nA growing community of researchers at Google are happily using\nFlax for their daily research and contributing to it, and now we're\nexpanding that to the open source community.\n\n[GitHub issues](http://github.com/google/flax/issues) are encouraged for open conversation, but\nin case you need to reach us directly, we're at flax-dev@google.com.\n ## Quickstart\n\n**\u27f6 [Full documentation and API reference](https://flax.readthedocs.io/)**\n\n**\u27f6 [Annotated full end-to-end MNIST example](docs/annotated_mnist.md)**\n\n**\u27f6 [The Flax Guide](https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html)** -- a guided walkthrough of the parts of Flax\n\n## Background: JAX\n\n[JAX](https://github.com/google/jax) is NumPy + autodiff + GPU/TPU\n\nIt allows for fast scientific computing and machine learning\nwith the normal NumPy API\n(+ additional APIs for special accelerator ops when needed)\n\nJAX comes with powerful primitives, which you can compose arbitrarily:\n\n* Autodiff (`jax.grad`): Efficient any-order gradients w.r.t any variables\n* JIT compilation (`jax.jit`): Trace any function \u27f6 fused accelerator ops\n* Vectorization (`jax.vmap`): Automatically batch code written for individual samples\n* Parallelization (`jax.pmap`): Automatically parallelize code across multiple accelerators (including across hosts, e.g. for TPU pods)\n\n## What is Flax?\n\nFlax is a high-performance neural network library for\nJAX that is **designed for flexibility**:\nTry new forms of training by forking an example and by modifying the training\nloop, not by adding features to a framework.\n\nFlax is being developed in close collaboration with the JAX team and \ncomes with everything you need to start your research, including:\n\n* Common layers (`flax.nn`): Dense, Conv, {Batch|Layer|Group} Norm, Attention, Pooling, {LSTM|GRU} Cell, Dropout\n\n* Optimizers (`flax.optim`): SGD, Momentum, Adam, LARS\n\n* Utilities and patterns: replicated training, serialization and checkpointing, metrics, prefetching on device\n\n* Educational examples that work out of the box: MNIST, LSTM seq2seq, Graph Neural Networks, Sequence Tagging\n\n* HOWTO guides -- diffs that add functionality to educational base exampless\n\n* Fast, tuned large-scale end-to-end examples: CIFAR10, ResNet ImageNet, Transformer LM1b\n\n### An annotated MNIST example\n\nSee [docs/annotated_mnist.md](docs/annotated_mnist.md) for an MNIST\nexample with detailed annotations for each code block.\n\n### Flax Modules\n\nThe core of Flax is the Module abstraction. Modules allow you to write parameterized functions just as if you were writing a normal numpy function with JAX. The Module api allows you to declare parameters and use them directly with the JAX api\u2019s.\n\nModules are the one part of Flax with \"magic\" -- the magic is constrained, and enables a very ergonomic style,\nwhere modules are defined in a single function with minimal boilerplate.\n\nA few things to know about Modules:\n\n1. Create a new module by subclassing `flax.nn.Module` and implementing the `apply` method.\n\n2. Within `apply`, call `self.param(name, shape, init_func)` to register a new parameter and returns its initial value.\n\n3. Apply submodules by calling `MySubModule(...args...)` within `MyModule.apply`. Parameters of `MySubModule` are stored\nas a dictionary under the parameters `MyModule`. **NOTE:** this returns the *output* of `MySubModule`, not an instance. To get an access to an instance of `MySubModule` for re-use, use [`Module.partial`](https://flax.readthedocs.io/en/latest/flax.nn.html#flax.nn.Module.partial) or [`Module.shared`](https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html#Parameter-sharing)\n\n4. `MyModule.init(rng, ...)` is a pure function that calls `apply` in \"init mode\" and returnes a nested Python dict of initialized parameter values\n\n5. `MyModule.call(params, ...)` is a pure function that calls `apply` in \"call mode\" and returnes the output of the module.\n\nFor example you can define a learned linear transformation as follows:\n\n```py\nfrom flax import nn\nimport jax.numpy as jnp\n\nclass Linear(nn.Module):\n  def apply(self, x, num_features, kernel_init_fn):\n    input_features = x.shape[-1]\n    W = self.param('W', (input_features, num_features), kernel_init_fn)\n    return jnp.dot(x, W)\n```\n\nYou can also use `nn.module` as a function decorator to create a new module, as\nlong as you don't need access to `self` for creating parameters directly:\n\n```py\n@nn.module\ndef DenseLayer(x, features):\n  x = flax.nn.Dense(x, features)\n  x = flax.nn.relu(x)\n  return x\n```\n\n**\u27f6 Read more about Modules in the [Flax Guide](https://flax.readthedocs.io/en/latest/notebooks/flax_intro.html#Flax-Modules)**\n\n## CPU-only Installation\n\nYou will need Python 3.5 or later.\n\nNow install `flax` from Github:\n\n```\n> pip install git+https://github.com/google-research/flax.git@prerelease\n```\n\n## GPU accelerated installation\n\nFirst install `jaxlib`; please follow the instructions in the\n[JAX readme](https://github.com/google/jax/blob/master/README.md).\nIf they are not already installed, you will need to install\n[CUDA](https://developer.nvidia.com/cuda-downloads) and\n[CuDNN](https://developer.nvidia.com/cudnn) runtimes.\n\nNow install `flax` from Github:\n\n```\n> pip install git+https://github.com/google-research/flax.git@prerelease\n```\n\n\n\n## List of end-to-end examples\n\n**NOTE**: We are still testing these examples across all supported hardware configurations.\n\n* [ResNet on ImageNet](examples/imagenet)\n\n* [Language Modeling on LM1b](examples/lm1b) with a Transformer architecture\n\n* WIP: [WMT translation](https://github.com/google/flax/pull/61) with a Transformer architecture and on-device beam decoding\n\n# Note\n\nThis is not an official Google product.\n\n\n", "description_content_type": "text/markdown", "docs_url": null, "download_url": "", "downloads": {"last_day": -1, "last_month": -1, "last_week": -1}, "home_page": "https://github.com/google-research/flax", "keywords": "", "license": "Apache", "maintainer": "", "maintainer_email": "", "name": "flax", "package_url": "https://pypi.org/project/flax/", "platform": "", "project_url": "https://pypi.org/project/flax/", "project_urls": {"Homepage": "https://github.com/google-research/flax"}, "release_url": "https://pypi.org/project/flax/0.1.0rc2/", "requires_dist": ["numpy (>=1.12)", "jaxlib (>=0.1.41)", "jax (>=0.1.59)", "matplotlib", "dataclasses", "msgpack"], "requires_python": "", "summary": "Flax: A neural network library for JAX designed for flexibility", "version": "0.1.0rc2"}, "last_serial": 6835011, "releases": {"0.1.0rc1": [{"comment_text": "", "digests": {"md5": "19dafd44431cb1176469b67df2004e37", "sha256": "8c7cceaf0a1116aeb38d5a7f4b24d9baea5a9dddfa4fdf87802363ed2b887ea7"}, "downloads": -1, "filename": "flax-0.1.0rc1-py3-none-any.whl", "has_sig": false, "md5_digest": "19dafd44431cb1176469b67df2004e37", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 60168, "upload_time": "2020-03-17T10:21:38", "upload_time_iso_8601": "2020-03-17T10:21:38.652850Z", "url": "https://files.pythonhosted.org/packages/8c/5e/b4d781e8a1689ed1b71296be8eaf79e21d3e6e4e473a79b6362e65faed02/flax-0.1.0rc1-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "de4c79d3ef5c4a3c3695dc123ddc4d32", "sha256": "d88f24cd59e22fcfe4bfc037c88ecc34d3f7d9ce8db297c317421f7664d6b678"}, "downloads": -1, "filename": "flax-0.1.0rc1.tar.gz", "has_sig": false, "md5_digest": "de4c79d3ef5c4a3c3695dc123ddc4d32", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 47915, "upload_time": "2020-03-17T10:21:40", "upload_time_iso_8601": "2020-03-17T10:21:40.282211Z", "url": "https://files.pythonhosted.org/packages/3d/1b/f363177a24ec73bb28cc6425d85de8f4d589ab51871d9af1b8af4e7c1e2b/flax-0.1.0rc1.tar.gz"}], "0.1.0rc2": [{"comment_text": "", "digests": {"md5": "029a266dd17edafddb1a5e0b01378b2f", "sha256": "1d7b093b11efd7faee42262956ad3b62b7ad9c46f59671e6ba7fda5f2efb19f8"}, "downloads": -1, "filename": "flax-0.1.0rc2-py3-none-any.whl", "has_sig": false, "md5_digest": "029a266dd17edafddb1a5e0b01378b2f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 66196, "upload_time": "2020-03-18T10:51:34", "upload_time_iso_8601": "2020-03-18T10:51:34.133346Z", "url": "https://files.pythonhosted.org/packages/0e/8f/6d772aba2fa63aa6c1fd10d267324c44288c1c5705ac971c7d79cea219bb/flax-0.1.0rc2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9780d569f5086cfc42a91382ca708e64", "sha256": "3cc797122d47ed69a28f93e81a66d316c5463e156f7bf2cc3bd1ef59cdcad93a"}, "downloads": -1, "filename": "flax-0.1.0rc2.tar.gz", "has_sig": false, "md5_digest": "9780d569f5086cfc42a91382ca708e64", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51279, "upload_time": "2020-03-18T10:51:35", "upload_time_iso_8601": "2020-03-18T10:51:35.071793Z", "url": "https://files.pythonhosted.org/packages/91/30/955d6b6143c9d6b819ba85ebce654aae7a235017962fa8e8519110e5ac5f/flax-0.1.0rc2.tar.gz"}]}, "urls": [{"comment_text": "", "digests": {"md5": "029a266dd17edafddb1a5e0b01378b2f", "sha256": "1d7b093b11efd7faee42262956ad3b62b7ad9c46f59671e6ba7fda5f2efb19f8"}, "downloads": -1, "filename": "flax-0.1.0rc2-py3-none-any.whl", "has_sig": false, "md5_digest": "029a266dd17edafddb1a5e0b01378b2f", "packagetype": "bdist_wheel", "python_version": "py3", "requires_python": null, "size": 66196, "upload_time": "2020-03-18T10:51:34", "upload_time_iso_8601": "2020-03-18T10:51:34.133346Z", "url": "https://files.pythonhosted.org/packages/0e/8f/6d772aba2fa63aa6c1fd10d267324c44288c1c5705ac971c7d79cea219bb/flax-0.1.0rc2-py3-none-any.whl"}, {"comment_text": "", "digests": {"md5": "9780d569f5086cfc42a91382ca708e64", "sha256": "3cc797122d47ed69a28f93e81a66d316c5463e156f7bf2cc3bd1ef59cdcad93a"}, "downloads": -1, "filename": "flax-0.1.0rc2.tar.gz", "has_sig": false, "md5_digest": "9780d569f5086cfc42a91382ca708e64", "packagetype": "sdist", "python_version": "source", "requires_python": null, "size": 51279, "upload_time": "2020-03-18T10:51:35", "upload_time_iso_8601": "2020-03-18T10:51:35.071793Z", "url": "https://files.pythonhosted.org/packages/91/30/955d6b6143c9d6b819ba85ebce654aae7a235017962fa8e8519110e5ac5f/flax-0.1.0rc2.tar.gz"}]}