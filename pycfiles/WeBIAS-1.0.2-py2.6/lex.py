# uncompyle6 version 3.7.4
# Python bytecode 2.6 (62161)
# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
# [GCC 8.4.0]
# Embedded file name: build/bdist.linux-x86_64/egg/webias/gnosis/xml/relax/lex.py
# Compiled at: 2015-04-13 16:10:49
r"""
lex.py

This module builds lex-like scanners based on regular expression rules.
To use the module, simply write a collection of regular expression rules
and actions like this:

# lexer.py
import lex

# Define a list of valid tokens
tokens = (
    'IDENTIFIER', 'NUMBER', 'PLUS', 'MINUS'
    )

# Define tokens as functions
def t_IDENTIFIER(t):
    r' ([a-zA-Z_](\w|_)* '
    return t

def t_NUMBER(t):
    r' \d+ '
    return t

# Some simple tokens with no actions
t_PLUS = r'\+'
t_MINUS = r'-'

# Initialize the lexer
lex.lex()

The tokens list is required and contains a complete list of all valid
token types that the lexer is allowed to produce.  Token types are
restricted to be valid identifiers.  This means that 'MINUS' is a valid
token type whereas '-' is not.

Rules are defined by writing a function with a name of the form
t_rulename.  Each rule must accept a single argument which is
a token object generated by the lexer. This token has the following
attributes:

    t.type   = type string of the token.  This is initially set to the
               name of the rule without the leading t_
    t.value  = The value of the lexeme.
    t.lineno = The value of the line number where the token was encountered
    
For example, the t_NUMBER() rule above might be called with the following:
    
    t.type  = 'NUMBER'
    t.value = '42'
    t.lineno = 3

Each rule returns the token object it would like to supply to the
parser.  In most cases, the token t is returned with few, if any
modifications.  To discard a token for things like whitespace or
comments, simply return nothing.  For instance:

def t_whitespace(t):
    r' \s+ '
    pass

For faster lexing, you can also define this in terms of the ignore set like this:

t_ignore = ' \t'

The characters in this string are ignored by the lexer. Use of this feature can speed
up parsing significantly since scanning will immediately proceed to the next token.

lex requires that the token returned by each rule has an attribute
t.type.  Other than this, rules are free to return any kind of token
object that they wish and may construct a new type of token object
from the attributes of t (provided the new object has the required
type attribute).

If illegal characters are encountered, the scanner executes the
function t_error(t) where t is a token representing the rest of the
string that hasn't been matched.  If this function isn't defined, a
LexError exception is raised.  The .text attribute of this exception
object contains the part of the string that wasn't matched.

The t.skip(n) method can be used to skip ahead n characters in the
input stream.  This is usually only used in the error handling rule.
For instance, the following rule would print an error message and
continue:

def t_error(t):
    print "Illegal character in input %s" % t.value[0]
    t.skip(1)

Of course, a nice scanner might wish to skip more than one character
if the input looks very corrupted.

The lex module defines a t.lineno attribute on each token that can be used
to track the current line number in the input.  The value of this
variable is not modified by lex so it is up to your lexer module
to correctly update its value depending on the lexical properties
of the input language.  To do this, you might write rules such as
the following:

def t_newline(t):
    r' \n+ '
    t.lineno += t.value.count("\n")

To initialize your lexer so that it can be used, simply call the lex.lex()
function in your rule file.  If there are any errors in your
specification, warning messages or an exception will be generated to
alert you to the problem.

(dave: this needs to be rewritten)
To use the newly constructed lexer from another module, simply do
this:

    import lex
    import lexer
    plex.input("position = initial + rate*60")

    while 1:
        token = plex.token()       # Get a token
        if not token: break        # No more tokens
        ... do whatever ...

Assuming that the module 'lexer' has initialized plex as shown
above, parsing modules can safely import 'plex' without having
to import the rule file or any additional imformation about the
scanner you have defined.
"""
__version__ = '1.3'
import re, types, sys, copy

class LexError(Exception):

    def __init__(self, message, s):
        self.args = (
         message,)
        self.text = s


class LexToken:

    def __str__(self):
        return 'LexToken(%s,%r,%d)' % (self.type, self.value, self.lineno)

    def __repr__(self):
        return str(self)

    def skip(self, n):
        try:
            self._skipn += n
        except AttributeError:
            self._skipn = n


class Lexer:

    def __init__(self):
        self.lexre = None
        self.lexdata = None
        self.lexpos = 0
        self.lexlen = 0
        self.lexindexfunc = []
        self.lexerrorf = None
        self.lextokens = None
        self.lexignore = None
        self.lineno = 1
        self.debug = 0
        self.optimize = 0
        self.token = self.errtoken
        return

    def __copy__(self):
        c = Lexer()
        c.lexre = self.lexre
        c.lexdata = self.lexdata
        c.lexpos = self.lexpos
        c.lexlen = self.lexlen
        c.lenindexfunc = self.lexindexfunc
        c.lexerrorf = self.lexerrorf
        c.lextokens = self.lextokens
        c.lexignore = self.lexignore
        c.lineno = self.lineno
        c.optimize = self.optimize
        c.token = c.realtoken

    def input(self, s):
        global token
        if not isinstance(s, types.StringType):
            raise ValueError, 'Expected a string'
        self.lexdata = s
        self.lexpos = 0
        self.lexlen = len(s)
        self.token = self.realtoken
        if token == self.errtoken:
            token = self.token

    def errtoken(self):
        raise RuntimeError, 'No input string given with input()'

    def realtoken(self):
        lexpos = self.lexpos
        lexlen = self.lexlen
        lexignore = self.lexignore
        lexdata = self.lexdata
        while lexpos < lexlen:
            if lexdata[lexpos] in lexignore:
                lexpos += 1
                continue
            m = self.lexre.match(lexdata, lexpos)
            if m:
                i = m.lastindex
                lexpos = m.end()
                tok = LexToken()
                tok.value = m.group()
                tok.lineno = self.lineno
                tok.lexer = self
                (func, tok.type) = self.lexindexfunc[i]
                if not func:
                    self.lexpos = lexpos
                    return tok
                self.lexpos = lexpos
                newtok = func(tok)
                self.lineno = tok.lineno
                if not newtok:
                    continue
                if not self.optimize:
                    if not self.lextokens.has_key(newtok.type):
                        raise LexError, (
                         "%s:%d: Rule '%s' returned an unknown token type '%s'" % (
                          func.func_code.co_filename, func.func_code.co_firstlineno,
                          func.__name__, newtok.type), lexdata[lexpos:])
                return newtok
            if self.lexerrorf:
                tok = LexToken()
                tok.value = self.lexdata[lexpos:]
                tok.lineno = self.lineno
                tok.type = 'error'
                tok.lexer = self
                oldpos = lexpos
                newtok = self.lexerrorf(tok)
                lexpos += getattr(tok, '_skipn', 0)
                if oldpos == lexpos:
                    self.lexpos = lexpos
                    raise LexError, ("Scanning error. Illegal character '%s'" % lexdata[lexpos], lexdata[lexpos:])
                if not newtok:
                    continue
                self.lexpos = lexpos
                return newtok
            self.lexpos = lexpos
            raise LexError, ('No match found', lexdata[lexpos:])

        self.lexpos = lexpos + 1
        return


def validate_file(filename):
    import os.path
    (base, ext) = os.path.splitext(filename)
    if ext != '.py':
        return 1
    try:
        f = open(filename)
        lines = f.readlines()
        f.close()
    except IOError:
        return 1
    else:
        fre = re.compile('\\s*def\\s+(t_[a-zA-Z_0-9]*)\\(')
        sre = re.compile('\\s*(t_[a-zA-Z_0-9]*)\\s*=')
        counthash = {}
        linen = 1
        noerror = 1
        for l in lines:
            m = fre.match(l)
            if not m:
                m = sre.match(l)
            if m:
                name = m.group(1)
                prev = counthash.get(name)
                if not prev:
                    counthash[name] = linen
                else:
                    print '%s:%d: Rule %s redefined. Previously defined on line %d' % (filename, linen, name, prev)
                    noerror = 0
            linen += 1

    return noerror


def _read_lextab(lexer, fdict, module):
    exec 'import %s as lextab' % module
    lexer.lexre = re.compile(lextab._lexre, re.VERBOSE)
    lexer.lexindexfunc = lextab._lextab
    for i in range(len(lextab._lextab)):
        t = lexer.lexindexfunc[i]
        if t:
            if t[0]:
                lexer.lexindexfunc[i] = (
                 fdict[t[0]], t[1])

    lexer.lextokens = lextab._lextokens
    lexer.lexignore = lextab._lexignore
    if lextab._lexerrorf:
        lexer.lexerrorf = fdict[lextab._lexerrorf]


def lex(module=None, debug=0, optimize=0, lextab='lextab'):
    global input
    global token
    ldict = None
    regex = ''
    error = 0
    files = {}
    lexer = Lexer()
    lexer.debug = debug
    lexer.optimize = optimize
    if module:
        if not isinstance(module, types.ModuleType):
            raise ValueError, 'Expected a module'
        ldict = module.__dict__
    else:
        try:
            raise RuntimeError
        except RuntimeError:
            (e, b, t) = sys.exc_info()
            f = t.tb_frame
            f = f.f_back
            ldict = f.f_globals

        if optimize and lextab:
            try:
                _read_lextab(lexer, ldict, lextab)
                if not lexer.lexignore:
                    lexer.lexignore = ''
                token = lexer.token
                input = lexer.input
                return lexer
            except ImportError:
                pass

        tokens = ldict.get('tokens', None)
        if not tokens:
            raise SyntaxError, "lex: module does not define 'tokens'"
        if not (isinstance(tokens, types.ListType) or isinstance(tokens, types.TupleType)):
            raise SyntaxError, 'lex: tokens must be a list or tuple.'
        lexer.lextokens = {}
    if not optimize:

        def is_identifier(s):
            for c in s:
                if not (c.isalnum() or c == '_'):
                    return 0

            return 1

        for n in tokens:
            if not is_identifier(n):
                print "lex: Bad token name '%s'" % n
                error = 1
            if lexer.lextokens.has_key(n):
                print "lex: Warning. Token '%s' multiply defined." % n
            lexer.lextokens[n] = None

    else:
        for n in tokens:
            lexer.lextokens[n] = None

        if debug:
            print "lex: tokens = '%s'" % lexer.lextokens.keys()
        tsymbols = [ f for f in ldict.keys() if f[:2] == 't_' ]
        fsymbols = []
        ssymbols = []
        for f in tsymbols:
            if isinstance(ldict[f], types.FunctionType):
                fsymbols.append(ldict[f])
            elif isinstance(ldict[f], types.StringType):
                ssymbols.append((f, ldict[f]))
            else:
                print 'lex: %s not defined as a function or string' % f
                error = 1

        fsymbols.sort(lambda x, y: cmp(x.func_code.co_firstlineno, y.func_code.co_firstlineno))
        ssymbols.sort(lambda x, y: (len(x[1]) < len(y[1])) - (len(x[1]) > len(y[1])))
        if len(fsymbols) == 0 and len(ssymbols) == 0:
            raise SyntaxError, 'lex: no rules of the form t_rulename are defined.'
        for f in fsymbols:
            line = f.func_code.co_firstlineno
            file = f.func_code.co_filename
            files[file] = None
            if not optimize:
                if f.func_code.co_argcount > 1:
                    print "%s:%d: Rule '%s' has too many arguments." % (file, line, f.__name__)
                    error = 1
                    continue
                if f.func_code.co_argcount < 1:
                    print "%s:%d: Rule '%s' requires an argument." % (file, line, f.__name__)
                    error = 1
                    continue
                if f.__name__ == 't_ignore':
                    print "%s:%d: Rule '%s' must be defined as a string." % (file, line, f.__name__)
                    error = 1
                    continue
            if f.__name__ == 't_error':
                lexer.lexerrorf = f
                continue
            if f.__doc__:
                if not optimize:
                    try:
                        c = re.compile(f.__doc__, re.VERBOSE)
                    except re.error, e:
                        print "%s:%d: Invalid regular expression for rule '%s'. %s" % (file, line, f.__name__, e)
                        error = 1
                        continue
                    else:
                        if debug:
                            print "lex: Adding rule %s -> '%s'" % (f.__name__, f.__doc__)
                if regex:
                    regex += '|'
                regex += '(?P<%s>%s)' % (f.__name__, f.__doc__)
            else:
                print "%s:%d: No regular expression defined for rule '%s'" % (file, line, f.__name__)

        for (name, r) in ssymbols:
            if name == 't_ignore':
                lexer.lexignore = r
                continue
            if not optimize:
                if name == 't_error':
                    raise SyntaxError, "lex: Rule 't_error' must be defined as a function"
                    error = 1
                    continue
                if not lexer.lextokens.has_key(name[2:]):
                    print "lex: Rule '%s' defined for an unspecified token %s." % (name, name[2:])
                    error = 1
                    continue
                try:
                    c = re.compile(r, re.VERBOSE)
                except re.error, e:
                    print "lex: Invalid regular expression for rule '%s'. %s" % (name, e)
                    error = 1
                    continue
                else:
                    if debug:
                        print "lex: Adding rule %s -> '%s'" % (name, r)
            if regex:
                regex += '|'
            regex += '(?P<%s>%s)' % (name, r)

        if not optimize:
            for f in files.keys():
                if not validate_file(f):
                    error = 1

        try:
            if debug:
                print "lex: regex = '%s'" % regex
            lexer.lexre = re.compile(regex, re.VERBOSE)
            lexer.lexindexfunc = [
             None] * (max(lexer.lexre.groupindex.values()) + 1)
            for (f, i) in lexer.lexre.groupindex.items():
                handle = ldict[f]
                if isinstance(handle, types.FunctionType):
                    lexer.lexindexfunc[i] = (
                     handle, handle.__name__[2:])
                else:
                    lexer.lexindexfunc[i] = (None, f[2:])

            if lextab and optimize:
                lt = open(lextab + '.py', 'w')
                lt.write("# %s.py.  This file automatically created by PLY. Don't edit.\n" % lextab)
                lt.write('_lexre = %s\n' % repr(regex))
                lt.write('_lextab = [\n')
                for i in range(0, len(lexer.lexindexfunc)):
                    t = lexer.lexindexfunc[i]
                    if t:
                        if t[0]:
                            lt.write("  ('%s',%s),\n" % (t[0].__name__, repr(t[1])))
                        else:
                            lt.write('  (None,%s),\n' % repr(t[1]))
                    else:
                        lt.write('  None,\n')

                lt.write(']\n')
                lt.write('_lextokens = %s\n' % repr(lexer.lextokens))
                lt.write('_lexignore = %s\n' % repr(lexer.lexignore))
                if lexer.lexerrorf:
                    lt.write('_lexerrorf = %s\n' % repr(lexer.lexerrorf.__name__))
                else:
                    lt.write('_lexerrorf = None\n')
                lt.close()
        except re.error, e:
            print 'lex: Fatal error. Unable to compile regular expression rules. %s' % e
            error = 1

        if error:
            raise SyntaxError, 'lex: Unable to build lexer.'
        if not lexer.lexerrorf:
            print 'lex: Warning. no t_error rule is defined.'
        if not lexer.lexignore:
            lexer.lexignore = ''
        token = lexer.token
        input = lexer.input
        return lexer


def runmain(lexer=None, data=None):
    if not data:
        try:
            filename = sys.argv[1]
            f = open(filename)
            data = f.read()
            f.close()
        except IndexError:
            print 'Reading from standard input (type EOF to end):'
            data = sys.stdin.read()

    if lexer:
        _input = lexer.input
    else:
        _input = input
    _input(data)
    if lexer:
        _token = lexer.token
    else:
        _token = token
    while 1:
        tok = _token()
        if not tok:
            break
        print "(%s,'%s',%d)" % (tok.type, tok.value, tok.lineno)