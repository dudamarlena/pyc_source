# uncompyle6 version 3.7.4
# Python bytecode 3.6 (3379)
# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
# [GCC 8.4.0]
# Embedded file name: build/bdist.linux-x86_64/egg/toil_vg/vg_common.py
# Compiled at: 2020-04-30 13:47:21
# Size of source mod 2**32: 46121 bytes
"""
Shared stuff between different modules in this package.  Some
may eventually move to or be replaced by stuff in toil-lib.
"""
import argparse, sys, os, os.path, random, subprocess, shutil, itertools, glob, json, timeit, errno, fcntl, select, time, threading
from uuid import uuid4
import pkg_resources, tempfile, datetime, logging
from distutils.spawn import find_executable
import collections, socket, uuid, platform, multiprocessing
from concurrent.futures import ThreadPoolExecutor, Future
from toil.common import Toil
from toil.job import Job
from toil.jobStores.fileJobStore import FileJobStore
from toil.realtimeLogger import RealtimeLogger
from toil.lib.docker import dockerCall, dockerCheckOutput, apiDockerCall
from toil_vg.singularity import singularityCall, singularityCheckOutput
from toil_vg.iostore import IOStore
logger = logging.getLogger(__name__)
environment_lock = threading.Lock()

def test_docker():
    """
    Return true if Docker is available on this machine, and False otherwise.
    """
    nowhere = open(os.devnull, 'wb')
    try:
        subprocess.check_call(['docker', 'version'], stdout=nowhere, stderr=nowhere)
        return True
    except:
        return False


def test_singularity():
    """
    Return true if Singularity is available on this machine, and False otherwise.
    """
    nowhere = open(os.devnull, 'wb')
    try:
        subprocess.check_call(['singularity', 'help'], stdout=nowhere, stderr=nowhere)
        return True
    except:
        return False


def add_container_tool_parse_args(parser):
    """ centralize shared container options and their defaults """
    parser.add_argument('--vg_docker', type=str, help='Docker image to use for vg')
    parser.add_argument('--container', default=None, choices=['Docker', 'Singularity', 'None'], help='Container type used for running commands. Use None to  run locally on command line')


def add_common_vg_parse_args(parser):
    """ centralize some shared io functions and their defaults """
    parser.add_argument('--config', default=None, type=str, help='Config file.  Use toil-vg generate-config to see defaults/create new file')
    parser.add_argument('--whole_genome_config', action='store_true', help='Use the default whole-genome config (as generated by toil-vg config --whole_genome)')
    parser.add_argument('--force_outstore', action='store_true', help='use output store instead of toil for all intermediate files (use only for debugging)')
    parser.add_argument('--realTimeStderr', action='store_true', help='print stderr from all commands through the realtime logger')


def get_container_tool_map(options):
    """ convenience function to parse the above _container options into a dictionary """
    cmap = [
     dict(), options.container]
    cmap[0]['vg'] = options.vg_docker
    cmap[0]['bcftools'] = options.bcftools_docker
    cmap[0]['tabix'] = options.tabix_docker
    cmap[0]['bgzip'] = options.tabix_docker
    cmap[0]['jq'] = options.jq_docker
    cmap[0]['rtg'] = options.rtg_docker
    cmap[0]['pigz'] = options.pigz_docker
    cmap[0]['samtools'] = options.samtools_docker
    cmap[0]['bwa'] = options.bwa_docker
    cmap[0]['minimap2'] = options.minimap2_docker
    cmap[0]['Rscript'] = options.r_docker
    cmap[0]['vcfremovesamples'] = options.vcflib_docker
    cmap[0]['freebayes'] = options.freebayes_docker
    cmap[0]['Platypus.py'] = options.platypus_docker
    cmap[0]['hap.py'] = options.happy_docker
    cmap[0]['bedtools'] = options.bedtools_docker
    cmap[0]['R'] = options.sveval_docker
    cmap[0]['bedops'] = options.bedops_docker
    return cmap


def toil_call(job, context, cmd, work_dir, out_path=None, out_append=False):
    """ use to run a one-job toil workflow just to call a command
    using context.runner """
    if out_path:
        open_flag = 'ab' if out_append is True else 'wb'
        with open(os.path.abspath(out_path), open_flag) as (out_file):
            context.runner.call(job, cmd, work_dir=work_dir, outfile=out_file)
    else:
        context.runner.call(job, cmd, work_dir=work_dir)


class ContainerRunner(object):
    __doc__ = ' Helper class to centralize container calling.  So we can toggle both\nDocker and Singularity on and off in just one place.\nto do: Should go somewhere more central '

    def __init__(self, container_tool_map=[{}, None], realtime_stderr=False):
        self.docker_tool_map = container_tool_map[0]
        self.container_support = container_tool_map[1]
        self.realtime_stderr = realtime_stderr

    def container_for_tool(self, name):
        """
        Return Docker, Singularity or None, which is how call() would be run
        on the given tool
        """
        if self.container_support == 'Docker':
            if name in self.docker_tool_map:
                if self.docker_tool_map[name]:
                    if self.docker_tool_map[name].lower() != 'none':
                        return 'Docker'
        if self.container_support == 'Singularity':
            if name in self.docker_tool_map:
                if self.docker_tool_map[name]:
                    if self.docker_tool_map[name].lower() != 'none':
                        return 'Singularity'
        return 'None'

    def call(self, job, args, work_dir='.', outfile=None, errfile=None, check_output=False, tool_name=None):
        """
        
        Run a command. Decide to use a container based on whether the tool
        (either the tool of the first command, or the tool named by tool_name)
        its in the container engine's tool map. Can handle args being either a
        single list of string arguments (starting with the binary name) or a
        list of such lists (in which case a pipeline is run, using no more than
        one container).
        
        Redirects standard output and standard error to the file objects outfile
        and errfile, if specified. If check_output is true, the call will block,
        raise an exception on a nonzero exit status, and return standard
        output's contents.
        
        """
        if outfile is not None:
            assert 'b' in outfile.mode
            if len(args) == 0 or len(args) > 0 and type(args[0]) is not list:
                args = [
                 args]
        else:
            for i in range(len(args)):
                args[i] = [str(x) for x in args[i]]

            name = tool_name if tool_name is not None else args[0][0]
            if self.realtime_stderr and not errfile:
                rfd, wfd = os.pipe()
                rfile = os.fdopen(rfd, 'rb', 0)
                wfile = os.fdopen(wfd, 'wb', 0)
                pid = os.fork()
                if pid == 0:
                    wfile.close()
                    while True:
                        data = rfile.readline()
                        if not data:
                            break
                        RealtimeLogger.info('(stderr) {}'.format(data.strip()))

                    os._exit(0)
                else:
                    assert pid > 0
                    rfile.close()
                    errfile = wfile
        container_type = self.container_for_tool(name)
        if container_type == 'Docker':
            return self.call_with_docker(job, args, work_dir, outfile, errfile, check_output, tool_name)
        else:
            if container_type == 'Singularity':
                return self.call_with_singularity(job, args, work_dir, outfile, errfile, check_output, tool_name)
            return self.call_directly(args, work_dir, outfile, errfile, check_output)

    def call_with_docker(self, job, args, work_dir, outfile, errfile, check_output, tool_name):
        """
        
        Thin wrapper for docker_call that will use internal lookup to
        figure out the location of the docker file.  Only exposes docker_call
        parameters used so far.  expect args as list of lists.  if (toplevel)
        list has size > 1, then piping interface used
        
        Does support redirecting output to outfile, unless check_output is
        used, in which case output is captured.
        
        """
        RealtimeLogger.info(truncate_msg('Docker Run: {}'.format(' | '.join(' '.join(x) for x in args))))
        start_time = timeit.default_timer()
        name = tool_name if tool_name is not None else args[0][0]
        tool = self.docker_tool_map[name]
        environment = {}
        entrypoint = None
        volumes = {}
        working_dir = None
        if name != 'Rscript':
            environment['TMPDIR'] = '.'
        if name == 'Rscript':
            environment['R_LIBS'] = '/tmp'
        if name == 'vg':
            environment['VG_FULL_TRACEBACK'] = '1'
        if tool == 'quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.7_1':
            if args[0][0] == 'Platypus.py':
                args[0][0] = '/usr/local/share/platypus-variant-0.8.1.1-1/Platypus.py'
        environment['LC_ALL'] = 'C'
        if work_dir is not None:
            volumes[os.path.abspath(work_dir)] = {'bind':'/data', 
             'mode':'rw'}
            working_dir = '/data'
        assert outfile is not None and not check_output
        output_fd = None
        use_fifo = platform.system() == 'Linux'
        if use_fifo:
            fifo_dir = tempfile.mkdtemp()
            fifo_host_path = os.path.join(fifo_dir, 'stdout.fifo')
            os.mkfifo(fifo_host_path)
            volumes[fifo_dir] = {'bind':'/control', 
             'mode':'rw'}
            parameters = args + [['dd', 'of=/control/stdout.fifo']]
            output_fd = os.open(fifo_host_path, os.O_RDONLY | os.O_NONBLOCK)
        else:
            listen_sock = socket.socket(socket.AF_INET)
            listen_sock.bind(('', 0))
            listen_sock.listen(1)
            listen_port = listen_sock.getsockname()[1]
            security_cookie = str(uuid.uuid4())
            parameters = args + [
             ['bash', '-c',
              'exec 3<>/dev/tcp/host.docker.internal/{}; cat <(echo {}) - >&3'.format(listen_port, security_cookie)]]
            RealtimeLogger.debug('Listening on port {} for output from Docker container'.format(listen_port))
        RealtimeLogger.debug('Final Docker command: {}'.format(' | '.join(' '.join(x) for x in parameters)))
        container = apiDockerCall(job, tool, parameters, volumes=volumes,
          working_dir=working_dir,
          entrypoint=entrypoint,
          environment=environment,
          detach=True)
        RealtimeLogger.debug('Asked for container {}'.format(container.id))
        if not use_fifo:
            listen_sock.settimeout(10)
            for attempt in range(3):
                connection_sock, remote_address = listen_sock.accept()
                RealtimeLogger.info('Got connection from {}'.format(remote_address))
                connection_sock.settimeout(10)
                received_cookie_and_newline = connection_sock.recv(len(security_cookie) + 1)
                if received_cookie_and_newline != security_cookie + '\n':
                    RealtimeLogger.warning('Received incorect security cookie message from {}'.format(remote_address))
                    continue
                else:
                    connection_sock.setblocking(True)
                    output_fd = connection_sock.fileno()
                    break

            if output_fd is None:
                raise RuntimeError('Could not establish network connection for Docker output!')
        try:
            last_chance = False
            saw_data = False
            while 1:
                if output_fd is not None:
                    can_read, can_write, had_error = select.select([output_fd], [], [output_fd], 10)
                else:
                    if len(can_read) > 0 or len(had_error) > 0:
                        try:
                            data = os.read(output_fd, 4096)
                            if len(data) == 0:
                                RealtimeLogger.debug('Got EOF')
                                break
                        except OSError as err:
                            if err.errno in [errno.EAGAIN, errno.EWOULDBLOCK]:
                                data = None
                            else:
                                raise err

                    else:
                        data = None
                if data:
                    if len(data) > 0:
                        outfile.write(data)
                        saw_data = True
                if not saw_data:
                    if last_chance:
                        RealtimeLogger.warning('Giving up on output form container {}'.format(container.id))
                        break
                    container.reload()
                    if container.status not in ('created', 'restarting', 'running',
                                                'removing'):
                        time.sleep(10)
                        last_chance = True
                        continue

        finally:
            os.close(output_fd)
            if not use_fifo:
                listen_sock.close()

        return_code = container.wait()
        if use_fifo:
            os.unlink(fifo_host_path)
            os.rmdir(fifo_dir)
        else:
            if len(args) == 1:
                parameters = [] if len(args[0]) == 1 else args[0][1:]
                entrypoint = args[0][0]
            else:
                parameters = args
            container = apiDockerCall(job, tool, parameters, volumes=volumes,
              working_dir=working_dir,
              entrypoint=entrypoint,
              environment=environment,
              detach=True)
            return_code = container.wait()
        if return_code != 0:
            command = ' | '.join(' '.join(x) for x in args)
            RealtimeLogger.error('Docker container for command {} failed with code {}'.format(command, return_code))
            RealtimeLogger.error('Dumping stderr...')
            for line in container.logs(stderr=True, stdout=False, stream=True):
                RealtimeLogger.error(line[:-1])

            if not check_output:
                if outfile is None:
                    RealtimeLogger.error('Dumping stdout...')
                    for line in container.logs(stderr=False, stdout=True, stream=True):
                        RealtimeLogger.error(line[:-1])

            raise RuntimeError('Docker container for command {} failed with code {}'.format(command, return_code))
        else:
            if errfile:
                for line in container.logs(stderr=True, stdout=False, stream=True):
                    errfile.write(line)

        if check_output:
            captured_stdout = container.logs(stderr=False, stdout=True)
        end_time = timeit.default_timer()
        run_time = end_time - start_time
        RealtimeLogger.info('Successfully docker ran {} in {} seconds.'.format(' | '.join(' '.join(x) for x in args), run_time))
        if outfile:
            outfile.flush()
            os.fsync(outfile.fileno())
        if check_output is True:
            return captured_stdout

    def call_with_singularity(self, job, args, work_dir, outfile, errfile, check_output, tool_name):
        """ Thin wrapper for singularity_call that will use internal lookup to
        figure out the location of the singularity file.  Only exposes singularity_call
        parameters used so far.  expect args as list of lists.  if (toplevel)
        list has size > 1, then piping interface used """
        global environment_lock
        RealtimeLogger.info(truncate_msg('Singularity Run: {}'.format(' | '.join(' '.join(x) for x in args))))
        start_time = timeit.default_timer()
        name = tool_name if tool_name is not None else args[0][0]
        tool = self.docker_tool_map[name]
        parameters = args[0] if len(args) == 1 else args
        with environment_lock:
            update_env = {'LC_ALL':'C', 
             'VG_FULL_TRACEBACK':'1'}
            if name == 'Rscript':
                update_env['R_LIBS'] = '/tmp'
            old_env = {}
            for env_name, env_val in list(update_env.items()):
                old_env[env_name] = os.environ.get(env_name)
                os.environ[env_name] = env_val

            if check_output is True:
                ret = singularityCheckOutput(job, tool, parameters=parameters, workDir=work_dir)
            else:
                ret = singularityCall(job, tool, parameters=parameters, workDir=work_dir, outfile=outfile)
            for env_name, env_val in list(update_env.items()):
                if old_env[env_name] is not None:
                    os.environ[env_name] = old_env[env_name]
                else:
                    del os.environ[env_name]

        end_time = timeit.default_timer()
        run_time = end_time - start_time
        RealtimeLogger.info('Successfully singularity ran {} in {} seconds.'.format(' | '.join(' '.join(x) for x in args), run_time))
        if outfile:
            outfile.flush()
            os.fsync(outfile.fileno())
        return ret

    def call_directly(self, args, work_dir, outfile, errfile, check_output):
        """ Just run the command without docker """
        RealtimeLogger.info(truncate_msg('Run: {}'.format(' | '.join(' '.join(x) for x in args))))
        start_time = timeit.default_timer()
        with environment_lock:
            my_env = os.environ.copy()
        my_env['TMPDIR'] = '{}'.format(os.getcwd())
        my_env['LC_ALL'] = 'C'
        my_env['VG_FULL_TRACEBACK'] = '1'
        procs = []
        for i in range(len(args)):
            stdin = procs[(i - 1)].stdout if i > 0 else None
            if i == len(args) - 1:
                if outfile is not None:
                    stdout = outfile
            else:
                stdout = subprocess.PIPE
            try:
                procs.append(subprocess.Popen((args[i]), stdout=stdout, stderr=errfile, stdin=stdin,
                  cwd=work_dir,
                  env=my_env))
            except OSError as e:
                if e.errno in (2, 13):
                    if not find_executable(args[i][0]):
                        raise RuntimeError('Command not found: {}'.format(args[i][0]))
                else:
                    raise e

        for p in procs[:-1]:
            p.stdout.close()

        output, errors = procs[(-1)].communicate()
        for i, proc in enumerate(procs):
            sts = proc.wait()
            if sts != 0:
                raise Exception('Command {} returned with non-zero exit status {}'.format(' '.join(args[i]), sts))

        end_time = timeit.default_timer()
        run_time = end_time - start_time
        RealtimeLogger.info('Successfully ran {} in {} seconds.'.format(' | '.join(' '.join(x) for x in args), run_time))
        if outfile:
            outfile.flush()
            os.fsync(outfile.fileno())
        if check_output:
            return output


def get_vg_script(job, runner, script_name, work_dir):
    """
    getting the path to a script in vg/scripts is different depending on if we're
    in docker or not.  wrap logic up here, where we get the script from wherever it
    is then put it in the work_dir
    """
    vg_container_type = runner.container_for_tool('vg')
    if vg_container_type != 'None':
        cmd = ['cp', os.path.join('/vg', 'scripts', script_name), '.']
        runner.call(job, cmd, work_dir=work_dir, tool_name='vg')
    else:
        scripts_path = os.path.join(os.path.dirname(find_executable('vg')), '..', 'scripts')
        shutil.copy2(os.path.join(scripts_path, script_name), os.path.join(work_dir, script_name))
    return os.path.join(work_dir, script_name)


def set_r_cran_url(script_path):
    """
    our R docker image gets its packages from https://mran.microsoft.com/snapshot/... which
    seems to be less than reliable.  this is a hack to specify cran.r-project.org instead
    (https://stackoverflow.com/questions/11488174/how-to-select-a-cran-mirror-in-r)
    """
    with open(script_path) as (script_file):
        lines = [line for line in script_file]
    fixed = False
    with open(script_path, 'w') as (script_file):
        for line in lines:
            script_file.write(line)
            if line.strip().startswith('#!') and not fixed:
                script_file.write('\n# Default repo\nlocal({r <- getOption("repos")\nr["CRAN"] <- "http://cran.r-project.org"\noptions(repos=r)\n})\n')
                fixed = True


def get_files_by_file_size(dirname, reverse=False):
    """ Return list of file paths in directory sorted by file size """
    filepaths = []
    for basename in os.listdir(dirname):
        filename = os.path.join(dirname, basename)
        if os.path.isfile(filename):
            filepaths.append(filename)

    for i in range(len(filepaths)):
        filepaths[i] = (
         filepaths[i], os.path.getsize(filepaths[i]))

    return filepaths


def make_url(path):
    """ Turn filenames into URLs, whileleaving existing URLs alone """
    if ':' not in path:
        return 'file://' + os.path.abspath(path)
    else:
        return path


def require(expression, message):
    if not expression:
        raise Exception('\n\n' + message + '\n\n')


def parse_id_ranges(job, id_ranges_file_id):
    """Returns list of triples chrom, start, end
    """
    work_dir = job.fileStore.getLocalTempDir()
    id_range_file = os.path.join(work_dir, 'id_ranges.tsv')
    job.fileStore.readGlobalFile(id_ranges_file_id, id_range_file)
    return parse_id_ranges_file(id_range_file)


def parse_id_ranges_file(id_ranges_filename):
    """Returns list of triples chrom, start, end
    """
    id_ranges = []
    with open(id_ranges_filename) as (f):
        for line in f:
            toks = line.split()
            if len(toks) == 3:
                id_ranges.append((toks[0], int(toks[1]), int(toks[2])))

    return id_ranges


def remove_ext(string, ext=None):
    """
    Strip a suffix from a string. Case insensitive.
    If no suffix given, strips the last . and everything after (like file extension)
    """
    if ext is None:
        if string.rfind('.') >= 0:
            ext = string[string.rfind('.'):]
        else:
            ext = ''
    if string.lower().endswith(ext.lower()):
        return string[:-len(ext)]
    else:
        return string


def truncate_msg(msg, max_len=2000):
    """
    Truncate a message string so it doesn't get lost (todo: automatically do this in realtime logger class)
    """
    if len(msg) <= max_len:
        return msg
    else:
        trunc_info = '<log message truncated to fit buffer>'
        assert len(trunc_info) < max_len
        return msg[:max_len - len(trunc_info)] + trunc_info


class TimeTracker:
    __doc__ = ' helper dictionary to keep tabs on several named runtimes. '

    def __init__(self, name=None):
        """ create. optionally start a timer"""
        self.times = collections.defaultdict(float)
        self.running = {}
        if name:
            self.start(name)

    def start(self, name):
        """ start a timer """
        assert name not in self.running
        self.running[name] = timeit.default_timer()

    def stop(self, name=None):
        """ stop a timer. if no name, do all running """
        names = [name] if name else list(self.running.keys())
        ti = timeit.default_timer()
        for name in names:
            self.times[name] += ti - self.running[name]
            del self.running[name]

    def add(self, time_dict):
        """ add in all times from another TimeTracker """
        for key, value in list(time_dict.times.items()):
            self.times[key] += value

    def total(self, names=None):
        if not names:
            names = list(self.times.keys())
        return sum([self.times[name] for name in names])

    def names(self):
        return list(self.times.keys())


def run_concat_lists(job, *args):
    """
    Toil job to join all the given lists and return the merged list.
    """
    concat = []
    for input_list in args:
        concat += input_list

    return concat


def parse_plot_set(plot_set_string):
    """
    
    Given one of the string arguments to the --plot-sets option, parse out a
    data structure representing which conditions ought to be compared against
    each other, and what those comparison plots/tables should be called.

    The syntax of a plot set is [title:]condition[,condition[,condition...]].
    
    The first condition is the comparison baseline, when applicable.
    
    Returns a tuple of a plot set title, or None if unspecified, and a list of
    condition names.
    
    """
    colon_pos = plot_set_string.find(':')
    if colon_pos != -1:
        title = plot_set_string[0:colon_pos]
        plot_set_string = plot_set_string[colon_pos + 1:]
    else:
        title = None
    return (
     title, plot_set_string.split(','))


def parse_plot_sets(plot_sets_list):
    """
    
    Given a list of plot set strings, parses each with parse_plot_set.
    
    Returns a list of tuples. Each tuple is a plot set title, or None if no
    title is to be applied, and a list of condition names, or None if all
    conditions are to be included.
    
    If no plot sets are specified in the list, returns a single plot set for
    all conditions.
    
    """
    plot_sets = [parse_plot_set(spec) for spec in plot_sets_list]
    if len(plot_sets) == 0:
        plot_sets = [
         (None, None)]
    return plot_sets


def title_to_filename(kind, i, title, extension):
    """
    Given the kind of thing you want to save ('table', 'plot-qq', etc.), the
    number of the thign out of all things of that type, the human-readable
    title ('All Conditions vs. Whatever'), and an extansion, come up with a
    safe filename to save the plot under.
    
    The title may be None, in which case it is ommitted.
    
    The extension may be None, in which case it is omitted.
    """
    if title is not None:
        safe_title = ''.join(c for c in title if c.isalnum())
    else:
        safe_title = None
    part_list = [
     kind]
    if i != 0:
        part_list.append('-{:02d}'.format(i))
    if title is not None:
        safe_title = ''.join(c for c in title if c.isalnum())
        part_list.append('-{}'.format(safe_title))
    if extension is not None:
        part_list.append('.{}'.format(extension))
    return ''.join(part_list)


def ensure_disk(job, job_fn, job_fn_args, job_fn_kwargs, file_id_list, factor=8, padding=1073741824):
    """
    Ensure that the currently running job has enough disk to load all the given
    file IDs (passed as any number of lists of file IDs), and process them,
    producing factor times as much data, plus padding.
    
    Takes the job, the function that is the job, the list of arguments passed
    in (except the job object), the dict of keyword args passed in, and then
    a file ID list or iterable.
    
    If there is not enough disk, re-queues the job with more disk, and returns
    the promise for the result.
    
    If there is enough disk, returns None
    
    TODO: Convert to promised requirements if it is sufficiently expressive.
    """
    required_disk = 0
    for file_id in file_id_list:
        required_disk += file_id.size

    required_disk *= factor
    required_disk += padding
    if job.disk < required_disk:
        RealtimeLogger.info('Re-queueing job with {} bytes of disk; originally had {}'.format(required_disk, job.disk))
        requeued = (job.addChildJobFn)(job_fn, *job_fn_args, cores=job.cores, memory=job.memory, disk=required_disk, **job_fn_kwargs)
        return requeued.rv()
    else:
        return


def run_concat_files(job, context, file_ids, dest_name=None, header=None):
    """
    Utility job to concatenate some files. Returns the concatenated file ID.
    If given a dest_name, writes the result to the out store with the given name.
    (We wanted to use name, but that kwarg is reserved by Toil.)
    If given a header, prepends the header to the file with a trailing newline.
    """
    requeue_promise = ensure_disk(job, run_concat_files, [context, file_ids], {'dest_name':dest_name, 
     'header':header},
      file_ids, factor=2)
    if requeue_promise is not None:
        return requeue_promise
    else:
        work_dir = job.fileStore.getLocalTempDir()
        out_name = os.path.join(work_dir, 'output.dat' if dest_name is None else dest_name)
        with open(out_name, 'wb') as (out_file):
            if header is not None:
                out_file.write('{}\n'.format(header).encode())
            for file_id in file_ids:
                with job.fileStore.readGlobalFileStream(file_id) as (in_file):
                    shutil.copyfileobj(in_file, out_file)

        if dest_name is None:
            RealtimeLogger.info('Concatenated {} files into intermediate file {}'.format(len(file_ids), out_name))
            return context.write_intermediate_file(job, out_name)
        RealtimeLogger.info('Concatenated {} files into output file {} -> {}'.format(len(file_ids), out_name, dest_name))
        return context.write_output_file(job, out_name, dest_name)


class AsyncImporter(object):
    __doc__ = ' \n    Importing big files is a bottleneck.  We can improve things somewhat by using threads\n    '

    def __init__(self, toil, max_threads=multiprocessing.cpu_count(), retry_count=3):
        self.toil = toil
        self.threads = max_threads
        self.retry_count = retry_count
        if not isinstance(self.toil._jobStore, FileJobStore):
            self.threads = 1
        self.executor = ThreadPoolExecutor(max_workers=(self.threads))
        self.start_time = timeit.default_timer()
        self.count = 0
        logger.info('Importing input files into Toil')

    def load(self, file_path, wait_on=None):
        """ 
        Do a toil import asynchronously.  vg construct will actually fail if the tbi is 
        imported after the vcf.gz, so the wait_on option is used, for example,
        to make sure indexes get imported after the file they index 
        """
        self.count += 1

        def wait_import():
            if wait_on:
                wait_on.result()
            for i in range(self.retry_count):
                try:
                    return self.toil.importFile(file_path)
                except:
                    if i >= self.retry_count - 1:
                        raise
                    else:
                        time.sleep(i)

        return self.executor.submit(wait_import)

    def wait(self):
        """ 
        Wait until everything's finished running.  Doesn't do much, in effect, beyond
        the log message...
        """
        self.executor.shutdown(wait=True)
        end_time = timeit.default_timer()
        logger.info('Imported {} input files into Toil in {} seconds'.format(self.count, end_time - self.start_time))

    def resolve(self, result):
        """ 
        Transform our promises to values.
        Supports lists, tuples, dicts and Namespaces and some nested combos thereof
        """
        if result is None:
            return
        else:
            if isinstance(result, (list, tuple)):
                return [self.resolve(x) for x in result]
            else:
                if isinstance(result, dict):
                    return dict([(k, self.resolve(v)) for k, v in list(result.items())])
                if isinstance(result, argparse.Namespace):
                    result.__dict__ = self.resolve(result.__dict__)
                    return result
                if isinstance(result, Future):
                    return result.result()
            return result