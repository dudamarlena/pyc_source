# uncompyle6 version 3.7.4
# Python bytecode 3.8 (3413)
# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
# [GCC 8.4.0]
# Embedded file name: build/bdist.macosx-10.9-x86_64/egg/lsqfitgp/__init__.py
# Compiled at: 2020-04-26 19:03:15
# Size of source mod 2**32: 3598 bytes
from autograd import numpy
from . import _gvar_autograd
from ._GP import *
from ._Kernel import *
from ._kernels import *
from ._array import *
from ._fit import *
from ._Deriv import *
__version__ = '0.0.2'
__doc__ = '\n\nModule to fit gaussian processes with gvar/lsqfit. It can both be used\nstandalone to fit data with a gaussian process only, and with lsqfit inside a\npossibly nonlinear model with other parameters. In lsqfit style, all the\nresults will be properly correlated with prior, data, and other non-gaussian\nprocess parameters in the fit, even when doing conditional prediction.\n\nThe main class is `GP`, which represents a gaussian process over arbitrary\ninput. It can be used both autonomously and with lsqfit. The inputs/outputs can\nbe arrays or dictionaries of arrays. It supports doing inference with the\nderivatives of the process, using `autograd` to compute automatically\nderivatives of the kernels. Indirectly, this can be used to make inference with\nintegrals.\n\nFunctions and classes\n---------------------\n    \n    GP : class\n        Class of objects representing a gaussian process.\n    kernel : decorator\n        Decorator for marking a function as a kernel.\n    isotropickernel : decorator\n        Decorator for marking a function as an isotropic kernel.\n    empbayes_fit : function\n        Fit the hyperpriors of a gaussian process.\n    StructuredArray : class\n        Autograd-friendly wrapper of numpy structured arrays.\n    where : function\n        Make a kernel that switches between two kernels based on a condition.\n\nKernels\n-------\n\nThe covariance kernels are represented by subclasses of class `Kernel`. There\'s\nalso `IsotropicKernel` for covariance functions that depend only on the\ndistance between the arguments. Kernel objects can be summed, multiplied and\nraised to a power.\n\nTo make a custom kernel, you can instantiate one of the two general classes by\npassing them a function, or subclass them. For convenience, decorators are\nprovided to convert a function to a covariance kernel. Otherwise, use one of\nthe already available subclasses listed below. Isotropic kernels are normalized\nto have unit variance and roughly unit lengthscale.\n\n    Constant :\n        Equivalent to fitting with a constant.\n    Linear :\n        Equivalent to fitting with a line.\n    Polynomial :\n        Equivalent to fitting with a polynomial.\n    ExpQuad :\n        Gaussian kernel.\n    White :\n        White noise, each point is indipendent.\n    Matern :\n        Matérn kernel, you can set how many times it is differentiable.\n    Matern12, Matern32, Matern52 :\n        Matérn kernel for the specific cases nu = 1/2, 3/2, 5/2.\n    GammaExp :\n        Gamma exponential. Not differentiable, but you can set how close it is\n        to being differentiable.\n    RatQuad :\n        Equivalent to a mixture of gaussian kernels with gamma-distributed\n        length scales.\n    NNKernel :\n        Equivalent to training a neural network with one latent infinite layer.\n    Wiener :\n        Random walk.\n    Gibbs :\n        A gaussian kernel with a custom variable length scale.\n    Periodic :\n        A periodic gaussian kernel, represents a periodic function.\n    Categorical :\n        Arbitrary covariance matrix over a finite set of values.\n    Cos :\n        A cosine.\n    FracBrownian :\n        Fractional Brownian motion, like Wiener but with correlations.\n    PPKernel :\n        Finite support isotropic kernel.\n    Rescaling :\n        Kernel used to change the variance of other kernels.\n\nReference: Rasmussen et al. (2006), "Gaussian Processes for Machine Learning".\n\n'