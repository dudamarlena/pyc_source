# uncompyle6 version 3.7.4
# Python bytecode 2.7 (62211)
# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
# [GCC 8.4.0]
# Embedded file name: build/bdist.linux-x86_64/egg/toil_vg/vg_common.py
# Compiled at: 2018-11-03 15:09:40
"""
Shared stuff between different modules in this package.  Some
may eventually move to or be replaced by stuff in toil-lib.
"""
from __future__ import print_function
import argparse, sys, os, os.path, random, subprocess, shutil, itertools, glob, json, timeit, errno, fcntl, select, time, threading
from uuid import uuid4
import pkg_resources, tempfile, datetime, logging
from distutils.spawn import find_executable
import collections
from toil.common import Toil
from toil.job import Job
from toil.realtimeLogger import RealtimeLogger
from toil.lib.docker import dockerCall, dockerCheckOutput, apiDockerCall
from toil_vg.singularity import singularityCall, singularityCheckOutput
from toil_vg.iostore import IOStore
logger = logging.getLogger(__name__)
environment_lock = threading.Lock()

def test_docker():
    """
    Return true if Docker is available on this machine, and False otherwise.
    """
    nowhere = open(os.devnull, 'wb')
    try:
        subprocess.check_call(['docker', 'version'], stdout=nowhere, stderr=nowhere)
        return True
    except:
        return False


def add_container_tool_parse_args(parser):
    """ centralize shared container options and their defaults """
    parser.add_argument('--vg_docker', type=str, help='Docker image to use for vg')
    parser.add_argument('--container', default=None, choices=['Docker', 'Singularity', 'None'], help='Container type used for running commands. Use None to  run locally on command line')
    return


def add_common_vg_parse_args(parser):
    """ centralize some shared io functions and their defaults """
    parser.add_argument('--config', default=None, type=str, help='Config file.  Use toil-vg generate-config to see defaults/create new file')
    parser.add_argument('--whole_genome_config', action='store_true', help='Use the default whole-genome config (as generated by toil-vg config --whole_genome)')
    parser.add_argument('--force_outstore', action='store_true', help='use output store instead of toil for all intermediate files (use only for debugging)')
    parser.add_argument('--realTimeStderr', action='store_true', help='print stderr from all commands through the realtime logger')
    return


def get_container_tool_map(options):
    """ convenience function to parse the above _container options into a dictionary """
    cmap = [
     dict(), options.container]
    cmap[0]['vg'] = options.vg_docker
    cmap[0]['bcftools'] = options.bcftools_docker
    cmap[0]['tabix'] = options.tabix_docker
    cmap[0]['bgzip'] = options.tabix_docker
    cmap[0]['jq'] = options.jq_docker
    cmap[0]['rtg'] = options.rtg_docker
    cmap[0]['pigz'] = options.pigz_docker
    cmap[0]['samtools'] = options.samtools_docker
    cmap[0]['bwa'] = options.bwa_docker
    cmap[0]['Rscript'] = options.r_docker
    cmap[0]['vcfremovesamples'] = options.vcflib_docker
    cmap[0]['freebayes'] = options.freebayes_docker
    cmap[0]['Platypus.py'] = options.platypus_docker
    cmap[0]['hap.py'] = options.happy_docker
    cmap[0]['bedtools'] = options.bedtools_docker
    cmap[0]['bedops'] = options.bedops_docker
    return cmap


def toil_call(job, context, cmd, work_dir, out_path=None, out_append=False):
    """ use to run a one-job toil workflow just to call a command
    using context.runner """
    if out_path:
        open_flag = 'a' if out_append is True else 'w'
        with open(os.path.abspath(out_path), open_flag) as (out_file):
            context.runner.call(job, cmd, work_dir=work_dir, outfile=out_file)
    else:
        context.runner.call(job, cmd, work_dir=work_dir)


class ContainerRunner(object):
    """ Helper class to centralize container calling.  So we can toggle both
Docker and Singularity on and off in just one place.
to do: Should go somewhere more central """

    def __init__(self, container_tool_map=[{}, None], realtime_stderr=False):
        self.docker_tool_map = container_tool_map[0]
        self.container_support = container_tool_map[1]
        self.realtime_stderr = realtime_stderr

    def container_for_tool(self, name):
        """
        Return Docker, Singularity or None, which is how call() would be run
        on the given tool
        """
        if self.container_support == 'Docker' and name in self.docker_tool_map and self.docker_tool_map[name] and self.docker_tool_map[name].lower() != 'none':
            return 'Docker'
        else:
            if self.container_support == 'Singularity' and name in self.docker_tool_map and self.docker_tool_map[name] and self.docker_tool_map[name].lower() != 'none':
                return 'Singularity'
            return 'None'

    def call(self, job, args, work_dir='.', outfile=None, errfile=None, check_output=False, tool_name=None):
        """
        
        Run a command. Decide to use a container based on whether the tool
        (either the tool of the first command, or the tool named by tool_name)
        its in the container engine's tool map. Can handle args being either a
        single list of string arguments (starting with the binary name) or a
        list of such lists (in which case a pipeline is run, using no more than
        one container).
        
        Redirects standard output and standard error to the file objects outfile
        and errfile, if specified. If check_output is true, the call will block,
        raise an exception on a nonzero exit status, and return standard
        output's contents.
        
        """
        if len(args) == 0 or len(args) > 0 and type(args[0]) is not list:
            args = [
             args]
        for i in range(len(args)):
            args[i] = [ str(x) for x in args[i] ]

        name = tool_name if tool_name is not None else args[0][0]
        if self.realtime_stderr and not errfile:
            rfd, wfd = os.pipe()
            rfile = os.fdopen(rfd, 'r', 0)
            wfile = os.fdopen(wfd, 'w', 0)
            pid = os.fork()
            if pid == 0:
                wfile.close()
                while 1:
                    data = rfile.readline()
                    if not data:
                        break
                    RealtimeLogger.info(('(stderr) {}').format(data.strip()))

                os._exit(0)
            else:
                assert pid > 0
                rfile.close()
                errfile = wfile
        container_type = self.container_for_tool(name)
        if container_type == 'Docker':
            return self.call_with_docker(job, args, work_dir, outfile, errfile, check_output, tool_name)
        else:
            if container_type == 'Singularity':
                return self.call_with_singularity(job, args, work_dir, outfile, errfile, check_output, tool_name)
            else:
                return self.call_directly(args, work_dir, outfile, errfile, check_output)

            return

    def call_with_docker(self, job, args, work_dir, outfile, errfile, check_output, tool_name):
        """
        
        Thin wrapper for docker_call that will use internal lookup to
        figure out the location of the docker file.  Only exposes docker_call
        parameters used so far.  expect args as list of lists.  if (toplevel)
        list has size > 1, then piping interface used
        
        Does support redirecting output to outfile, unless check_output is
        used, in which case output is captured.
        
        """
        RealtimeLogger.info(('Docker Run: {}').format((' | ').join((' ').join(x) for x in args)))
        start_time = timeit.default_timer()
        name = tool_name if tool_name is not None else args[0][0]
        tool = self.docker_tool_map[name]
        environment = {}
        entrypoint = None
        volumes = {}
        working_dir = None
        if name != 'Rscript':
            environment['TMPDIR'] = '.'
        if name == 'Rscript':
            environment['R_LIBS'] = '/tmp'
        if name == 'vg':
            environment['VG_FULL_TRACEBACK'] = '1'
        if tool == 'quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.7_1' and args[0][0] == 'Platypus.py':
            args[0][0] = '/usr/local/share/platypus-variant-0.8.1.1-1/Platypus.py'
        environment['LC_ALL'] = 'C'
        if work_dir is not None:
            volumes[os.path.abspath(work_dir)] = {'bind': '/data', 'mode': 'rw'}
            working_dir = '/data'
        if outfile is not None:
            assert not check_output
            fifo_dir = tempfile.mkdtemp()
            fifo_host_path = os.path.join(fifo_dir, 'stdout.fifo')
            os.mkfifo(fifo_host_path)
            volumes[fifo_dir] = {'bind': '/control', 'mode': 'rw'}
            parameters = args + [['dd', 'of=/control/stdout.fifo']]
            container = apiDockerCall(job, tool, parameters, volumes=volumes, working_dir=working_dir, entrypoint=entrypoint, environment=environment, detach=True)
            RealtimeLogger.debug(('Asked for container {}').format(container.id))
            fifo_fd = os.open(fifo_host_path, os.O_RDONLY | os.O_NONBLOCK)
            try:
                last_chance = False
                saw_data = False
                while True:
                    can_read, can_write, had_error = select.select([fifo_fd], [], [fifo_fd], 10)
                    if len(can_read) > 0 or len(had_error) > 0:
                        try:
                            data = os.read(fifo_fd, 4096)
                            if data == '':
                                RealtimeLogger.debug('Got EOF')
                                break
                        except OSError as err:
                            if err.errno in [errno.EAGAIN, errno.EWOULDBLOCK]:
                                data = None
                            else:
                                raise err

                    else:
                        data = None
                    if data is not None:
                        outfile.write(data)
                        saw_data = True
                    elif not saw_data:
                        if last_chance:
                            RealtimeLogger.warning(('Giving up on output form container {}').format(container.id))
                            break
                        container.reload()
                        if container.status not in ('created', 'restarting', 'running',
                                                    'removing'):
                            time.sleep(10)
                            last_chance = True
                            continue

            finally:
                os.close(fifo_fd)

            return_code = container.wait()
            os.unlink(fifo_host_path)
            os.rmdir(fifo_dir)
        else:
            if len(args) == 1:
                parameters = [] if len(args[0]) == 1 else args[0][1:]
                entrypoint = args[0][0]
            else:
                parameters = args
            container = apiDockerCall(job, tool, parameters, volumes=volumes, working_dir=working_dir, entrypoint=entrypoint, environment=environment, detach=True)
            return_code = container.wait()
        if return_code != 0:
            command = (' | ').join((' ').join(x) for x in args)
            RealtimeLogger.error(('Docker container for command {} failed with code {}').format(command, return_code))
            RealtimeLogger.error('Dumping stderr...')
            for line in container.logs(stderr=True, stdout=False, stream=True):
                RealtimeLogger.error(line[:-1])

            if not check_output and outfile is None:
                RealtimeLogger.error('Dumping stdout...')
                for line in container.logs(stderr=False, stdout=True, stream=True):
                    RealtimeLogger.error(line[:-1])

            raise RuntimeError(('Docker container for command {} failed with code {}').format(command, return_code))
        elif errfile:
            for line in container.logs(stderr=True, stdout=False, stream=True):
                errfile.write(line)

        if check_output:
            captured_stdout = container.logs(stderr=False, stdout=True)
        end_time = timeit.default_timer()
        run_time = end_time - start_time
        RealtimeLogger.info(('Successfully docker ran {} in {} seconds.').format((' | ').join((' ').join(x) for x in args), run_time))
        if outfile:
            outfile.flush()
            os.fsync(outfile.fileno())
        if check_output is True:
            return captured_stdout
        else:
            return

    def call_with_singularity(self, job, args, work_dir, outfile, errfile, check_output, tool_name):
        """ Thin wrapper for singularity_call that will use internal lookup to
        figure out the location of the singularity file.  Only exposes singularity_call
        parameters used so far.  expect args as list of lists.  if (toplevel)
        list has size > 1, then piping interface used """
        global environment_lock
        RealtimeLogger.info(('Singularity Run: {}').format((' | ').join((' ').join(x) for x in args)))
        start_time = timeit.default_timer()
        name = tool_name if tool_name is not None else args[0][0]
        tool = self.docker_tool_map[name]
        parameters = args[0] if len(args) == 1 else args
        with environment_lock:
            update_env = {'LC_ALL': 'C', 'VG_FULL_TRACEBACK': '1', 'TMPDIR': '.'}
            old_env = {}
            for env_name, env_val in update_env.items():
                old_env[env_name] = os.environ.get(env_name)
                os.environ[env_name] = env_val

            if check_output is True:
                ret = singularityCheckOutput(job, tool, parameters=parameters, workDir=work_dir)
            else:
                ret = singularityCall(job, tool, parameters=parameters, workDir=work_dir, outfile=outfile)
            for env_name, env_val in update_env.items():
                if old_env[env_name] is not None:
                    os.environ[env_name] = old_env[env_name]
                else:
                    del os.environ[env_name]

        end_time = timeit.default_timer()
        run_time = end_time - start_time
        RealtimeLogger.info(('Successfully singularity ran {} in {} seconds.').format((' | ').join((' ').join(x) for x in args), run_time))
        if outfile:
            outfile.flush()
            os.fsync(outfile.fileno())
        return ret

    def call_directly(self, args, work_dir, outfile, errfile, check_output):
        """ Just run the command without docker """
        RealtimeLogger.info(('Run: {}').format((' | ').join((' ').join(x) for x in args)))
        start_time = timeit.default_timer()
        with environment_lock:
            my_env = os.environ.copy()
        my_env['TMPDIR'] = '.'
        my_env['LC_ALL'] = 'C'
        my_env['VG_FULL_TRACEBACK'] = '1'
        procs = []
        for i in range(len(args)):
            stdin = procs[(i - 1)].stdout if i > 0 else None
            if i == len(args) - 1 and outfile is not None:
                stdout = outfile
            else:
                stdout = subprocess.PIPE
            try:
                procs.append(subprocess.Popen(args[i], stdout=stdout, stderr=errfile, stdin=stdin, cwd=work_dir, env=my_env))
            except OSError as e:
                if e.errno in (2, 13) and not find_executable(args[i][0]):
                    raise RuntimeError(('Command not found: {}').format(args[i][0]))
                else:
                    raise e

        for p in procs[:-1]:
            p.stdout.close()

        output, errors = procs[(-1)].communicate()
        for i, proc in enumerate(procs):
            sts = proc.wait()
            if sts != 0:
                raise Exception(('Command {} returned with non-zero exit status {}').format((' ').join(args[i]), sts))

        end_time = timeit.default_timer()
        run_time = end_time - start_time
        RealtimeLogger.info(('Successfully ran {} in {} seconds.').format((' | ').join((' ').join(x) for x in args), run_time))
        if outfile:
            outfile.flush()
            os.fsync(outfile.fileno())
        if check_output:
            return output
        else:
            return


def get_vg_script(job, runner, script_name, work_dir):
    """
    getting the path to a script in vg/scripts is different depending on if we're
    in docker or not.  wrap logic up here, where we get the script from wherever it
    is then put it in the work_dir
    """
    vg_container_type = runner.container_for_tool('vg')
    if vg_container_type != 'None':
        cmd = ['cp', os.path.join('/vg', 'scripts', script_name), '.']
        runner.call(job, cmd, work_dir=work_dir, tool_name='vg')
    else:
        scripts_path = os.path.join(os.path.dirname(find_executable('vg')), '..', 'scripts')
        shutil.copy2(os.path.join(scripts_path, script_name), os.path.join(work_dir, script_name))
    return os.path.join(work_dir, script_name)


def get_files_by_file_size(dirname, reverse=False):
    """ Return list of file paths in directory sorted by file size """
    filepaths = []
    for basename in os.listdir(dirname):
        filename = os.path.join(dirname, basename)
        if os.path.isfile(filename):
            filepaths.append(filename)

    for i in xrange(len(filepaths)):
        filepaths[i] = (
         filepaths[i], os.path.getsize(filepaths[i]))

    return filepaths


def make_url(path):
    """ Turn filenames into URLs, whileleaving existing URLs alone """
    if ':' not in path:
        return 'file://' + os.path.abspath(path)
    else:
        return path


def require(expression, message):
    if not expression:
        raise Exception('\n\n' + message + '\n\n')


def parse_id_ranges(job, id_ranges_file_id):
    """Returns list of triples chrom, start, end
    """
    work_dir = job.fileStore.getLocalTempDir()
    id_range_file = os.path.join(work_dir, 'id_ranges.tsv')
    job.fileStore.readGlobalFile(id_ranges_file_id, id_range_file)
    return parse_id_ranges_file(id_range_file)


def parse_id_ranges_file(id_ranges_filename):
    """Returns list of triples chrom, start, end
    """
    id_ranges = []
    with open(id_ranges_filename) as (f):
        for line in f:
            toks = line.split()
            if len(toks) == 3:
                id_ranges.append((toks[0], int(toks[1]), int(toks[2])))

    return id_ranges


def remove_ext(string, ext):
    """
    Strip a suffix from a string. Case insensitive.
    """
    if string.lower().endswith(ext.lower()):
        return string[:-len(ext)]
    else:
        return string


class TimeTracker():
    """ helper dictionary to keep tabs on several named runtimes. """

    def __init__(self, name=None):
        """ create. optionally start a timer"""
        self.times = collections.defaultdict(float)
        self.running = {}
        if name:
            self.start(name)

    def start(self, name):
        """ start a timer """
        assert name not in self.running
        self.running[name] = timeit.default_timer()

    def stop(self, name=None):
        """ stop a timer. if no name, do all running """
        names = [name] if name else self.running.keys()
        ti = timeit.default_timer()
        for name in names:
            self.times[name] += ti - self.running[name]
            del self.running[name]

    def add(self, time_dict):
        """ add in all times from another TimeTracker """
        for key, value in time_dict.times.items():
            self.times[key] += value

    def total(self, names=None):
        if not names:
            names = self.times.keys()
        return sum([ self.times[name] for name in names ])

    def names(self):
        return self.times.keys()


def run_concat_lists(job, *args):
    """
    Toil job to join all the given lists and return the merged list.
    """
    concat = []
    for input_list in args:
        concat += input_list

    return concat


def parse_plot_set(plot_set_string):
    """
    
    Given one of the string arguments to the --plot-sets option, parse out a
    data structure representing which conditions ought to be compared against
    each other, and what those comparison plots/tables should be called.

    The syntax of a plot set is [title:]condition[,condition[,condition...]].
    
    The first condition is the comparison baseline, when applicable.
    
    Returns a tuple of a plot set title, or None if unspecified, and a list of
    condition names.
    
    """
    colon_pos = plot_set_string.find(':')
    if colon_pos != -1:
        title = plot_set_string[0:colon_pos]
        plot_set_string = plot_set_string[colon_pos + 1:]
    else:
        title = None
    return (
     title, plot_set_string.split(','))


def parse_plot_sets(plot_sets_list):
    """
    
    Given a list of plot set strings, parses each with parse_plot_set.
    
    Returns a list of tuples. Each tuple is a plot set title, or None if no
    title is to be applied, and a list of condition names, or None if all
    conditions are to be included.
    
    If no plot sets are specified in the list, returns a single plot set for
    all conditions.
    
    """
    plot_sets = [ parse_plot_set(spec) for spec in plot_sets_list ]
    if len(plot_sets) == 0:
        plot_sets = [
         (None, None)]
    return plot_sets


def title_to_filename(kind, i, title, extension):
    """
    Given the kind of thing you want to save ('table', 'plot-qq', etc.), the
    number of the thign out of all things of that type, the human-readable
    title ('All Conditions vs. Whatever'), and an extansion, come up with a
    safe filename to save the plot under.
    
    The title may be None, in which case it is ommitted.
    
    The extension may be None, in which case it is omitted.
    """
    if title is not None:
        safe_title = ('').join(c for c in title if c.isalnum())
    else:
        safe_title = None
    part_list = [
     kind]
    if i != 0:
        part_list.append(('-{:02d}').format(i))
    if title is not None:
        safe_title = ('').join(c for c in title if c.isalnum())
        part_list.append(('-{}').format(safe_title))
    if extension is not None:
        part_list.append(('.{}').format(extension))
    return ('').join(part_list)