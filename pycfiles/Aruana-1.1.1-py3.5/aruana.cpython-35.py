# uncompyle6 version 3.7.4
# Python bytecode 3.5 (3351)
# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
# [GCC 8.4.0]
# Embedded file name: build/bdist.macosx-10.6-x86_64/egg/aruana/aruana.py
# Compiled at: 2019-02-08 03:28:50
# Size of source mod 2**32: 61490 bytes
import emoji, math, numpy as np, os, pandas as pd, pickle, random, re, time, unicodedata, urllib.request, warnings, zipfile
from aruana import same_words
from aruana import stopwords
from aruana import strings
from aruana import vocab
from collections import Counter
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation
from keras.optimizers import Adam
from keras import backend as K
from keras.callbacks import ModelCheckpoint
from keras.models import load_model
from matplotlib import pyplot as plt
from nltk.stem.snowball import SnowballStemmer
from sklearn.model_selection import train_test_split
from tqdm import tqdm

class Aruana:
    __doc__ = 'Aruana is a collection of methods that can be used for simple NLP tasks.\n    It can be used for tasks involving text preprocessing for machine learning.\n    Aruana works mainly with text strings and lists of strings. \n\n    It requires NLTK and TQDM to be installed.\n\n        Attributes\n        ----------\n        language : str\n            The language used to initialize Aruana. Accepts \'pt-br\', \'en\' and \'fr\'\n        nltk_language : str\n            The language used for NLTK, generated by the method __language_for_nltk\n        vocab : Counter()\n            The number of unique tokens found on the text\n        path : str\n            Used to tell where to look for the models used by Aruana. Default is Aruana workspace\n        model_type : str\n            The name of the model that has to be loaded. Default is the tagger model\n        maxlen : int\n            The value used for sentence padding. Is updated automatically after a model is loaded\n        model\n            the model that has to be loaded by Aruana\n\n        Methods\n        -------\n        __language_for_nltk()\n            Converts the language inputed during Aruana init to a format that can be used by NLTK\n        \n        remove_html_tags(text:str)\n            Removes html tags from text\n\n        replace_html_tags(text:str, placeholder:str)\n            Replaces html tags from text by a given placeholder\n        \n        remove_urls(text:str)\n            Removes urls from text\n        \n        replace_urls(text:str, placeholder:str)\n            Replaces links from text by a given placeholder\n        \n        remove_hashtags(text:str)\n            Removes hashtags from text\n        \n        replace_hashtags(text:str, placeholder:str)\n            Replaces hashtags from text by a given placeholder\n        \n        remove_ips(text:str)\n            Removes ips from text\n\n        replace_ips(text:str, placeholder:str)\n            Replaces ips from text by a given placeholder\n\n        remove_handles(text:str)\n            Removes handles from text\n\n        replace_handles(text:str, placeholder:str)\n            Replaces handles from text by a given placeholder\n\n        strip_accents(text:str)\n            Strip accents from tokens\n        \n        replace_quotes(text:str, placeholder:str)\n            Replaces quotes by a placeholder\n\n        remove_punctuation(text:str)\n            Removes ponctuation and special chars\n        \n        replace_punctuation(text:str, sign=\'\', placeholder=\'\')\n            Replaces punctuation by a placeholder\n        \n        remove_numbers(text:str)\n            Removes all numbers\n\n        replace_numbers(text:str)\n            Replaces all numbers for string\n\n        stem_sentence(text:str)\n            Uses NLTK stemmer to stem tokens\n\n        remove_stopwords(text:str, custom_list=[], extend_set=False)\n            Removes stopwords\n\n        reduce_words_with_repeated_chars(text:str)\n            Reduces words with chars repeated more than 3 times to a single char. \n            Useful to replace words such as loooooooong by long. Be careful, \n            as it can change abreviations such as AAA to single A \n        \n        remove_excessive_spaces(text:str)\n            Removes excessive space from text\n\n        merge_same_tokens(text:str)\n            If two tokens are the same, this will merge them. \n            Specially useful when dealing with social media text.\n\n        expand_contractions(text:str)\n            Expands some of the most popular English contractions\n\n        lower_remove_white(text:str)\n            Use it to lower text and to remove training whitespaces\n\n        tokenize(text:str):\n            Receives a string and splits it into tokens.\n            Returns a list with all the tokens found.\n\n        preprocess(text:str, tokenize=True, stem=True, remove_stopwords=False, tag_text=False)\n            Preprocess text before data handling with most common settings.\n            Text is processed in the following order:\n\n            - lower text\n            - strip trailing whitespaces\n            - convert emojis to text\n            - expand contractions\n            - replace urls\n            - remove tags\n            - replace hashtags\n            - replace ips\n            - remove social media handles\n            - tags text if True\n            - remove stopwords if True\n            - remove accents\n            - remove punctuation        \n            - replace numbers\n            - reduce loooooong words\n            - remove excessive spaces\n            - stem text if True\n            - tokenize text (if True, will return a list of tokens)\n        \n        preprocess_list(text_list:list, tokenize=True, stem=True, remove_stopwords=False, tag_text=False)\n            Preprocess every text in a list of strings\n\n        create_corpus(text_list:list)\n            Receives a list of strings and returns a single string with all the text\n\n        vocab_size(corpus:str)\n            Returns the number of tokens found on the corpus\n\n        lexical_diversity(corpus_slice:str, corpus:str)\n            Counts the vocab of one part of the corpus and compares it to the entire corpus\n\n        random_classification(list_to_classify:list, classes:list, balanced=True):\n        \n            Receives a list of sentences and assigns randomly a label for each item \n            on the list. Returns a list with all the random labels.\n\n        replace_with_blob(list_of_texts:list):\n        \n            Receives a list of sentences and creates blobs with the same \n            vocabulary found on corpus.\n\n        tag_tokens(text:str, sep="_"):\n        \n            Tags each token on a list of tokens and merges their tags to the original token\n        \n        ingest(path, text_column = \'\'):\n        \n            Reads a csv file and returns the corpora in the form of a list of sentences.\n        \n        _save_object(object_name:str, object_to_save):\n\n            Receives an Python object and saves it to Aruana work space using pickle\n\n        _path_model_language(path=\'\', model_name=\'\', language=\'\'):\n            \n            Defines the path, the model and the language to save or load a model\n        \n        _load_models():\n        \n            Loads a previously saved model\n            \n        _load_object(object_name:str)\n        \n            Loads a Python object saved on the Aruana work space on pickle format\n        \n        train_annotations(annotated_sentences:list, \n                          path=\'\',\n                          model_name=\'\',\n                          language=\'\',\n                          embedding_output_dim=128,\n                          lstm_units=256,\n                          save_model=True, \n                          batch_size=128,\n                          epochs=40,\n                          validation_split=0.2):\n\n            Trains a new annotation model on a set of annotated tokens. A new model\n            will be created and saved on Aruana workspace, erasing any previous model\n            saved with the same name. The local where you want to save your model, the \n            name and the language can be changed.\n        \n        _to_categorical(sequences:list, categories:int)\n        \n            Transforms the sequences of tags to sequences of categorical values\n        \n        _word_to_int_dict(train_sentences:list, \n                         test_sentences:list, \n                         train_tags:list, \n                         test_tags:list):\n\n            Creates a dictionary with the tokens and assign an int to each token\n        \n        _logits_to_tokens(sequences:list, index:int):\n\n            Converts the integers back to the tags\n        \n        annotate(list_to_predict:list):\n        \n            Does the annotations on a new unseen list of sentences using a previously saved model. \n\n        download(model_name=\'\', model_version=\'\', language=\'\'):\n        \n            Downloads and saves the models necessary for Annotation execution\n        \n    '

    def __init__(self, language: str, model_type='tagger',
                 model_version='0_0_1', path='/usr/local/share/aruana_data/models/'):
        """
        Parameters
        ----------
        language : str
            The language used to initialize Aruana. Accepts 'pt-br', 'en' and 'fr'
        nltk_language : str
            The language used for NLTK, generated by the method language_for_nltk
        vocab : Counter()
            The number of unique tokens found on the text
        path : str
            Used to tell where to look for the models used by Aruana. Default is Aruana workspace
        model_type : str
            The name of the model that has to be loaded. Default is the tagger model
        maxlen : int
            The value used for sentence padding. Is updated automatically after a model is loaded
        model
            the model that has to be loaded by Aruana
        """
        self.language = language
        self.nltk_language = self._Aruana__language_for_nltk()
        self.vocab = Counter()
        self.path = path + model_type + '/'
        self.model_type = model_type
        self.model_version = model_version
        self.maxlen = 0
        self.pos_tagger_model = self._load_models(model_name='tagger')

    def __language_for_nltk(self):
        """ Converts the language inputed during Aruana init to a format that can be used by NLTK"""
        if self.language == 'pt-br':
            return 'portuguese'
        if self.language == 'en':
            return 'english'
        if self.language == 'fr':
            return 'french'
        if self.language == 'custom':
            return 'portuguese'

    def remove_html_tags(self, text: str):
        """Removes html tags from text

        >>> remove_html_tags('<h1><strong>Hello world!</strong></h1>')
        'Hello world!'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('<[^>]*>', '', text)
        return text

    def replace_html_tags(self, text: str, placeholder: str):
        """Replaces html tags from text by a given placeholder
        
        >>> replace_html_tags('<h1><strong>Hello world!</strong></h1>', 'HTML')
        'HTML HTML Hello world! HTML HTML'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the html tag
        """
        text = re.sub('<[^>]*>', ' ' + placeholder + ' ', text)
        text = self.remove_excessive_spaces(text)
        return text

    def remove_urls(self, text: str):
        """Removes urls from text

        >>> remove_urls('You can go to the page http://homepage.com to see the content.')
        'You can go to the page to see the content.'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_urls(self, text: str, placeholder: str):
        """Replaces links from text by a given placeholder
        
        >>> replace_urls('You can go to the page http://homepage.com to see the content.', 'URL')
        'You can go to the page URL to see the content.'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the url
        """
        text = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', placeholder, text)
        return text

    def remove_hashtags(self, text: str):
        """Removes hashtags from text
        
        >>> remove_hashtags('I wish you all #love and #peanceandlove')
        'I wish you all and !'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('\\B(\\#[a-zA-Z]+\\b)(?!;)', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_hashtags(self, text: str, placeholder: str):
        """Replaces hashtags from text by a given placeholder

        >>> replace_hashtags('I wish you all #love and #peace', 'HASHTAG')
        'I wish you all HASHTAG and HASHTAG'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the hashtag
        """
        text = re.sub('\\B(\\#[a-zA-Z]+\\b)(?!;)', placeholder, text)
        return text

    def remove_ips(self, text: str):
        """Removes ips from text

        >>> remove_ips('This can be accessed on the address 198.162.0.1.')
        'This can be accessed on the address .'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_ips(self, text: str, placeholder: str):
        """Replaces ips from text by a given placeholder

        >>> replace_ips('This can be accessed on the address 198.162.0.1.', 'IP')
        'This can be accessed on the address IP.'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the ip
        """
        text = re.sub('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', placeholder, text)
        return text

    def remove_handles(self, text: str):
        """Removes handles from text

        >>> remove_handles('Can you come tonight, @alice?')
        'Can you come tonight, ?'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('\\B(\\@[a-zA-Z_0-9]+\\b)(?!;)', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_handles(self, text: str, placeholder: str):
        """Replaces handles from text by a given placeholder

        >>> replace_handles('Can you come tonight, @alice?', 'USERNAME')
        'Can you come tonight, USERNAME?'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the handle
        """
        text = re.sub('\\B(\\@[a-zA-Z_0-9]+\\b)(?!;)', placeholder, text)
        return text

    def strip_accents(self, text: str):
        """Strip accents from tokens

        >>> strip_accents('Mamãe me disse: você é o menino mais ágil do time.')
        'Mamae me disse: voce e o menino mais agil do time.'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        try:
            text = unicode(text, 'utf-8')
        except NameError:
            pass

        text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')
        return str(text)

    def replace_quotes(self, text: str, placeholder: str):
        """Replaces quotes by a placeholder

        >>> replace_quotes('She told me: "you are a confident and strong woman".', 'QUOTED')
        'She told me: QUOTED you are a confident and strong woman QUOTED .'

        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the quote sign
        """
        text = re.sub('"', ' ' + placeholder + ' ', text)
        text = re.sub("'", ' ' + placeholder + ' ', text)
        text = self.remove_excessive_spaces(text)
        return text

    def remove_punctuation(self, text: str):
        """Removes ponctuation and special chars

        >>> remove_punctuation('Hey, are you here??? I really need your help!')
        'Hey are you here I really need your help'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        placeholder : str
            The string that will replace the punctuation and the special chars found
        """
        text = re.sub('[^\\w\\s]', ' ', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_punctuation(self, text: str, sign='',
                            placeholder=''):
        """Replaces punctuation by a placeholder.

        >>> replace_punctuation('Hey, are you here??? I really need your help!', sign='?', placeholder='QUESTION')
        'Hey, are you here QUESTION QUESTION QUESTION I really need your help!'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        sign : str
            The symbol you want to replace. If no symbol is provided, will replace all special chars and punctuation by the placeholder
        placeholder : str
            The string that will replace the punctuation and the special chars found
        """
        if sign is '':
            text = re.sub('[^\\w\\s]', ' ' + placeholder + ' ', text)
        else:
            text = text.replace(sign, ' ' + placeholder + ' ')
        text = self.remove_excessive_spaces(text)
        return text

    def remove_numbers(self, text: str):
        """Removes all numbers

        >>> remove_numbers('I told him 1,2,3,4 times that he could not do that!')
        'I told him ,,, times that he could not do that!'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('[0-9]', '', text)
        text = self.remove_excessive_spaces(text)
        return text

    def replace_numbers(self, text: str):
        """Replaces all numbers for string

        >>> replace_numbers('I told him 1,2,3,4 times that he could not do that!')
        'I told him one, two, three, four times that he could not do that!'. 
        Attention: It does NOT convert numbers like 20 to twenty, but to
        two zero.

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        if self.language == 'pt-br':
            text = text.replace('0', ' zero ')
            text = text.replace('1', ' um ')
            text = text.replace('2', ' dois ')
            text = text.replace('3', ' três ')
            text = text.replace('4', ' quatro ')
            text = text.replace('5', ' cinco ')
            text = text.replace('6', ' seis ')
            text = text.replace('7', ' sete ')
            text = text.replace('8', ' oito ')
            text = text.replace('9', ' nove ')
        else:
            if self.language == 'fr':
                text = text.replace('0', ' zéro ')
                text = text.replace('1', ' un ')
                text = text.replace('2', ' deux ')
                text = text.replace('3', ' trois ')
                text = text.replace('4', ' quatre ')
                text = text.replace('5', ' cinc ')
                text = text.replace('6', ' six ')
                text = text.replace('7', ' sept ')
                text = text.replace('8', ' huit ')
                text = text.replace('9', ' neuf ')
            elif self.language == 'en':
                text = text.replace('0', ' zero ')
                text = text.replace('1', ' one ')
                text = text.replace('2', ' two ')
                text = text.replace('3', ' three ')
                text = text.replace('4', ' four ')
                text = text.replace('5', ' five ')
                text = text.replace('6', ' six ')
                text = text.replace('7', ' seven ')
                text = text.replace('8', ' eight ')
                text = text.replace('9', ' nine ')
        text = self.remove_excessive_spaces(text)
        return text

    def stem_sentence(self, text: str, tagged_text=False,
                      sep='_'):
        """Uses NLTK stemmer to stem tokens

        >>> stem_sentence('I love to go shopping with my mother and friends')
        'i love to go shop with my mother and friend'

        Parameters
        ----------
        text : str
            The string that will be transformed
        tagged_text : boolean
            If you are using a text that was previously tagged, set it to True
        sep : str
            Here you can inform wich separator was used to merge the word to the tag
        """
        stemmer = SnowballStemmer(self.nltk_language)
        sentence = []
        words_tags = []
        words = []
        tags = []
        text = re.sub('\\s{2,}', ' ', text)
        if tagged_text == True:
            for tagged_word in text.split(' '):
                for tokens in tagged_word.split(sep):
                    words_tags.append(tokens)

            for i, token in enumerate(words_tags):
                if i % 2 == 0:
                    words.append(stemmer.stem(str(token)))
                    tags.append(words_tags[(i + 1)])

            for i, word in enumerate(words):
                sentence.append(word + sep + tags[i])

            text = ' '.join(sentence)
        else:
            for word in text.split(' '):
                word = stemmer.stem(str(word))
                sentence.append(word)

            text = ' '.join(sentence)
        return text

    def remove_stopwords(self, text: str, custom_list=[], extend_set=False):
        """ Removes stopwords

        >>> remove_stopwords('I love to go shopping with my mother and friends')
        'love go shopping mother friends'

        Parameters
        ----------
        text : str
            The string that will be transformed
        custom_list : list
            A list with the custom stopwords list
        extend_set : boolean
            Use this option to extend the stopwords of a given language with your custom list 
        """
        if self.language == 'pt-br':
            stop_words = stopwords.portuguese
        else:
            if self.language == 'en':
                stop_words = stopwords.english
            else:
                if self.language == 'fr':
                    stop_words = stopwords.french
                else:
                    if self.language == 'custom':
                        stop_words = custom_list
                    else:
                        print('This set is not available. No stopword will be removed!')
                        stop_words = []
        if extend_set == True:
            stop_words = stop_words + custom_list
        text = ' ' + text + ' '
        for word in stop_words:
            text = re.sub('\\s' + word + '\\s', ' ', text, flags=re.I)
            if self.language == 'fr':
                text = re.sub(re.escape("j'"), ' ', text, flags=re.I)
                text = re.sub(re.escape("d'"), ' ', text, flags=re.I)
                text = re.sub(re.escape("l'"), ' ', text, flags=re.I)
                text = re.sub(re.escape("m'"), ' ', text, flags=re.I)
                text = re.sub(re.escape("s'"), ' ', text, flags=re.I)
                text = re.sub(re.escape("t'"), ' ', text, flags=re.I)

        text = text.strip()
        text = self.remove_excessive_spaces(text)
        return text

    def reduce_words_with_repeated_chars(self, text: str):
        """Reduces words with chars repeated more than 3 times to a single char. 
        Useful to replace words such as loooooooong by long. Be careful, 
        as it can change abreviations such as AAA to single A 

        >>> reduce_words_with_repeated_chars('I loooooooooooooove pizza so muuuchhhh')
        'I love pizza so much'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        findings = re.findall('(\\w)\\1{2,}', text)
        for char in findings:
            find = char + '{3,}'
            replace = '???' + char + '???'
            text = re.sub(find, repr(replace), text)

        text = text.replace("'???", '')
        text = text.replace("???'", '')
        text = self.remove_excessive_spaces(text)
        return text

    def remove_excessive_spaces(self, text: str):
        """Removes excessive space from text

        >>> remove_excessive_spaces('I  can't     stop looking at  you')
        'I can't stop looking at you'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = re.sub('(\\s)\\1{1,}', ' ', text)
        text = text.strip()
        return text

    def merge_same_tokens(self, text: str):
        """If two tokens are the same, this will merge them. Specially useful when dealing with social
        media text

        >>> merge_same_tokens("eu acho q vc tb estava lá")
        'eu acho que você também estava lá'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        if self.language == 'pt-br':
            text = ' ' + text + ' '
            for k, v in same_words.pt_br.items():
                text = re.sub(re.escape(k), v, text, flags=re.I)

            text = text.strip()
        return text

    def expand_contractions(self, text: str):
        """ Expands some contractions and merge same tokens
        
        >>> expand_contractions("you're solely responsible for your success and your failure.")
        'you are solely responsible for your success and your failure.'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        if self.language == 'en':
            text = ' ' + text + ' '
            for k, v in same_words.en.items():
                text = re.sub(re.escape(k), v, text, flags=re.I)

            text = text.strip()
        else:
            if self.language == 'fr':
                text = ' ' + text + ' '
                for k, v in same_words.fr.items():
                    text = re.sub(re.escape(k), v, text, flags=re.I)

                text = text.strip()
            elif self.language == 'pt-br':
                text = ' ' + text + ' '
                for k, v in same_words.pt_br.items():
                    text = re.sub(re.escape(k), v, text, flags=re.I)

                text = text.strip()
        return text

    def lower_remove_white(self, text: str):
        """ Use it to lower text and to remove training whitespaces
        
        >>> lower_remove_white("- Mary is coming! 
- When? 
- Today!")
        '- mary is coming!  - when?  - today!'

        Parameters
        ----------
        text : str
            The string that will be transformed
        """
        text = text.lower()
        text = re.sub('\\n', ' ', text)
        return text

    def preprocess(self, text: str, tokenize=True,
                   stem=True, remove_stopwords=False, tag_text=False):
        """Preprocess text before data handling with most common settings.
        Text is processed in the following order:

            - lower text
            - strip trailing whitespaces
            - convert emojis to text
            - expand contractions
            - replace urls
            - remove tags
            - replace hashtags
            - replace ips
            - remove social media handles
            - replace numbers
            - reduce loooooong words
            - remove punctuation
            - remove excessive spaces     
            - tag text if True
            - remove stopwords if True
            - remove accents
            - remove excessive spaces
            - stem text if True
            - tokenize text (if True, will return a list of tokens)

        >>> preprocess("At the end of the day, you're solely responsible for your success and your failure. And the sooner you realize that, you accept that, and integrate that into your work ethic, you will start being successful. As long as you blame others for the reason you aren't where you want to be, you will always be a failure.")
        '['end', 'day', 'sole', 'respons', 'success', 'failur', 'sooner', 'realiz', 'that', 'accept', 'that', 'integr', 'work', 'ethic', 'start', 'success', 'long', 'blame', 'other', 'reason', 'want', 'be', 'alway', 'failur']'
        
        Parameters
        ----------
        text : str
            The string that will be transformed
        tokenize : boolean
            Defines if the text should be tokenized or not
        stem : boolean
            Defines if the tokens should be stemmed or not
        remove_stopwords : boolean
            Defines if stopwords should be removed.
        tag_text : boolean
            Defines if text should be tagged
        """
        text = str(text)
        text = self.lower_remove_white(text)
        text = emoji.demojize(text)
        text = self.expand_contractions(text)
        text = self.replace_urls(text, 'URL')
        text = self.remove_html_tags(text)
        text = self.replace_hashtags(text, 'HASHTAG')
        text = self.replace_ips(text, 'IP')
        text = self.remove_handles(text)
        text = self.replace_numbers(text)
        text = self.reduce_words_with_repeated_chars(text)
        text = self.remove_punctuation(text)
        text = self.remove_excessive_spaces(text)
        if remove_stopwords:
            text = self.remove_stopwords(text)
        if tag_text:
            annotated_sentence = self.tag_tokens(text=text, sep='taggedtext')
            text = ' '.join(annotated_sentence)
        text = self.strip_accents(text)
        text = self.remove_excessive_spaces(text)
        if stem:
            text = self.stem_sentence(text, tagged_text=tag_text, sep='taggedtext')
        if tokenize:
            text = self.tokenize(text)
            self.vocab.update(text)
        else:
            self.vocab.update(list(text))
        if tag_text == True and tokenize == False:
            text = text.replace('taggedtext', '_')
        elif tag_text == True and tokenize == True:
            text = [t.replace('taggedtext', '_') for t in text]
        return text

    def preprocess_list(self, text_list: list, tokenize=True,
                        stem=True, remove_stopwords=False, tag_text=False):
        """Preprocess every text in a list of strings

        >>> list_of_strings = [
        'I love you',
        'Please, never leave me alone',
        'If you go, I will die',
        'I am watching a lot of romantic comedy lately',
        'I have to eat icecream'
        ]

        >>> list_processed = aruana_en.preprocess_list(list_of_strings, stem=False)
        '['end', 'day', 'sole', 'respons', 'success', 'failur', 'sooner', 'realiz', 'that', 'accept', 'that', 'integr', 'work', 'ethic', 'start', 'success', 'long', 'blame', 'other', 'reason', 'want', 'be', 'alway', 'failur']'

        Parameters
        ----------
        text : list
            The list of strings to be transformed
        tokenize : boolean
            Defines if the text should be tokenized or not
        stem : boolean
            Defines if the tokens should be stemmed or not
        remove_stopwords : boolean
            Defines if stopwords should be removed.
        tag_text : boolean
            Defines if text should be tagged
        """
        texts = []
        for text in tqdm(text_list):
            text = self.preprocess(text, tokenize=tokenize, stem=stem, remove_stopwords=remove_stopwords, tag_text=tag_text)
            texts.append(text)

        return texts

    def create_corpus(self, text_list: list):
        """Receives a list of strings and returns a single string with all the text

        >>> list_of_strings = ['I love you', 'Please, never leave me alone', 'If you go, I will die', 'I am watching a lot of romantic comedy lately', 'I have to eat icecream']
        >>> create_corpus(list_of_strings)
        'I love you Please, never leave me alone If you go, I will die I am watching a lot of romantic comedy lately I have to eat icecream'

        Parameters
        ----------
        text : list
            The list of strings that will be used to return the corpus
        """
        corpus = ' '
        for text in text_list:
            corpus = corpus + ' ' + str(text)

        return corpus

    def vocab_size(self, corpus: str):
        """Returns the number of unique tokens found on the corpus

        >>> vocab_size('  I love you Please, never leave me alone If you go, I will die I am watching a lot of romantic comedy lately I have to eat icecream')
        24

        Parameters
        ----------
        corpus : str
            The string that holds the entire corpus
        """
        words_map_dict = Counter(corpus.split())
        unique_words = len(words_map_dict.keys())
        vocab_size = int(unique_words)
        return vocab_size

    def lexical_diversity(self, corpus_slice: str, corpus: str):
        """Counts the vocab of one part of the corpus and compares it to the entire corpus

        >>> lexical_diversity(list_of_strings[0], corpus)
        0.125

        Parameters
        ----------
        corpus_slice : str
            The string that you want to analyse
        corpus : str
            The string that holds the entire corpus
        """
        slice_vocab = self.vocab_size(corpus_slice)
        all_vocab = self.vocab_size(corpus)
        diversity = slice_vocab / all_vocab
        return diversity

    def random_classification(self, list_to_classify: list, classes: list, balanced=True):
        """Receives a list of sentences and assigns randomly a label for each item
        on the list. Returns a list with all the random labels.
        
        >>> random_classification(['A sentence', 'Another sentence'][0,1])
            [1,1]

        Parameters
        ----------
        list_to_classify : list
            A list that holds all the text that needs to be randomly labelized
                
        classes : list
            A list that holds the possible classes

        balanced : boolean
            If True, resulting labels will have an equal or near equal number of examples
        """
        labels = []
        if balanced == False:
            for sentence in list_to_classify:
                labels.append(random.choice(classes))

        else:
            examples_len = len(list_to_classify)
            classes_len = len(classes)
            examples_per_class = math.ceil(examples_len / classes_len)
            class_counter = {}
            for available_label in classes:
                class_counter[available_label] = 0

            for sentence in list_to_classify:
                random_class = random.choice(classes)
                if class_counter[random_class] <= examples_per_class:
                    labels.append(random_class)
                    class_counter[random_class] += 1
                else:
                    classes.remove(random_class)
                    labels.append(random.choice(classes))

            for k, v in class_counter.items():
                print('Class {}: {} values'.format(k, v))

        return labels

    def replace_with_blob(self, list_of_texts: list):
        """Receives a list of sentences and creates blobs with the same 
        vocabulary found on corpus.
        
        >>> random_classification(['A sentence', 'Another sentence'][0,1])
            ['sentence Another', 'A Another']

        Parameters
        ----------
        list_of_texts : list
            A list that holds all the text that needs to become blob
        """
        vocab = list(self.create_corpus(list_of_texts).split(' '))
        sentence = []
        sentences = []
        for text in list_of_texts:
            text_len = len(str(text).split(' '))
            for x in range(text_len):
                sentence.append(random.choice(vocab))

            text = ' '.join(sentence)
            sentence = []
            sentences.append(text)

        return sentences

    def tokenize(self, text: str):
        """Receives a string and splits it into tokens.
        Returns a list with all the tokens found.

        >>> tokenize('I want to eat mow.")
            ['I', 'want', 'to', 'eat', 'now', '.']

        Parameters
        ----------
        text : str
            The text that will be tokenized
        """
        text = text.replace('...', '…')
        url_results = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text, re.MULTILINE)
        text = self.replace_urls(text, 'URLHOLDER')
        for p in emoji.UNICODE_EMOJI:
            text = text.replace(p, ' ' + p + ' ')

        for p in strings.punctuation:
            text = text.replace(p, ' ' + p + ' ')

        text = self.remove_excessive_spaces(text)
        for url in url_results:
            text = text.replace('URLHOLDER', url, 1)

        tokens = []
        for token in text.split(' '):
            if token != ' ' or token != '':
                tokens.append(token)

        return tokens

    def tag_tokens(self, text: str, merge_tags=True,
                   sep='_'):
        """Tags each token on a list of tokens and merges their tags to the original token if merge_tags=True
        Parameters
        ----------
        text : str
            A list that holds all the tokens to be tagged
        merge_tags : boolean
            If True, will merge tages to the original tokens. If false, will return a list of tuples
        sep : str
            The symbol you want to use to merge the token with its tag
        """
        tagged_sentence = self.annotate([text])
        tagged_sentence = tagged_sentence[0]
        if merge_tags == True:
            merged = []
            for ensemble in tagged_sentence:
                merged_token = ensemble[0] + sep + ensemble[1]
                merged.append(merged_token)

            return merged
        else:
            return tagged_sentence

    def ingest(self, path, text_column=''):
        """Reads a csv file and returns the corpora in the form of a list
        of sentences.
        
        Parameters
        ----------
        path : str
            The place where your corpora is saved 
        text_column : ''
            The name of the column that holds the text
        """
        data = pd.read_csv(path)
        data = data.dropna(subset=[text_column])
        data = list(data[text_column])
        return data

    def _save_object(self, object_name: str, object_to_save, path='',
                     model_name='', model_version='', language=''):
        """Receives an Python object and saves it to Aruana work space
        using pickle
        
        Parameters
        ----------
        object_name : str
            The name of the object to be saved
        object_to_save
            The object you want to save
        path : str
            The path where you want to save your model. Default value is 
            Aruana workspace
        model_name : str
            The name of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        model_version : str
            The version of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        language : str
            The language of the model. Default is the one you choosed 
            when Aruana was started
        """
        path, model_name, model_version, language = self._path_model_language(path=path, model_name=model_name, model_version=model_version, language=language)
        if os.path.isdir(self.path) == True:
            with open('{}{}_{}_{}_{}'.format(path, object_name, model_name, model_version, language), 'wb') as (file):
                pickle.dump(object_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)
                print('Object saved')
        try:
            access_rights = 493
            os.makedirs(self.path, access_rights)
            with open('{}{}_{}_{}_{}'.format(path, object_name, model_name, model_version, language), 'wb') as (file):
                pickle.dump(object_to_save, file, protocol=pickle.HIGHEST_PROTOCOL)
                print('Object saved')
        except OSError:
            print('Creation of the directory %s failed' % path)
        else:
            print('Successfully created the directory %s' % path)

    def _load_models(self, path='', model_name='', model_version='', language=''):
        """Loads a previously saved model
        
        Parameters
        ----------
        path : str
            The path of the model
        model_name : str
            The name of the model
        model_version : str
            The version of the model
        language : str
            The language of the model
        """
        path, model_name, model_version, language = self._path_model_language(path=path, model_name=model_name, model_version=model_version, language=language)
        try:
            clf = load_model('{}{}_{}_{}'.format(path, model_name, model_version, language))
            return clf
        except:
            warnings.warn("Could not load model. Execution will continue, but you can't do annotations unless you download the necessary models.", Warning)

    def _load_object(self, object_name: str):
        """Loads a Python object saved on the Aruana work space on pickle format
        
        Parameters
        ----------
        object_name : str
            The name of the object to be loaded
        """
        try:
            filename = '{}{}_{}_{}_{}'.format(self.path, object_name, self.model_type, self.model_version, self.language)
            with open(filename, 'rb') as (file):
                return pickle.load(file)
        except Exception as e:
            loaded_object = ''
            return loaded_object
            print('It was not possible to load the object because:', e)

    def _path_model_language(self, path='', model_name='', model_version='', language=''):
        """Defines the path, the model and the language to save or load a model

        Parameters
        ----------
        path : str
            The path of the model
        model_name : str
            The name of the model
        model_version : str
            The version of the model
        language : str
            The language of the model
        """
        if path is not '':
            path = path
        else:
            path = self.path
        if model_name is not '':
            model_name = model_name
        else:
            model_name = self.model_type
        if model_version is not '':
            model_version = model_version
        else:
            model_version = self.model_version
        if language is not '':
            language = language
        else:
            language = self.language
        return (path, model_name, model_version, language)

    def train_annotations(self, annotated_sentences: list, path='',
                          model_name='',
                          model_version='',
                          language='',
                          embedding_output_dim=128,
                          lstm_units=256,
                          save_model=True,
                          batch_size=128,
                          epochs=40,
                          validation_split=0.2):
        """Trains a new annotation model on a set of annotated tokens. A new model
        will be created and saved on Aruana workspace, erasing any previous model
        saved with the same name. The local where you want to save your model, the 
        name and the language can be changed.
        
        Parameters
        ----------
        annotated_sentences : list
            A list with the sentences that will be used to create the model
        path : str
            The path where you want to save your model. Default value is 
            Aruana workspace
        model_name : str
            The name of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        model_version : str
            The version of the model that you are producing. Default is the one you choosed 
            when Aruana was started
        language : str
            The language of the model. Default is the one you choosed 
            when Aruana was started
        embedding_output_dim : int
            Dimension of the dense embedding
        lstm_units : int
            Dimensionality of the output space for the LSTM Keras layer
        save_model : boolean
            If True, will save the model each time it improves after an epoch. If 
            False, won't save the model at all
        batch_size : int
            The number of samples that will be propagated through the network
        epochs : int
            The number of full training cycles on the training set. 
        validation_split : float
            A value between 0 and 1 that defines how many examples will be used
            in validation
        """
        sentences, sentence_tags = [], []
        for tagged_sentence in annotated_sentences:
            sentence, tags = zip(*tagged_sentence)
            sentences.append(np.array(sentence))
            sentence_tags.append(np.array(tags))

        train_sentences, test_sentences, train_tags, test_tags = train_test_split(sentences, sentence_tags, test_size=0.2)
        train_sentences_X, test_sentences_X, train_tags_y, test_tags_y, word2index, tag2index = self._word_to_int_dict(train_sentences, test_sentences, train_tags, test_tags)
        path, model_name, model_version, language = self._path_model_language(path=path, model_name=model_name, model_version=model_version, language=language)
        self._save_object(object_to_save=word2index, object_name='word2index', path=path, model_name=model_name, model_version=model_version, language=language)
        self._save_object(object_to_save=tag2index, object_name='tag2index', path=path, model_name=model_name, model_version=model_version, language=language)
        model = Sequential()
        model.add(InputLayer(input_shape=(self.maxlen,)))
        model.add(Embedding(len(word2index), embedding_output_dim))
        model.add(Bidirectional(LSTM(lstm_units, return_sequences=True)))
        model.add(TimeDistributed(Dense(len(tag2index))))
        model.add(Activation('softmax'))
        model.compile(loss='categorical_crossentropy', optimizer=Adam(0.001), metrics=[
         'accuracy'])
        model.summary()
        cat_train_tags_y = self._to_categorical(train_tags_y, len(tag2index))
        checkpointer = ModelCheckpoint(filepath='{}{}_{}_{}'.format(path, model_name, model_version, language), verbose=1, save_best_only=save_model)
        history = model.fit(train_sentences_X, self._to_categorical(train_tags_y, len(tag2index)), batch_size=batch_size, epochs=epochs, validation_split=validation_split, callbacks=[
         checkpointer], verbose=1, shuffle=True)
        scores = model.evaluate(test_sentences_X, self._to_categorical(test_tags_y, len(tag2index)))
        print('{}: {}'.format(model.metrics_names[1], scores[1] * 100))
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title('model train vs validation loss')
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.legend(['train', 'validation'], loc='upper right')
        plt.show()
        return model

    def _to_categorical(self, sequences: list, categories: int):
        """ Transforms the sequences of tags to sequences of categorical values
        
        Parameters
        ----------
        sequences : list
            A list with the tags found on the set
        categories : int
            The number of categories present on the set
        """
        cat_sequences = []
        for s in sequences:
            cats = []
            for item in s:
                cats.append(np.zeros(categories))
                cats[(-1)][item] = 1.0

            cat_sequences.append(cats)

        return np.array(cat_sequences)

    def _word_to_int_dict(self, train_sentences: list, test_sentences: list, train_tags: list, test_tags: list):
        """ Creates a dictionary with the tokens and assign an int to each token
        
        Parameters
        ----------
        train_sentences : list
            A list with the training sentences
        test_sentences : list
            A list with the test sentences
        train_tags : list
            A list with the training tags
        test_tags : list
            A list with the test tags
        """
        words, tags = set([]), set([])
        for s in train_sentences:
            for w in s:
                words.add(w.lower())

        for ts in train_tags:
            for t in ts:
                tags.add(t)

        word2index = {w:i + 2 for i, w in enumerate(list(words))}
        word2index['PAD'] = 0
        word2index['OOV'] = 1
        tag2index = {t:i + 1 for i, t in enumerate(list(tags))}
        tag2index['PAD'] = 0
        train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = ([], [], [], [])
        for s in train_sentences:
            s_int = []
            for w in s:
                try:
                    s_int.append(word2index[w.lower()])
                except KeyError:
                    s_int.append(word2index['OOV'])

            train_sentences_X.append(s_int)

        for s in test_sentences:
            s_int = []
            for w in s:
                try:
                    s_int.append(word2index[w.lower()])
                except KeyError:
                    s_int.append(word2index['OOV'])

            test_sentences_X.append(s_int)

        for s in train_tags:
            train_tags_y.append([tag2index[t] for t in s])

        for s in test_tags:
            test_tags_y.append([tag2index[t] for t in s])

        self.maxlen = len(max(train_sentences_X, key=len))
        train_sentences_X = pad_sequences(train_sentences_X, maxlen=self.maxlen, padding='post')
        test_sentences_X = pad_sequences(test_sentences_X, maxlen=self.maxlen, padding='post')
        train_tags_y = pad_sequences(train_tags_y, maxlen=self.maxlen, padding='post')
        test_tags_y = pad_sequences(test_tags_y, maxlen=self.maxlen, padding='post')
        return (
         train_sentences_X, test_sentences_X, train_tags_y, test_tags_y, word2index, tag2index)

    def _logits_to_tokens(self, sequences: list, index: int):
        """Converts the integers back to the tags
        
        Parameters
        ----------
        sequences : list
            A list with the sequences
        index : int
            The index value
        """
        token_sequences = []
        for categorical_sequence in sequences:
            token_sequence = []
            for categorical in categorical_sequence:
                token_sequence.append(index[np.argmax(categorical)])

            token_sequences.append(token_sequence)

        return token_sequences

    def annotate(self, list_to_predict: list):
        """Does the annotations on a new unseen list of sentences
        using a previously saved model. 
        
        Parameters
        ----------
        list_to_predict : list
            The list with the sentences to be annotated
        """
        if self.model_type == 'tagger':
            model = self.pos_tagger_model
        word2index = self._load_object('word2index')
        tag2index = self._load_object('tag2index')
        list_to_predict_X = []
        for sentence in list_to_predict:
            s_int = []
            sentence = self.tokenize(sentence)
            for token in sentence:
                try:
                    s_int.append(word2index[token.lower()])
                except KeyError:
                    s_int.append(word2index['OOV'])

            list_to_predict_X.append(s_int)

        list_to_predict_X = pad_sequences(list_to_predict_X, maxlen=model.input_shape[1], padding='post')
        predictions = model.predict(list_to_predict_X)
        tags = self._logits_to_tokens(predictions, {i:t for t, i in tag2index.items()})
        return_predictions = []
        for i, sentence in enumerate(list_to_predict):
            sentence = self.tokenize(sentence)
            tagged = zip(sentence, tags[i])
            return_predictions.append(list(tagged))

        return return_predictions

    def download(self, model_name='', model_version='', language=''):
        """Downloads and saves the models necessary for Annotation execution
        
        Parameters
        ----------
        model_name : str
            The name of the model
        model_version : str
            The version of the model. Must be in the format x_x_x.
        language : str
            The language of the model
        """
        path, model_name, model_version, language = self._path_model_language(path='', model_name=model_name, model_version=model_version, language=language)
        if os.path.isdir(path) == False:
            try:
                access_rights = 493
                os.makedirs(path, access_rights)
            except OSError:
                print('Creation of the directory %s failed' % path)
            else:
                print('Successfully created the directory %s' % path)
        if model_name == 'tagger' or model_name == 'lemmatizer':
            try:
                models_zipped_filename = model_name + '_' + model_version + '_' + language + '.zip'
                models_zipped = 'http://nheeng.com/aruana/models/' + model_name + '/' + models_zipped_filename
                print('Downloading the compressed model files')
                urllib.request.urlretrieve(models_zipped, path + models_zipped_filename)
                print('Decompressing files')
                zip_models = zipfile.ZipFile(path + models_zipped_filename, 'r')
                zip_models.extractall(path)
                zip_models.close()
                print('Remove zip file')
                os.remove(path + models_zipped_filename)
                print('Files are ready for use. Reload Aruana!')
            except Exception as e:
                print('It was not possible to download the models because:', e)