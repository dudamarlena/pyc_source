# uncompyle6 version 3.7.4
# Python bytecode 3.7 (3394)
# Decompiled from: Python 3.6.9 (default, Apr 18 2020, 01:56:04) 
# [GCC 8.4.0]
# Embedded file name: /home/nwinter/PycharmProjects/projects_nils/photon_core/photonai/base/hyperpipe.py
# Compiled at: 2019-11-21 10:19:36
# Size of source mod 2**32: 66283 bytes
import datetime, importlib, importlib.util, inspect, logging, os, pickle, re, shutil, traceback, zipfile, json, time
from copy import deepcopy
import __main__, dask, numpy as np, pandas as pd
from bson.objectid import ObjectId
from dask.distributed import Client
from sklearn.base import BaseEstimator
from sklearn.dummy import DummyClassifier, DummyRegressor
from sklearn.externals import joblib
from sklearn.model_selection._split import BaseCrossValidator
from sklearn.model_selection import KFold
from photonai.__init__ import __version__
from photonai.base.cache_manager import CacheManager
from photonai.base.photon_elements import Stack, Switch, Preprocessing, CallbackElement, Branch, PipelineElement, PhotonNative
from photonai.base.photon_pipeline import PhotonPipeline
from photonai.optimization import GridSearchOptimizer, TimeBoxedRandomGridSearchOptimizer, RandomGridSearchOptimizer, SkOptOptimizer, RandomSearchOptimizer, IntegerRange, FloatRange, Categorical
from photonai.optimization.smac.smac_new import SMACOptimizer
import photonai.photonlogger.logger as logger
from photonai.processing import ResultsHandler
from photonai.processing.metrics import Scorer
from photonai.processing.outer_folds import OuterFoldManager
from photonai.processing.photon_folds import FoldInfo
from photonai.processing.results_structure import MDBHyperpipe, MDBHyperpipeInfo, MDBDummyResults, MDBHelper, FoldOperations, MDBConfig, MDBOuterFold

class OutputSettings:
    __doc__ = "\n    Configuration class that specifies the format in which the results are saved. Results can be saved to a MongoDB\n    or a simple son-file. You can also choose whether to save predictions and/or feature importances.\n\n    Parameters\n    ----------\n    * `mongodb_connect_url` [str]:\n        Valid mongodb connection url that specifies a database for storing the results\n\n    * `save_predictions` [str, default='best']:\n        Possible options are 'best' to save only the predictions of the best configuration for every outer fold, 'all'\n        to save all predictions or 'None' to not save any predictions at all.\n\n    * `save_feature_importances` [str, default='best']:\n        Possible options are 'best' to save only the feature importances of the best configuration for every outer fold,\n        'all' to save all feature importances or 'None' to not save any at all. Feature importances can only be saved\n        if the chosen estimators have an attribute 'coef_' or 'feature_importances_'.\n\n    * `project_folder` [bool, default=True]:\n        If True, PHOTON writes a summary_file, the results of the hyperparameter optimization, the best model and the\n        console output to the filesystem into the given project folder.\n\n    * `project_folder` [str, default='']:\n        The output folder in which all files generated by the PHOTON project are saved to.\n\n    * `user_id` [str]:\n       The user name of the according PHOTON Wizard login\n\n    * `wizard_object_id` [str]:\n       The object id to map the designed pipeline in the PHOTON Wizard to the results in the PHOTON CORE Database\n\n    * `wizard_project_name` [str]:\n       How the project is titled in the PHOTON Wizard\n    "

    def __init__(self, mongodb_connect_url: str=None, save_output: bool=True, plots: bool=True, overwrite_results: bool=False, project_folder: str='', user_id: str='', wizard_object_id: str='', wizard_project_name: str=''):
        self.mongodb_connect_url = mongodb_connect_url
        self.overwrite_results = overwrite_results
        self.__main_file__ = __main__.__file__
        if project_folder == '':
            self.project_folder = os.path.dirname(self.__main_file__)
        else:
            self.project_folder = project_folder
        self.results_folder = None
        self.log_file = None
        self.initialize_log_file()
        self.save_output = save_output
        self.save_predictions_from_best_config_inner_folds = None
        self.plots = plots
        self.logging_file_handler = None
        self.verbosity = 0
        self.user_id = user_id
        self.wizard_object_id = wizard_object_id
        self.wizard_project_name = wizard_project_name

    def initialize_log_file(self):
        self.log_file = os.path.join(self.project_folder, 'photon_setup_errors.log')

    def _update_settings(self, name, timestamp):
        if not os.path.exists(self.project_folder):
            os.makedirs(self.project_folder)
        if self.save_output:
            if self.overwrite_results:
                self.results_folder = os.path.join(self.project_folder, name + '_results')
            else:
                self.results_folder = os.path.join(self.project_folder, name + '_results_' + timestamp)
            if not os.path.exists(self.results_folder):
                os.makedirs(self.results_folder)
            shutil.copy(self.__main_file__, os.path.join(self.results_folder, 'photon_code.py'))
            if os.path.basename(self.log_file) == 'photon_setup_errors.log':
                self.log_file = 'photon_output.log'
            self.log_file = self._add_timestamp(self.log_file)
            self.set_log_file()

    def _add_timestamp(self, file):
        return os.path.join(self.results_folder, os.path.basename(file))

    def _get_log_level(self):
        if self.verbosity == 0:
            level = 25
        else:
            if self.verbosity == 1:
                level = logging.INFO
            else:
                if self.verbosity == 2:
                    level = logging.DEBUG
                else:
                    level = logging.WARN
        return level

    def set_log_file(self):
        logfile_directory = os.path.dirname(self.log_file)
        if not os.path.exists(logfile_directory):
            os.makedirs(logfile_directory)
        elif self.logging_file_handler is None:
            self.logging_file_handler = logging.FileHandler(self.log_file)
            self.logging_file_handler.setLevel(self._get_log_level())
            logger.addHandler(self.logging_file_handler)
        else:
            self.logging_file_handler.close()
            self.logging_file_handler.baseFilename = self.log_file

    def set_log_level(self):
        verbose_num = self._get_log_level()
        logger.setLevel(verbose_num)
        for handler in logger.handlers:
            handler.setLevel(verbose_num)


class Hyperpipe(BaseEstimator):
    __doc__ = '\n    Wrapper class for machine learning pipeline, holding all pipeline elements\n    and managing the optimization of the hyperparameters\n\n    Parameters\n    ----------\n    * `name` [str]:\n        Name of hyperpipe instance\n\n    * `inner_cv` [BaseCrossValidator]:\n        Cross validation strategy to test hyperparameter configurations, generates the validation set\n\n    * `outer_cv` [BaseCrossValidator]:\n        Cross validation strategy to use for the hyperparameter search itself, generates the test set\n\n    * `optimizer` [str or object, default="grid_search"]:\n        Hyperparameter optimization algorithm\n\n        - In case a string literal is given:\n            - "grid_search": optimizer that iteratively tests all possible hyperparameter combinations\n            - "random_grid_search": a variation of the grid search optimization that randomly picks hyperparameter\n               combinations from all possible hyperparameter combinations\n            - "timeboxed_random_grid_search": randomly chooses hyperparameter combinations from the set of all\n               possible hyperparameter combinations and tests until the given time limit is reached\n               - `limit_in_minutes`: int\n\n        - In case an object is given:\n          expects the object to have the following methods:\n           - `next_config_generator`: returns a hyperparameter configuration in form of an dictionary containing\n              key->value pairs in the sklearn parameter encoding `model_name__parameter_name: parameter_value`\n           - `prepare`: takes a list of pipeline elements and their particular hyperparameters to test\n           - `evaluate_recent_performance`: gets a tested config and the respective performance in order to\n              calculate a smart next configuration to process\n\n    * `metrics` [list of metric names as str]:\n        Metrics that should be calculated for both training, validation and test set\n        Use the preimported metrics from sklearn and photonai, or register your own\n\n        - Metrics for `classification`:\n            - `accuracy`: sklearn.metrics.accuracy_score\n            - `matthews_corrcoef`: sklearn.metrics.matthews_corrcoef\n            - `confusion_matrix`: sklearn.metrics.confusion_matrix,\n            - `f1_score`: sklearn.metrics.f1_score\n            - `hamming_loss`: sklearn.metrics.hamming_loss\n            - `log_loss`: sklearn.metrics.log_loss\n            - `precision`: sklearn.metrics.precision_score\n            - `recall`: sklearn.metrics.recall_score\n        - Metrics for `regression`:\n            - `mean_squared_error`: sklearn.metrics.mean_squared_error\n            - `mean_absolute_error`: sklearn.metrics.mean_absolute_error\n            - `explained_variance`: sklearn.metrics.explained_variance_score\n            - `r2`: sklearn.metrics.r2_score\n        - Other metrics\n            - `pearson_correlation`: photon_core.framework.Metrics.pearson_correlation\n            - `variance_explained`:  photon_core.framework.Metrics.variance_explained_score\n            - `categorical_accuracy`: photon_core.framework.Metrics.categorical_accuracy_score\n\n    * `best_config_metric` [str]:\n        The metric that should be maximized or minimized in order to choose the best hyperparameter configuration\n\n    * `eval_final_performance` [bool, default=True]:\n        If the metrics should be calculated for the test set, otherwise the test set is seperated but not used\n\n    * `test_size` [float, default=0.2]:\n        the amount of the data that should be left out if no outer_cv is given and\n        eval_final_perfomance is set to True\n\n    * `set_random_seed` [bool, default=False]:\n        If True sets the random seed to 42\n\n    * `verbosity` [int, default=0]:\n        The level of verbosity, 0 is least talkative and gives only warn and error, 1 gives adds info and 2 adds debug\n\n    * `groups` [array-like, default=None]:\n        Info for advanced cross validation strategies, such as LeaveOneSiteOut-CV about the affiliation\n        of the rows in the data. Also works with continuous values and StratifiedKFoldRegression. In case a group\n        variable and a StratifiedCV is passed, the targets will be ignored and only the group variable will be used\n        for the stratification.\n\n    Attributes\n    ----------\n    * `optimum_pipe` [Pipeline]:\n        An sklearn pipeline object that is fitted to the training data according to the best hyperparameter\n        configuration found. Currently, we don\'t create an ensemble of all best hyperparameter configs over all folds.\n        We find the best config by comparing the test error across outer folds. The hyperparameter config of the best\n        fold is used as the optimal model and is then trained on the complete set.\n\n    * `best_config` [dict]:\n        Dictionary containing the hyperparameters of the best configuration.\n        Contains the parameters in the sklearn interface of model_name__parameter_name: parameter value\n\n    * `results` [MDBHyperpipe]:\n        Object containing all information about the for the performed hyperparameter search.\n        Holds the training and test metrics for all outer folds, inner folds and configurations, as well as\n        additional information.\n\n    * `elements` [list]:\n        Contains all PipelineElement or Hyperpipe objects that are added to the pipeline.\n\n    Example\n    -------\n        manager = Hyperpipe(\'test_manager\',\n                            optimizer=\'timeboxed_random_grid_search\', optimizer_params={\'limit_in_minutes\': 1},\n                            outer_cv=ShuffleSplit(test_size=0.2, n_splits=1),\n                            inner_cv=KFold(n_splits=10, shuffle=True),\n                            metrics=[\'accuracy\', \'precision\', \'recall\', "f1_score"],\n                            best_config_metric=\'accuracy\', eval_final_performance=True,\n                            verbose=2)\n\n   '

    def __init__(self, name, inner_cv: BaseCrossValidator=KFold(n_splits=3), outer_cv=None, optimizer='grid_search', optimizer_params: dict={}, metrics=None, best_config_metric=None, eval_final_performance=True, test_size: float=0.2, calculate_metrics_per_fold: bool=True, calculate_metrics_across_folds: bool=False, random_seed=False, verbosity=1, output_settings=None, performance_constraints=None, permutation_id: str=None, cache_folder: str=None, nr_of_processes: int=1):
        self.name = re.sub('\\W+', '', name)
        self.permutation_id = permutation_id
        if cache_folder:
            self.cache_folder = os.path.join(cache_folder, self.name)
        else:
            self.cache_folder = None
        if not calculate_metrics_across_folds:
            if not calculate_metrics_per_fold:
                raise NotImplementedError("Apparently, you've set calculate_metrics_across_folds=False and calculate_metrics_per_fold=False. In this case PHOTON does not calculate any metrics which doesn't make any sense. Set at least one to True.")
        else:
            self.cross_validation = Hyperpipe.CrossValidation(inner_cv=inner_cv, outer_cv=outer_cv,
              eval_final_performance=eval_final_performance,
              test_size=test_size,
              calculate_metrics_per_fold=calculate_metrics_per_fold,
              calculate_metrics_across_folds=calculate_metrics_across_folds)
            self.data = Hyperpipe.Data()
            if output_settings:
                self.output_settings = output_settings
            else:
                self.output_settings = OutputSettings()
        self.results_handler = None
        self.results = None
        self.best_config = None
        self.elements = []
        self._pipe = None
        self.optimum_pipe = None
        self.preprocessing = None
        self.optimization = Hyperpipe.Optimization(metrics=metrics, best_config_metric=best_config_metric,
          optimizer_input=optimizer,
          optimizer_params=optimizer_params,
          performance_constraints=performance_constraints)
        self.optimization.sanity_check_metrics()
        self.is_final_fit = False
        self.nr_of_processes = nr_of_processes
        self.random_state = random_seed
        if random_seed:
            import random
            random.seed(random_seed)
        self._verbosity = 0
        self.verbosity = verbosity
        self.output_settings.set_log_file()

    class CrossValidation:

        def __init__(self, inner_cv, outer_cv, eval_final_performance, test_size, calculate_metrics_per_fold, calculate_metrics_across_folds):
            self.inner_cv = inner_cv
            self.outer_cv = outer_cv
            self.eval_final_performance = eval_final_performance
            self.test_size = test_size
            self.calculate_metrics_per_fold = calculate_metrics_per_fold
            self.calculate_metrics_across_folds = calculate_metrics_across_folds
            self.outer_folds = None
            self.inner_folds = dict()

    class Data:

        def __init__(self, X=None, y=None, kwargs=None):
            self.X = X
            self.y = y
            self.kwargs = kwargs

    class Optimization:
        OPTIMIZER_DICTIONARY = {'grid_search':GridSearchOptimizer, 
         'random_grid_search':RandomGridSearchOptimizer, 
         'timeboxed_random_grid_search':TimeBoxedRandomGridSearchOptimizer, 
         'sk_opt':SkOptOptimizer, 
         'smac':SMACOptimizer, 
         'random_search':RandomSearchOptimizer}

        def __init__(self, optimizer_input, optimizer_params, metrics, best_config_metric, performance_constraints):
            self._optimizer_input = ''
            self.optimizer_input_str = optimizer_input
            self.optimizer_params = optimizer_params
            self.metrics = metrics
            self._best_config_metric = ''
            self.maximize_metric = True
            self.best_config_metric = best_config_metric
            self.performance_constraints = performance_constraints

        @property
        def best_config_metric(self):
            return self._best_config_metric

        @best_config_metric.setter
        def best_config_metric(self, value):
            self._best_config_metric = value
            if isinstance(self.best_config_metric, str):
                self.maximize_metric = Scorer.greater_is_better_distinction(self.best_config_metric)

        @property
        def optimizer_input_str(self):
            return self._optimizer_input

        @optimizer_input_str.setter
        def optimizer_input_str(self, value):
            if isinstance(value, str):
                if value not in Hyperpipe.Optimization.OPTIMIZER_DICTIONARY:
                    raise ValueError('Optimizer ' + value + 'not supported right now.')
            self._optimizer_input = value

        def sanity_check_metrics(self):
            if not isinstance(self.best_config_metric, list):
                if not isinstance(self.best_config_metric, str):
                    if self.metrics is not None:
                        warning_text = 'Best Config Metric must be a single metric given as string, no list. PHOTON chose the first one from the list of metrics to calculate.'
                        self.best_config_metric = self.metrics[0]
                        logger.warn(warning_text)
                        raise Warning(warning_text)
                    else:
                        error_msg = 'No metrics were chosen. Please choose metrics to quantify your performance and set the best_config_metric so that PHOTON which optimizes for'
                        logger.error(error_msg)
                        raise ValueError(error_msg)
                if self.best_config_metric is not None:
                    if self.metrics is None:
                        self.metrics = [
                         self.best_config_metric]
            elif self.best_config_metric not in self.metrics:
                self.metrics.append(self.best_config_metric)
            if self.best_config_metric is None and len(self.metrics) > 0:
                self.best_config_metric = self.metrics[0]
                warning_text = 'No best config metric was given, so PHOTON chose the first in the list of metrics as criteria for choosing the best configuration.'
                logger.warn(warning_text)
                raise Warning(warning_text)
            else:
                if self.metrics is None or len(self.metrics) == 0:
                    metric_error_text = 'List of Metrics to calculate should not be empty'
                    logger.error(metric_error_text)
                    raise ValueError(metric_error_text)

        def get_optimizer(self):
            if isinstance(self.optimizer_input_str, str):
                optimizer_class = self.OPTIMIZER_DICTIONARY[self.optimizer_input_str]
                optimizer_instance = optimizer_class(**self.optimizer_params)
                return optimizer_instance
            return self.optimizer_input_str

        def get_optimum_config(self, tested_configs, fold_operation=FoldOperations.MEAN):
            """
            Looks for the best configuration according to the metric with which the configurations are compared -> best config metric
            :param tested_configs: the list of tested configurations and their performances
            :return: MDBConfiguration that has performed best
            """
            list_of_config_vals = []
            list_of_non_failed_configs = [conf for conf in tested_configs if not conf.config_failed]
            if len(list_of_non_failed_configs) == 0:
                raise Warning('No Configs found which did not fail.')
            try:
                if len(list_of_non_failed_configs) == 1:
                    best_config_outer_fold = list_of_non_failed_configs[0]
                else:
                    for config in list_of_non_failed_configs:
                        list_of_config_vals.append(MDBHelper.get_metric(config, fold_operation, (self.best_config_metric), train=False))

                    if self.maximize_metric:
                        best_config_metric_nr = np.argmax(list_of_config_vals)
                    else:
                        best_config_metric_nr = np.argmin(list_of_config_vals)
                    best_config_outer_fold = list_of_non_failed_configs[best_config_metric_nr]
                logger.debug('Optimizer metric: ' + self.best_config_metric + '\n' + '   --> Maximize metric: ' + str(self.maximize_metric))
                logger.info('Number of tested configurations: ' + str(len(tested_configs)))
                logger.photon_system_log('---------------------------------------------------------------------------------------------------------------')
                logger.photon_system_log('BEST_CONFIG ')
                logger.photon_system_log('---------------------------------------------------------------------------------------------------------------')
                logger.photon_system_log(json.dumps((best_config_outer_fold.human_readable_config), indent=4, sort_keys=True))
                return best_config_outer_fold
            except BaseException as e:
                try:
                    logger.error(str(e))
                finally:
                    e = None
                    del e

        def get_optimum_config_outer_folds(self, outer_folds):
            list_of_scores = list()
            for outer_fold in outer_folds:
                metrics = outer_fold.best_config.best_config_score.validation.metrics
                list_of_scores.append(metrics[self.best_config_metric])

            if self.maximize_metric:
                best_config_metric_nr = np.argmax(list_of_scores)
            else:
                best_config_metric_nr = np.argmin(list_of_scores)
            best_config = outer_folds[best_config_metric_nr].best_config
            return best_config

    @property
    def verbosity(self):
        return self._verbosity

    @verbosity.setter
    def verbosity(self, value):
        self._verbosity = value
        self.output_settings.verbosity = self._verbosity
        self.output_settings.set_log_level()

    def __iadd__(self, pipe_element):
        """
        Add an element to the machine learning pipeline
        Returns self

        Parameters
        ----------
        * 'pipe_element' [PipelineElement]:
            The object to add to the machine learning pipeline, being either a transformer or an estimator.

        """
        if isinstance(pipe_element, Preprocessing):
            self.preprocessing = pipe_element
        else:
            if isinstance(pipe_element, CallbackElement):
                pipe_element.needs_y = True
                self.elements.append(pipe_element)
            else:
                if isinstance(pipe_element, PipelineElement) or issubclass(type(pipe_element), PhotonNative):
                    self.elements.append(pipe_element)
                else:
                    raise TypeError('Element must be of type Pipeline Element')
        return self

    def add(self, pipe_element):
        """
           Add an element to the machine learning pipeline
           Returns self

           Parameters
           ----------
           * `pipe_element` [PipelineElement or Hyperpipe]:
               The object to add to the machine learning pipeline, being either a transformer or an estimator.

           """
        self.__iadd__(pipe_element)

    def _prepare_dummy_estimator(self):
        self.results.dummy_estimator = MDBDummyResults()
        if self.estimation_type == 'regressor':
            self.results.dummy_estimator.strategy = 'mean'
            return DummyRegressor(strategy=(self.results.dummy_estimator.strategy))
        if self.estimation_type == 'classifier':
            self.results.dummy_estimator.strategy = 'most_frequent'
            return DummyClassifier(strategy=(self.results.dummy_estimator.strategy))
        logger.info('Estimator does not specify whether it is a regressor or classifier. DummyEstimator step skipped.')
        return

    def _prepare_result_logging(self, start_time):
        self.results = MDBHyperpipe(name=(self.name), version=__version__)
        self.results.hyperpipe_info = MDBHyperpipeInfo()
        if not self.cross_validation.eval_final_performance:
            self.output_settings.save_predictions_from_best_config_inner_folds = True
        self.results_handler = ResultsHandler(self.results, self.output_settings)
        self.results.computation_start_time = start_time
        self.results.hyperpipe_info.estimation_type = self.estimation_type
        self.results.output_folder = self.output_settings.results_folder
        if self.permutation_id is not None:
            self.results.permutation_id = self.permutation_id
        if self.output_settings:
            if hasattr(self.output_settings, 'wizard_object_id'):
                if self.output_settings.wizard_object_id:
                    self.name = self.output_settings.wizard_object_id
                    self.results.name = self.output_settings.wizard_object_id
                    self.results.wizard_object_id = ObjectId(self.output_settings.wizard_object_id)
                    self.results.wizard_system_name = self.output_settings.wizard_project_name
                    self.results.user_id = self.output_settings.user_id
        self.results.outer_folds = []
        self.results.hyperpipe_info.eval_final_performance = self.cross_validation.eval_final_performance
        self.results.hyperpipe_info.best_config_metric = self.optimization.best_config_metric
        self.results.hyperpipe_info.metrics = self.optimization.metrics
        self.results.hyperpipe_info.maximize_best_config_metric = self.optimization.maximize_metric

        def _format_cross_validation(cv):
            if cv:
                string = '{}('.format(cv.__class__.__name__)
                for key, val in cv.__dict__.items():
                    string += '{}={}, '.format(key, val)

                return string[:-2] + ')'
            return 'None'

        self.results.hyperpipe_info.cross_validation = {'OuterCV':_format_cross_validation(self.cross_validation.outer_cv), 
         'InnerCV':_format_cross_validation(self.cross_validation.inner_cv)}
        self.results.hyperpipe_info.data = {'X_shape':self.data.X.shape,  'y_shape':self.data.y.shape}
        self.results.hyperpipe_info.optimization = {'Optimizer':self.optimization.optimizer_input_str,  'OptimizerParams':str(self.optimization.optimizer_params), 
         'BestConfigMetric':self.optimization.best_config_metric}
        try:
            flowchart = FlowchartCreator(self.elements)
            self.results.hyperpipe_info.flowchart = flowchart.create_str()
        except:
            self.results.hyperpipe_info.flowchart = ''

    def _finalize_optimization(self):
        logger.clean_info('')
        logger.clean_info('***************************************************************************************************************')
        logger.info('Finished all outer fold hyperparameter optimizations.')
        logger.info('Now analysing the results...')
        config_item = MDBConfig()
        dummy_results = [outer_fold.dummy_results for outer_fold in self.results.outer_folds]
        config_item.inner_folds = [f for f in dummy_results if f is not None]
        if len(config_item.inner_folds) > 0:
            self.results.dummy_estimator.train, self.results.dummy_estimator.test = MDBHelper.aggregate_metrics_for_inner_folds(config_item.inner_folds, self.optimization.metrics)
        self.results.metrics_train, self.results.metrics_test = MDBHelper.aggregate_metrics_for_outer_folds(self.results.outer_folds, self.optimization.metrics)
        best_config = self.optimization.get_optimum_config_outer_folds(self.results.outer_folds)
        self.best_config = best_config.config_dict
        self.results.best_config = best_config
        logger.photon_system_log('')
        logger.photon_system_log('===============================================================================================================')
        logger.photon_system_log('OVERALL BEST CONFIGURATION')
        logger.photon_system_log('===============================================================================================================')
        logger.photon_system_log(json.dumps((self.results.best_config.human_readable_config), indent=4, sort_keys=True))
        self.results.computation_end_time = datetime.datetime.now()
        self.results.computation_completed = True
        self.results_handler.save()
        self.results_handler.write_convenience_files()
        logger.info('Fitting best model...')
        self.optimum_pipe = self._pipe
        (self.optimum_pipe.set_params)(**self.best_config)
        self.recursive_cache_folder_propagation(self.optimum_pipe, self.cache_folder, 'fixed_fold_id')
        self.optimum_pipe.caching = False
        self.disable_multiprocessing_recursively(self.optimum_pipe)
        (self.optimum_pipe.fit)((self.data.X), (self.data.y), **(self.data).kwargs)
        self.optimum_pipe._add_preprocessing(self.preprocessing)
        self.recursive_cache_folder_propagation(self.optimum_pipe, None, None)
        if self.output_settings.save_output:
            try:
                pretrained_model_filename = os.path.join(self.output_settings.results_folder, 'photon_best_model.photon')
                PhotonModelPersistor.save_optimum_pipe(self, pretrained_model_filename)
                logger.info('Saved best model to file.')
            except FileNotFoundError as e:
                try:
                    logger.info('Could not save best model to file')
                    logger.error(str(e))
                finally:
                    e = None
                    del e

            logger.info('Mapping back feature importances...')
            feature_importances = self.optimum_pipe.feature_importances_
            if not feature_importances:
                logger.info('No feature importances available for {}!'.format(self.optimum_pipe.elements[(-1)][0]))
            else:
                self.results.best_config_feature_importances = feature_importances
                backmapping, _, _ = self.optimum_pipe.inverse_transform(feature_importances, None)
                self.results_handler.save_backmapping(filename='optimum_pipe_feature_importances_backmapped', backmapping=backmapping)
        elapsed_time = self.results.computation_end_time - self.results.computation_start_time
        logger.photon_system_log('')
        logger.photon_system_log('Analysis ' + self.name + ' done in ' + str(elapsed_time))
        if self.output_settings.results_folder is not None:
            logger.photon_system_log('Your results are stored in ' + self.output_settings.results_folder)
        logger.photon_system_log('***************************************************************************************************************')
        logger.photon_system_log('PHOTON ' + str(__version__) + ' - www.photon-ai.com ')

    @staticmethod
    def disable_multiprocessing_recursively--- This code section failed: ---

 L. 764         0  LOAD_GLOBAL              isinstance
                2  LOAD_FAST                'pipe'
                4  LOAD_GLOBAL              Stack
                6  LOAD_GLOBAL              Branch
                8  LOAD_GLOBAL              Switch
               10  LOAD_GLOBAL              Preprocessing
               12  BUILD_TUPLE_4         4 
               14  CALL_FUNCTION_2       2  '2 positional arguments'
               16  POP_JUMP_IF_FALSE    74  'to 74'

 L. 765        18  LOAD_GLOBAL              hasattr
               20  LOAD_FAST                'pipe'
               22  LOAD_STR                 'nr_of_processes'
               24  CALL_FUNCTION_2       2  '2 positional arguments'
               26  POP_JUMP_IF_FALSE    34  'to 34'

 L. 766        28  LOAD_CONST               1
               30  LOAD_FAST                'pipe'
               32  STORE_ATTR               nr_of_processes
             34_0  COME_FROM            26  '26'

 L. 767        34  SETUP_LOOP          136  'to 136'
               36  LOAD_FAST                'pipe'
               38  LOAD_ATTR                elements
               40  GET_ITER         
             42_0  COME_FROM            54  '54'
               42  FOR_ITER             70  'to 70'
               44  STORE_FAST               'child'

 L. 768        46  LOAD_GLOBAL              hasattr
               48  LOAD_FAST                'child'
               50  LOAD_STR                 'base_element'
               52  CALL_FUNCTION_2       2  '2 positional arguments'
               54  POP_JUMP_IF_FALSE    42  'to 42'

 L. 769        56  LOAD_GLOBAL              Hyperpipe
               58  LOAD_METHOD              disable_multiprocessing_recursively
               60  LOAD_FAST                'child'
               62  LOAD_ATTR                base_element
               64  CALL_METHOD_1         1  '1 positional argument'
               66  POP_TOP          
               68  JUMP_BACK            42  'to 42'
               70  POP_BLOCK        
               72  JUMP_FORWARD        136  'to 136'
             74_0  COME_FROM            16  '16'

 L. 770        74  LOAD_GLOBAL              isinstance
               76  LOAD_FAST                'pipe'
               78  LOAD_GLOBAL              PhotonPipeline
               80  CALL_FUNCTION_2       2  '2 positional arguments'
               82  POP_JUMP_IF_FALSE   120  'to 120'

 L. 771        84  SETUP_LOOP          136  'to 136'
               86  LOAD_FAST                'pipe'
               88  LOAD_ATTR                named_steps
               90  LOAD_METHOD              items
               92  CALL_METHOD_0         0  '0 positional arguments'
               94  GET_ITER         
               96  FOR_ITER            116  'to 116'
               98  UNPACK_SEQUENCE_2     2 
              100  STORE_FAST               'name'
              102  STORE_FAST               'child'

 L. 772       104  LOAD_GLOBAL              Hyperpipe
              106  LOAD_METHOD              disable_multiprocessing_recursively
              108  LOAD_FAST                'child'
              110  CALL_METHOD_1         1  '1 positional argument'
              112  POP_TOP          
              114  JUMP_BACK            96  'to 96'
              116  POP_BLOCK        
              118  JUMP_FORWARD        136  'to 136'
            120_0  COME_FROM            82  '82'

 L. 774       120  LOAD_GLOBAL              hasattr
              122  LOAD_FAST                'pipe'
              124  LOAD_STR                 'nr_of_processes'
              126  CALL_FUNCTION_2       2  '2 positional arguments'
              128  POP_JUMP_IF_FALSE   136  'to 136'

 L. 775       130  LOAD_CONST               1
              132  LOAD_FAST                'pipe'
              134  STORE_ATTR               nr_of_processes
            136_0  COME_FROM           128  '128'
            136_1  COME_FROM           118  '118'
            136_2  COME_FROM_LOOP       84  '84'
            136_3  COME_FROM            72  '72'
            136_4  COME_FROM_LOOP       34  '34'

Parse error at or near `COME_FROM_LOOP' instruction at offset 136_2

    def _input_data_sanity_checks(self, data, targets, **kwargs):
        logger.info('Checking input data...')
        self.data.X = data
        self.data.y = targets
        self.data.kwargs = kwargs
        try:
            if self.data.X is None:
                raise ValueError('(Input-)data is a NoneType.')
            else:
                if self.data.y is None:
                    raise ValueError('(Input-)target is a NoneType.')
                else:
                    shape_x = np.shape(self.data.X)
                    shape_y = np.shape(self.data.y)
                    if len(shape_y) != 1:
                        if len(np.shape(np.squeeze(self.data.y))) == 1:
                            self.data.y = np.squeeze(self.data.y)
                            shape_y = np.shape(self.data.y)
                        else:
                            raise ValueError('Target is not one-dimensional.')
                assert shape_x[0] == shape_y[0], 'Size of targets mismatch to size of the data: ' + str(shape_x[0]) + ' - ' + str(shape_y[0])
        except IndexError as ie:
            try:
                logger.error('IndexError: ' + str(ie))
                raise ie
            finally:
                ie = None
                del ie

        except ValueError as ve:
            try:
                logger.error('ValueError: ' + str(ve))
                raise ve
            finally:
                ve = None
                del ve

        except Exception as e:
            try:
                logger.error('Error: ' + str(e))
                raise e
            finally:
                e = None
                del e

        if isinstance(self.data.X, list):
            self.data.X = np.asarray(self.data.X)
        else:
            if isinstance(self.data.X, (pd.DataFrame, pd.Series)):
                self.data.X = self.data.X.to_numpy()
            if isinstance(self.data.y, list):
                self.data.y = np.asarray(self.data.y)
            else:
                if isinstance(self.data.y, pd.Series) or isinstance(self.data.y, pd.DataFrame):
                    self.data.y = self.data.y.to_numpy()
                try:
                    nans_in_y = np.isnan(self.data.y)
                    nr_of_nans = len(np.where(nans_in_y == 1)[0])
                    if nr_of_nans > 0:
                        logger.info('You have ' + str(nr_of_nans) + ' Nans in your target vector, PHOTON erases every data item that has a Nan Target')
                        self.data.X = self.data.X[(~nans_in_y)]
                        self.data.y = self.data.y[(~nans_in_y)]
                except Exception as e:
                    try:
                        logger.error('Removing Nans in target vector failed: ' + str(e))
                    finally:
                        e = None
                        del e

                logger.info('Running analysis with ' + str(self.data.y.shape[0]) + ' samples.')

    @staticmethod
    def recursive_cache_folder_propagation(element, cache_folder, inner_fold_id):
        if isinstance(element, (Switch, Stack, Preprocessing)):
            for child in element.elements:
                Hyperpipe.recursive_cache_folder_propagation(child, cache_folder, inner_fold_id)

        else:
            if isinstance(element, Branch):
                if cache_folder:
                    cache_folder = os.path.join(cache_folder, element.name)
                Hyperpipe.recursive_cache_folder_propagation(element.base_element, cache_folder, inner_fold_id)
            else:
                if isinstance(element, PhotonPipeline):
                    element.fold_id = inner_fold_id
                    element.cache_folder = cache_folder
                    for name, child in element.named_steps.items():
                        Hyperpipe.recursive_cache_folder_propagation(child, cache_folder, inner_fold_id)

    def preprocess_data(self):
        if self.preprocessing is not None:
            logger.info('Applying preprocessing steps...')
            (self.preprocessing.fit)((self.data.X), (self.data.y), **(self.data).kwargs)
            self.data.X, self.data.y, self.data.kwargs = (self.preprocessing.transform)((self.data.X), (self.data.y), **(self.data).kwargs)

    def _prepare_pipeline(self):
        self._pipe = Branch.prepare_photon_pipe(self.elements)
        self._pipe = Branch.sanity_check_pipeline(self._pipe)
        if self.random_state:
            self._pipe.random_state = self.random_state

    @property
    def estimation_type(self):
        estimation_type = getattr(self.elements[(-1)], '_estimator_type')
        if estimation_type is None:
            raise NotImplementedError('Last element in Hyperpipe should be an estimator.')
        else:
            return estimation_type

    @staticmethod
    def fit_outer_folds(outer_fold_computer, X, y, kwargs, cache_folder):
        try:
            (outer_fold_computer.fit)(X, y, **kwargs)
        finally:
            CacheManager.clear_cache_files(cache_folder)

    def fit(self, data, targets, **kwargs):
        """
        Starts the hyperparameter search and/or fits the pipeline to the data and targets

        Manages the nested cross validated hyperparameter search:

        1. Filters the data according to filter strategy (1) and according to the imbalanced_data_strategy (2)
        2. requests new configurations from the hyperparameter search strategy, the optimizer,
        3. initializes the testing of a specific configuration,
        4. communicates the result to the optimizer,
        5. repeats 2-4 until optimizer delivers no more configurations to test
        6. finally searches for the best config in all tested configs,
        7. trains the pipeline with the best config and evaluates the performance on the test set

        Parameters
        ----------
         * `data` [array-like, shape=[N, D]]:
            the training and test data, where N is the number of samples and D is the number of features.

         * `targets` [array-like, shape=[N]]:
            the truth values, where N is the number of samples.

        Returns
        -------
         * 'self'
            Returns self

        """
        start = datetime.datetime.now()
        self.output_settings._update_settings(self.name, start.strftime('%Y-%m-%d_%H-%M-%S'))
        logger.photon_system_log('***************************************************************************************************************')
        logger.photon_system_log('PHOTON ANALYSIS: ' + self.name)
        logger.photon_system_log('***************************************************************************************************************')
        logger.info('Preparing data and PHOTON objects for analysis...')
        if self.nr_of_processes > 1:
            hyperpipe_client = Client(threads_per_worker=1, n_workers=(self.nr_of_processes), processes=False)
        try:
            try:
                (self._input_data_sanity_checks)(data, targets, **kwargs)
                self._prepare_pipeline()
                self._prepare_result_logging(start)
                self.preprocess_data()
                if not self.is_final_fit:
                    outer_folds = FoldInfo.generate_folds(self.cross_validation.outer_cv, self.data.X, self.data.y, self.data.kwargs, self.cross_validation.eval_final_performance, self.cross_validation.test_size)
                    self.cross_validation.outer_folds = {f.fold_id:f for f in outer_folds}
                    delayed_jobs = []
                    dummy_estimator = self._prepare_dummy_estimator()
                    if self.cache_folder is not None:
                        logger.info('Removing cache files...')
                        CacheManager.clear_cache_files((self.cache_folder), force_all=True)
                    for i, outer_f in enumerate(outer_folds):
                        outer_fold = MDBOuterFold(fold_nr=(outer_f.fold_nr))
                        outer_fold_computer = OuterFoldManager((self._pipe), (self.optimization),
                          (outer_f.fold_id),
                          (self.cross_validation),
                          cache_folder=(self.cache_folder),
                          cache_updater=(self.recursive_cache_folder_propagation),
                          dummy_estimator=dummy_estimator,
                          result_obj=outer_fold)
                        self.results.outer_folds.append(outer_fold)
                        if self.nr_of_processes > 1:
                            result = dask.delayed(Hyperpipe.fit_outer_folds)(outer_fold_computer, self.data.X, self.data.y, self.data.kwargs, self.cache_folder)
                            delayed_jobs.append(result)
                        else:
                            try:
                                (outer_fold_computer.fit)((self.data.X), (self.data.y), **(self.data).kwargs)
                                self.results_handler.save()
                            finally:
                                CacheManager.clear_cache_files(self.cache_folder)

                    if self.nr_of_processes > 1:
                        (dask.compute)(*delayed_jobs)
                        self.results_handler.save()
                    self._finalize_optimization()
                    CacheManager.clear_cache_files((self.cache_folder), force_all=True)
                else:
                    self.preprocess_data()
                    (self._pipe.fit)((self.data.X), (self.data.y), **kwargs)
            except Exception as e:
                try:
                    logger.error(e)
                    logger.error(traceback.format_exc())
                    traceback.print_exc()
                    raise e
                finally:
                    e = None
                    del e

        finally:
            if self.nr_of_processes > 1:
                hyperpipe_client.close()

        return self

    def predict(self, data, **kwargs):
        """
        Use the optimum pipe to predict the data

        Returns
        -------
            predicted targets

        """
        if self._pipe:
            return (self.optimum_pipe.predict)(data, **kwargs)

    def predict_proba(self, data, **kwargs):
        """
        Predict probabilities

        Returns
        -------
        predicted probabilities

        """
        if self._pipe:
            return (self.optimum_pipe.predict_proba)(data, **kwargs)

    def transform(self, data, **kwargs):
        """
        Use the optimum pipe to transform the data
        """
        if self._pipe:
            X, _, _ = (self.optimum_pipe.transform)(data, y=None, **kwargs)
            return X

    def copy_me(self):
        """
        Helper function to copy an entire Hyperpipe
        :return: Hyperpipe
        """
        signature = inspect.getfullargspec(OutputSettings.__init__)[0]
        settings = OutputSettings()
        for attr in signature:
            if hasattr(self.output_settings, attr):
                setattr(settings, attr, getattr(self.output_settings, attr))

        settings.initialize_log_file()
        pipe_copy = Hyperpipe(name=(self.name), inner_cv=(deepcopy(self.cross_validation.inner_cv)),
          outer_cv=(deepcopy(self.cross_validation.outer_cv)),
          best_config_metric=(self.optimization.best_config_metric),
          metrics=(self.optimization.metrics),
          optimizer=(self.optimization.optimizer_input_str),
          optimizer_params=(self.optimization.optimizer_params),
          output_settings=settings)
        signature = inspect.getfullargspec(self.__init__)[0]
        for attr in signature:
            if hasattr(self, attr) and attr != 'output_settings':
                setattr(pipe_copy, attr, getattr(self, attr))

        if hasattr(self, 'preprocessing'):
            if self.preprocessing:
                preprocessing = Preprocessing()
                for element in self.preprocessing.elements:
                    preprocessing += element.copy_me()

                pipe_copy += preprocessing
        if hasattr(self, 'elements'):
            for element in self.elements:
                pipe_copy += element.copy_me()

        return pipe_copy

    def save_optimum_pipe(self, filename=None, password=None):
        if filename is None:
            filename = 'photon_' + self.name + '_best_model.p'
        PhotonModelPersistor.save_optimum_pipe(self, filename, password)

    @staticmethod
    def load_optimum_pipe(file, password=None):
        return PhotonModelPersistor.load_optimum_pipe(file, password)

    def inverse_transform_pipeline(self, hyperparameters: dict, data, targets, data_to_inverse):
        """
        Inverse transform data for a pipeline with specific hyperparameter configuration

        1. Copy Sklearn Pipeline,
        2. Set Parameters
        3. Fit Pipeline to data and targets
        4. Inverse transform data with that pipeline

        Parameters
        ----------
        * `hyperparameters` [dict]:
            The concrete configuration settings for the pipeline elements
        * `data` [array-like]:
            The training data to which the pipeline is fitted
        * `targets` [array-like]:
            The truth values for training
        * `data_to_inverse` [array-like]:
            The data that should be inversed after training

        Returns
        -------
        Inversed data as array
        """
        copied_pipe = self.pipe.copy_me()
        (copied_pipe.set_params)(**hyperparameters)
        copied_pipe.fit(data, targets)
        return copied_pipe.inverse_transform(data_to_inverse)


class FlowchartCreator(object):

    def __init__(self, pipeline_elements):
        self.pipeline_elements = pipeline_elements
        self.chart_str = ''

    def create_str(self):
        header_layout = ''
        header_relate = ''
        old_element = ''
        for pipeline_element in self.pipeline_elements:
            header_layout = header_layout + '[' + pipeline_element.name + ']'
            if old_element:
                header_relate = header_relate + '[' + old_element + ']' + '->' + '[' + pipeline_element.name + ']\n'
            old_element = pipeline_element.name

        self.chart_str = 'Layout:\n' + header_layout + '\nRelate:\n' + header_relate + '\n'
        for pipeline_element in self.pipeline_elements:
            self.chart_str = self.chart_str + self.recursive_element(pipeline_element, '')

        return self.chart_str

    @staticmethod
    def format_cross_validation(cv):
        if cv:
            string = '{}('.format(cv.__class__.__name__)
            for key, val in cv.__dict__.items():
                string += '{}={}, '.format(key, val)

            return string[:-2] + ')'
        return 'None'

    @staticmethod
    def format_optimizer(optimizer):
        return (optimizer.optimizer_input_str, optimizer.optimizer_params, optimizer.metrics, optimizer.best_config_metric)

    def format_kwargs(self, kwargs):
        pass

    @staticmethod
    def format_hyperparameter(hyperparameter):
        if isinstance(hyperparameter, IntegerRange):
            return 'IntegerRange(start: {},\n                                   stop: {}, \n                                   step: {}, \n                                   range_type: {})'.format(hyperparameter.start, hyperparameter.stop, hyperparameter.step, hyperparameter.range_type)
        if isinstance(hyperparameter, FloatRange):
            return 'FloatRange(start: {},\n                                   stop: {}, \n                                   step: {}, \n                                   range_type: {})'.format(hyperparameter.start, hyperparameter.stop, hyperparameter.step, hyperparameter.range_type)
        if isinstance(hyperparameter, Categorical):
            return str(hyperparameter.values)
        return str(hyperparameter)

    def recursive_element(self, pipe_element, parent):
        string = ''
        if isinstance(pipe_element, Stack):
            if parent == '':
                string = '[' + pipe_element.name + ']:\n' + 'Layout:\n'
            else:
                string = '[' + parent[1:] + '.' + pipe_element.name + ']:\n' + 'Layout:\n'
            for pelement in list(pipe_element.elements):
                string = string + '[' + pelement.name + ']|\n'

            string = string + '\n'
            for pelement in list(pipe_element.elements):
                string = string + '\n' + self.recursive_element(pelement, parent=(parent + '.' + pipe_element.name))

        else:
            if isinstance(pipe_element, Switch):
                if parent == '':
                    string = '[' + pipe_element.name + ']:\n' + 'Layout:\n'
                else:
                    string = '[' + parent[1:] + '.' + pipe_element.name + ']:\n' + 'Layout:\n'
                for pelement in pipe_element.elements:
                    string = string + '[' + pelement.name + ']\n'

                string = string + '\n'
                string = string + '\n' + 'Relate:\n'
                old_element = ''
                for pelement in pipe_element.elements:
                    if old_element:
                        string = string + '[' + old_element + ']' + '<:-:>' + '[' + pelement.name + ']\n'
                    old_element = pelement.name
                    string = string + '\n'

                for pelement in pipe_element.elements:
                    string = string + '\n' + self.recursive_element(pelement, parent=(parent + '.' + pipe_element.name))

            else:
                if isinstance(pipe_element, Branch):
                    if parent == '':
                        string = '[' + pipe_element.name + ']:\n' + 'Layout:\n'
                    else:
                        string = '[' + parent[1:] + '.' + pipe_element.name + ']:\n' + 'Layout:\n'
                    for pelement in pipe_element.elements:
                        string = string + '[' + pelement.name + ']'

                    string = string + '\n' + 'Relate:\n'
                    old_element = ''
                    for pelement in pipe_element.elements:
                        if old_element:
                            string = string + '[' + old_element + ']' + '->' + '[' + pelement.name + ']\n'
                        old_element = pelement.name
                        string = string + '\n'

                    for pelement in pipe_element.elements:
                        string = string + '\n' + self.recursive_element(pelement, parent=(parent + '.' + pipe_element.name))

                else:
                    if isinstance(pipe_element, PipelineElement):
                        if parent == '':
                            string = '[' + pipe_element.name + ']:\n' + 'Define:\n'
                        else:
                            string = '[' + parent[1:] + '.' + pipe_element.name + ']:\n' + 'Define:\n'
                        hyperparameters = None
                        kwargs = None
                        if hasattr(pipe_element, 'hyperparameters'):
                            hyperparameters = pipe_element.hyperparameters
                            for name, parameter in pipe_element.hyperparameters.items():
                                string += '{}: {}\n'.format(name.split('__')[(-1)], self.format_hyperparameter(parameter))

                        if hasattr(pipe_element, 'kwargs'):
                            kwargs = pipe_element.kwargs
                            for name, parameter in pipe_element.kwargs.items():
                                string += '{}: {}\n'.format(name.split('__')[(-1)], self.format_hyperparameter(parameter))

                        if not kwargs:
                            if not hyperparameters:
                                string += 'default\n'
                    return string


class PhotonModelPersistor:

    @staticmethod
    def save_elements(elements, folder):
        if not os.path.exists(folder):
            os.makedirs(folder)
        element_identifier = list()
        for i, element in enumerate(elements):
            if hasattr(element, 'disabled'):
                if element.disabled:
                    continue
                else:
                    if isinstance(element, (Stack, Branch, Preprocessing)):
                        filename = '_' + str(i) + '_' + element.name
                        new_folder = os.path.join(folder, filename)
                        element_identifier.append({'element_name':element.name,  'filename':filename})
                        elements = element.elements
                        PhotonModelPersistor.save_elements(elements=elements, folder=new_folder)
                        element.elements = []
                        joblib.dump(element, (os.path.join(folder, filename) + '.pkl'), compress=1)
                        element_identifier[(-1)]['mode'] = 'PhotonBuildingBlock'
                        element.elements = elements
                if not hasattr(element, 'base_element'):
                    base_element = element
                else:
                    base_element = element.base_element
                filename = '_' + str(i) + '_' + element.name
                element_identifier.append({'element_name':element.name,  'filename':filename})
                if hasattr(base_element, 'save'):
                    wrapper_file = inspect.getfile(base_element.__class__)
                    base_element.save(os.path.join(folder, filename))
                    element_identifier[(-1)]['mode'] = 'custom'
                    element_identifier[(-1)]['wrapper_script'] = os.path.basename(wrapper_file)
                    element_identifier[(-1)]['test_disabled'] = element.test_disabled
                    element_identifier[(-1)]['disabled'] = element.disabled
                    element_identifier[(-1)]['hyperparameters'] = element.hyperparameters
                    shutil.copy(wrapper_file, os.path.join(folder, os.path.basename(wrapper_file)))
            else:
                try:
                    joblib.dump(element, (os.path.join(folder, filename) + '.pkl'), compress=1)
                    element_identifier[(-1)]['mode'] = 'pickle'
                except:
                    raise NotImplementedError('Custom pipeline element must implement .save() method or allow pickle.')

        with open(os.path.join(folder, '_optimum_pipe_blueprint.pkl'), 'wb') as (f):
            pickle.dump(element_identifier, f)

    @staticmethod
    def save_optimum_pipe(optimum_pipe, zip_file, password=None):
        """
        Save optimal pipeline only. Complete hyperpipe will no not be saved.

        Parameters
        ----------
        * 'file' [str]:
            File path as string specifying file to save pipeline to
        * 'password' [str]:
            Password used to encrypt the pipeline file

        """
        folder = os.path.splitext(zip_file)[0]
        zip_file = folder + '.photon'
        if os.path.exists(folder):
            logger.warn('The file you specified already exists as a folder.')
        else:
            os.makedirs(folder)
        PhotonModelPersistor.save_elements(optimum_pipe.elements, folder)
        with open(os.path.join(folder, '_optimum_pipe_meta.pkl'), 'wb') as (f):
            meta_infos = {'photon_version': __version__}
            pickle.dump(meta_infos, f)
        files = list()
        for root, directories, filenames in os.walk(folder):
            for filename in filenames:
                files.append(os.path.join(root, filename))

        if password is not None:
            import pyminizip
            pyminizip.compress(files, zip_file, password)
        else:
            with zipfile.ZipFile(zip_file, 'w') as (myzip):
                root_len = len(os.path.dirname(zip_file)) + 1
                for f in files:
                    myzip.write(f, f[root_len:])
                    os.remove(f)

        shutil.rmtree(folder)

    @staticmethod
    def load_elements(folder):
        with open(os.path.join(folder, '_optimum_pipe_blueprint.pkl'), 'rb') as (f):
            setup_info = pickle.load(f)
            element_list = list()
            for element_info in setup_info:
                if element_info['mode'] == 'PhotonBuildingBlock':
                    photon_building_block = joblib.load(os.path.join(folder, element_info['filename'] + '.pkl'))
                    base_elements = PhotonModelPersistor.load_elements(os.path.join(folder, element_info['filename']))
                    for _, element in base_elements:
                        photon_building_block += element

                    element_list.append((element_info['element_name'], photon_building_block))
                elif element_info['mode'] == 'custom':
                    spec = importlib.util.spec_from_file_location(element_info['element_name'], os.path.join(folder, element_info['wrapper_script']))
                    imported_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(imported_module)
                    base_element = getattr(imported_module, element_info['element_name'])
                    custom_element = PipelineElement(name=(element_info['element_name']), base_element=(base_element()), hyperparameters=(element_info['hyperparameters']),
                      test_disabled=(element_info['test_disabled']),
                      disabled=(element_info['disabled']))
                    custom_element.base_element.load(os.path.join(folder, element_info['filename']))
                    element_list.append((element_info['element_name'], custom_element))
                else:
                    loaded_pipeline_element = joblib.load(os.path.join(folder, element_info['filename'] + '.pkl'))
                    element_list.append((element_info['element_name'], loaded_pipeline_element))

        return element_list

    @staticmethod
    def load_optimum_pipe(file, password=None):
        """
        Load optimal pipeline.

        Parameters
        ----------
        * `file` [str]:
            File path specifying .photon file to load optimal pipeline from

        Returns
        -------
        sklearn Pipeline with all trained photon_pipelines
        """
        if file.endswith('.photon'):
            folder = os.path.dirname(file)
            zf = zipfile.ZipFile(file)
            zf.extractall(folder, pwd=password)
        else:
            raise FileNotFoundError('Specify .photon file that holds PHOTON optimum pipe.')
        load_folder = os.path.join(folder, 'photon_best_model')
        meta_infos = {}
        try:
            with open(os.path.join(load_folder, '_optimum_pipe_meta.pkl'), 'rb') as (f):
                meta_infos = pickle.load(f)
        except:
            print('Could not load meta information for optimum pipe')

        element_list = PhotonModelPersistor.load_elements(folder=load_folder)
        from shutil import rmtree
        rmtree((os.path.join(folder, 'photon_best_model')), ignore_errors=True)
        photon_pipe = PhotonPipeline(element_list)
        photon_pipe._meta_information = meta_infos
        return photon_pipe